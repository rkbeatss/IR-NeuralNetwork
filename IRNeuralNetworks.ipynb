{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h1> Paraphrase Detection with Neural Networks - Natural Language Understanding </h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSI4106 Artificial Intelligence<br/>\n",
    "Project Type 3 : In-depth understanding of a solution approach to an AI problem <br/>\n",
    "Prepared by Abha Sharma (8254435) & Rupsi Kaushik (8199148) <br/>\n",
    "Group 33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Background </h2>\n",
    "\n",
    "With the growing trends of virtual assistants and chatbots, Natural Language Processing (NLP) is a topic that is becoming increasingly popular in the recent years. From Google AI's Transformer-based models that consider a word's double-sided context to IBM's training data generator, today we have cutting edge approaches to solving NLP tasks.  However, even with these latest breakthroughs, NLP still faces many challenges, namely the problem of accurately deciphering what humans mean when they express something, regardless of how they express it. This problem falls under Natural Language Understanding (NLU), a subtopic of NLP that aims to increase the proficiency of intelligent systems in exhibiting real knowledge of natural language. Within this field, the task of paraphrase detection - determining whether a pair of sentences convey identical meaning - is considered to be an important one. Through the improvement of paraphrase detection, other NLP tasks that are integral to the efficiency of existing intelligent systems, such as question answering, information retrieval, and text summarization, can also be improved. For this reason, in this report, we propose to enhance the capability of neural networks in the context of paraphrase detection through the use of traditional Information Retrieval (IR) techniques as input features. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Objectives</h2>\n",
    "\n",
    "The main objective of this report is to evaluate the performance of a neural network model given different IR features. Additionally, it will take a look at how the quality and number of features and hidden layers improve the overall performance of the model. These results will be compared among two different training sets that have been annotated for paraphrase detection. Below is the proposed architecture for our particular neural network: \n",
    "<img src=\"CSI4106-NN.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Datasets </h2>\n",
    "\n",
    "We will be working with the Quora Question Pairs and Microsoft Research Paraphrase Corpus datasets for this project. You can find them in this folder labelled as 'msr_train.csv' and 'questions_train.csv'. Each dataset contains pairs of sentences (Sentence_1 and Sentence_2), which have been annotated by humans to indicate whether these sentences capture a semantic equivalence (is_Paraphrase = 1) or not (is_Paraphrase = 0).\n",
    "\n",
    "The Quora Question Pairs dataset was obtained from: https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs . \n",
    "And\n",
    "the Microsoft Research Corpus dataset was obtained from: https://www.microsoft.com/en-ca/download/details.aspx?id=52398\n",
    "\n",
    "Due to the computational and speed limitations of our machines, we decided to only look at 4500 samples from the Quora Question Pairs dataset. Similarly, although, Microsoft has provided the public with both the train and test dataset, we will only be using the train dataset. This train data set will later be split into train and test sets for our neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure to import all these modules\n",
    "import pandas as pd\n",
    "pd.options.mode.use_inf_as_na = True\n",
    "from pyemd import emd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.wsd import lesk\n",
    "from nltk import ngrams\n",
    "from difflib import SequenceMatcher\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "word_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a look at the Quora Question Pairs dataset\n",
    "quora_data = pd.read_csv(\"questions_train.csv\", error_bad_lines=False)\n",
    "quora_data.Sentence_1 = quora_data.Sentence_1.astype(str)\n",
    "quora_data.Sentence_2 = quora_data.Sentence_2.astype(str)\n",
    "quora_data = quora_data[:4500]\n",
    "quora_data.is_Paraphrase = quora_data.is_Paraphrase.astype(int)\n",
    "quora_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking a look at the Microsoft Research Paraphrase dataset \n",
    "mrp_data = pd.read_csv(\"msr_train.csv\")\n",
    "mrp_data.Sentence_1 = mrp_data.Sentence_1.astype(str)\n",
    "mrp_data.Sentence_2 = mrp_data.Sentence_2.astype(str)\n",
    "mrp_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking a look at BoonBot Test Data \n",
    "boon_data = pd.read_csv('boon_test_nn.csv')\n",
    "boon_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Dataset Quality </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both datasets are slightly imbalanced. The Quora dataset contains more of the ‘not a paraphrase’ sample examples while the Microsoft dataset contains more of the ‘is a paraphrase’ sample examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Quora Data:\\nTotal samples in each class:\\n{}\".format(quora_data['is_Paraphrase'].value_counts()))\n",
    "print(\"Number of columns:{}\".format(quora_data.shape[1]))\n",
    "print(\"Number of rows: {}\".format(quora_data.shape[0]))\n",
    "print(\"\\n\")\n",
    "print(\"Mrp Data:\\nTotal samples in each class:\\n{}\".format(mrp_data['is_Paraphrase'].value_counts()))\n",
    "print(\"Number of columns: {}\".format(mrp_data.shape[1]))\n",
    "print(\"Number of rows: {}\".format(mrp_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Preprocessing & Transformation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing common words that provide little to no value to semantic information within a sentence.\n",
    "def remove_stop_words(sentence):\n",
    "    stop_words = stopwords.words('english')\n",
    "    processed_sentence = [re.sub(r\"[,.!?&$]+\",'', word) for word in sentence if not word in stop_words]\n",
    "    return processed_sentence     \n",
    "#Tokenize the sentence for further text processing.\n",
    "def tokenize(sentence):\n",
    "    tokenized_sentence = sentence.lower().split()\n",
    "    return tokenized_sentence\n",
    "#Lemmatization captures the root form of a word but ensures that it is a valid word in the language.\n",
    "def lemmatize(sentence):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = [lemmatizer.lemmatize(word) for word in sentence]\n",
    "    return lemmatized_sentence\n",
    "#Gets synonym for a given word. This lets us capture more semantic information than string matching does.\n",
    "def add_synonym(word):\n",
    "    synonym_list = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for name in syn.lemma_names():\n",
    "             synonym_list.append(name.split(\".\")[0].replace('_',' '))\n",
    "    return list(set(synonym_list))\n",
    "\n",
    "'''Gets antonym for a given word. This helps us later when we get sentences like \"I'm not happy.\"\n",
    "    When we detect a negation, we are now able to better capture the semantic value.\n",
    "'''\n",
    "def add_antonym(word):\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            if(lemma.antonyms()):\n",
    "                return lemma.antonyms()[0].name()\n",
    "            else:\n",
    "                return None     \n",
    "#Gets the hypernym (the broader category that a word belongs to) of a given word.(ie, clothing is a hypernym of shirt).  \n",
    "def add_hypernym(word):\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for hypernym in syn.hypernyms():\n",
    "            return hypernym.name().split('.')[0]\n",
    "#Handles negation term and includes antonyms of this given term. \n",
    "def tokenize_negation(sentence):\n",
    "    negation_adverbs = [\"no\", \"without\",\"not\", \"n't\", \"never\", \"neith\", \"nor\"]\n",
    "    tokens_with_negation = []\n",
    "    tokenized_sentence = tokenize(sentence)\n",
    "    i = 0\n",
    "    while i < (len(tokenized_sentence)):\n",
    "        if (i != len(tokenized_sentence)-1) and (tokenized_sentence[i] in negation_adverbs):\n",
    "            negation_token = add_antonym(tokenized_sentence[i+1])\n",
    "            if(negation_token):\n",
    "                tokens_with_negation.append(negation_token)\n",
    "                i += 2 \n",
    "            else:\n",
    "                tokens_with_negation.append(tokenized_sentence[i])\n",
    "                i +=1\n",
    "        else:\n",
    "            tokens_with_negation.append(tokenized_sentence[i])\n",
    "            i += 1\n",
    "    return tokens_with_negation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Baseline Model </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Pairwise Similarity </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Description </h4>\n",
    "\n",
    "Cosine Similarity is a simple IR measure that calculates similarity among pairwise input vectors projected in a multi-dimensional space, based on their cosine angle. In its core, this method is syntactic as it is purely based on common word occurrences/counts and does not take into account word order or other semantic information. However, it is more than enough to capture similarity for a baseline model. *explain why we chose this*. Let's do an example by hand with Quora dataset in order to illustrate this measure. \n",
    "<h4> Illustration </h4>\n",
    "<br /> Sentence_1: \"What is the step by step guide to invest in share market in India?\" <br/> Sentence_2: \"What is the step by step guide to invest in share market?\" <br />\n",
    "After tokenization, stopword removal, and lemmatization, our sentences would look something like this: <br/>\n",
    "Sentence_1: ['step', 'step', 'guide','invest','share','market','india'] <br />\n",
    "Sentence_2: ['step', 'step', 'guide', 'invest', 'share', 'market'] <br />\n",
    "Now we calculate <b>term frequency</b> and <b>inverse document frequency(tf-idf)</b>. Term frequency counts the frequency of word occurrence in each sentence: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The number of times a word occurs in each sentence\n",
    "term_document_matrix = {\"Document\": ['Sentence_1', 'Sentence_2'],\n",
    "                   \"step\":[2,2], \n",
    "                   \"guide\":[1,1], \n",
    "                   \"invest\":[1,1],\n",
    "                   \"share\":[1,1],\n",
    "                   \"market\":[1,1],\n",
    "                   \"india\":[1,0]}\n",
    "tdm_df = pd.DataFrame(term_document_matrix)\n",
    "print(tdm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term frequency can be further normalized to take into account sentence length (ie, the tf for 'step' would now be 2/7 for Sentence_1 and 1/3 for Sentence_2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalized term frequency by accounting for total number of words in each sentence\n",
    "extended_tdm = {\"Document\": ['Sentence_1', 'Sentence_2'],\n",
    "                   \"step\":[0.29, 0.33], \n",
    "                   \"guide\":[0.14,0.16], \n",
    "                   \"invest\":[0.14,0.16],\n",
    "                   \"share\":[0.14,0.16],\n",
    "                   \"market\":[0.14,0.16],\n",
    "                   \"india\":[0.14,0]}\n",
    "extended_tdm_df = pd.DataFrame(extended_tdm)\n",
    "print(extended_tdm_df)                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverse document frequency lets us to put more value on the occurrence of rare terms and put less value on the frequently occurring terms throughout the sentences. This is done to acknowledge that the frequent occurrence of a word like 'step' in both documents is less distinguishing than the occurrence of a word like 'india'. This concept of idf can be captured through the equation: <b> log(N/df(w))</b>, where N is the total number of sentences and df (aka document frequency) is the number of sentences with word w in it. In our case, N is 2 at each comparison. This means that the df at word w in our case is:\n",
    "df(step): 2, df(guide): 2, df(invest): 2, df(share): 2 , df(market): 2 , df(india): 1 <br /> \n",
    "The idf can, thus, be calculated through our equation as: log(2/2) +1 = 1, 1, 1, 1, 1, 1.3, respectively (adding 1 to accommodate for 0's). <br /> \n",
    "Then the tf-idf becomes tf * idf = 1*0.29 = 0.29, 0.14, 0.14, 0.14, 0.14, 0.182 for Sentence_1, and 0.33, 0.16, 0.16, 0.16,0.16, 0 for Sentence_2, respectively. <br />\n",
    "Now we can calculate the cosine similarity through the dot product: <img src=\"cosine.png\"> <br />\n",
    "Where d2 is the tf-idf vector for Sentence_1 and q is the tf-idf vector for Sentence_2 that we previously calculated. <br/>\n",
    " = (0.29 * 0.33 + 0.14 * 0.16 + ...) / (0.29 * 0.29 + 0.14 * 0.14 + ...) (0.33 * 0.33 + 0.16 * 0.16 + ...) <br />\n",
    " = 0.182/0.203 <br />\n",
    " = approx 0.89 or 0.46 radians <br/> \n",
    " Therefore, the cosine similarity of Sentence_1 and Sentence_2 is approximately 0.89, making them highly similar to one another.  <br />\n",
    "This process has been coded below with the help of sklearn in order to define our baseline approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(sentence_one, sentence_two):\n",
    "    #handles the calculation of tfidf and building of term document matrices for us\n",
    "    tfidf = TfidfVectorizer(preprocessor=' '.join)\n",
    "    tfidf_matrix = tfidf.fit_transform([sentence_one, sentence_two])\n",
    "    #calculates the cosine similarity of the pairwise sentences\n",
    "    similarity = cosine_similarity(tfidf_matrix)[0,1]\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_similarity(row):\n",
    "    sentence_one_tokenize = tokenize_negation(row['Sentence_1'])\n",
    "    sentence_two_tokenize = tokenize_negation(row['Sentence_2'])\n",
    "    filtered_sentence_one = remove_stop_words(sentence_one_tokenize)\n",
    "    filtered_sentence_two = remove_stop_words(sentence_two_tokenize)\n",
    "    lemmatize_sentence_one = lemmatize(filtered_sentence_one)\n",
    "    lemmatize_sentence_two = lemmatize(filtered_sentence_two)\n",
    "    return calculate_cosine_similarity(lemmatize_sentence_one, lemmatize_sentence_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Cosine Similaritye to Quora sentence pairs \n",
    "quora_data['Cosine_Similarity'] = quora_data.apply(get_cosine_similarity, axis=1)\n",
    "quora_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Cosine Similarity Coefficient to MRP Corpus sentence pairs \n",
    "mrp_data['Cosine_Similarity'] = mrp_data.apply(get_cosine_similarity, axis=1)\n",
    "mrp_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Cosine Similarity to BoonBot test data\n",
    "boon_data['Cosine_Similarity'] = boon_data.apply(get_cosine_similarity, axis=1)\n",
    "boon_data.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Evaluation Measures </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wil be using accuracy, recall, and precision in order to evaluate our models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate how many were right predictions out of the total predicted\n",
    "def calculate_accuracy(model, actual_tags):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for prediction, actual in zip(model, actual_tags):\n",
    "        total += 1\n",
    "        if prediction == actual:\n",
    "            correct += 1 \n",
    "    accuracy = correct / total \n",
    "    return '{0:.1%}'.format(accuracy)\n",
    "#Keeps track of true positives, true negatives, false positives, and false negatives.\n",
    "def build_confusion_matrix(actual_tags, model, classOfInterest):\n",
    "    confusion_matrix = {}\n",
    "    truePositives = len([p for p, a in zip(model, actual_tags) if p == a and p == classOfInterest])\n",
    "    trueNegatives = len([p for p, a in zip(model, actual_tags) if p == a and p != classOfInterest])\n",
    "    falsePositives = len([p for p, a in zip(model, actual_tags) if p != a and p == classOfInterest])\n",
    "    falseNegatives = len([p for p, a in zip(model, actual_tags) if p != a and p != classOfInterest])\n",
    "    confusion_matrix[\"tp\"] = truePositives\n",
    "    confusion_matrix[\"tn\"] = trueNegatives\n",
    "    confusion_matrix[\"fp\"] = falsePositives \n",
    "    confusion_matrix[\"fn\"] = falseNegatives\n",
    "    return confusion_matrix\n",
    "#Calculate how many relevant predictions are retrieved in general\n",
    "def calculate_recall(model, actual_tags, classOfInterest):\n",
    "    matrix = build_confusion_matrix(model, actual_tags, classOfInterest)\n",
    "    recall = matrix[\"tp\"] / ( matrix[\"tp\"] + matrix [\"fn\"])\n",
    "    return '{0:.1%}'.format(recall)\n",
    "#Calculate how many retrieved predictions are relevant\n",
    "def calculate_precision(model, actual_tags, classOfInterest):\n",
    "    matrix = build_confusion_matrix(model, actual_tags, classOfInterest)\n",
    "    precision = matrix[\"tp\"]/ (matrix[\"tp\"] + matrix[\"fp\"])\n",
    "    return '{0:.1%}'.format(precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Baseline Model Threshold </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to decide what similarity value is adequate to determine whether a pair of sentences are semantically equivalent. Are we going to accept them as a paraphrase if the measure is above 0.5? Above 0.8? Instead of randomly picking a threshold for the baseline method, we are going to 'learn' a threshold that yields relatively best results for us in terms of accuracy, recall, and precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cosine_classification(row, **kwargs):\n",
    "    if(row['Cosine_Similarity'] > kwargs['threshold']):\n",
    "        classification = 1\n",
    "    else:\n",
    "        classification = 0\n",
    "    return classification\n",
    "\n",
    "thresholds = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9]\n",
    "quora_data_thresholds = {}\n",
    "mrp_data_thresholds = {}\n",
    "boon_data_thresholds = {}\n",
    "accuracies = {}\n",
    "recalls = {}\n",
    "precisions = {}\n",
    "for threshold in thresholds:\n",
    "    quora_data_thresholds[threshold] = quora_data.apply(apply_cosine_classification, threshold = threshold, axis = 1)\n",
    "    accuracies[threshold] = calculate_accuracy(quora_data_thresholds[threshold], quora_data['is_Paraphrase'])\n",
    "    recalls[threshold] = calculate_recall(quora_data_thresholds[threshold], quora_data['is_Paraphrase'], 1)\n",
    "    precisions[threshold] = calculate_precision(quora_data_thresholds[threshold], quora_data['is_Paraphrase'], 1)\n",
    "print(\"Quora Accuracy: {}, \\n Quora Recall: {},\\nQuora Precision: {}\" .format(accuracies, recalls, precisions))\n",
    "\n",
    "for threshold in thresholds:\n",
    "    mrp_data_thresholds[threshold] = mrp_data.apply(apply_cosine_classification, threshold = threshold, axis = 1)\n",
    "    accuracies[threshold] = calculate_accuracy(mrp_data_thresholds[threshold], mrp_data['is_Paraphrase'])\n",
    "    recalls[threshold] = calculate_recall(mrp_data_thresholds[threshold], mrp_data['is_Paraphrase'],1)\n",
    "    precisions[threshold] = calculate_precision(mrp_data_thresholds[threshold], mrp_data['is_Paraphrase'], 1)\n",
    "print(\"MRP Accuracy: {}, \\nMRP Recall: {},\\nMRP Precision: {}\" .format(accuracies, recalls, precisions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Picking a Threshold </h4>\n",
    "We chose 0.45 as our threshold. We take into account all three metrics and the results of both datasets but put a higher emphasis on accuracy and precision. A high accuracy can indicate that the model is great, however, all the other parameters must be taken into account. We might have higher accuracies at other thresholds but taking a look at the confusion matrix at each threshold, there is an imbalance of classes at these thresholds with higher accuracies, which is not necessarily good. For example, let’s say our dataset has 99% of one class and 1% of the other, then our accuracy is pretty high but this  clearly shows that there is equal cost to false positives and false negatives.  Therefore, metrics need to also be relative to each class. The precision at 0.45 is overall decent for both datasets, indicating that at this threshold the specificity is pretty good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quora_baseline = {}\n",
    "quora_baseline['Cosine_Classification'] = quora_data.apply(apply_cosine_classification, threshold=0.45, axis=1)\n",
    "quora_baseline_df = pd.DataFrame(quora_baseline)\n",
    "quora_baseline_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrp_baseline = {}\n",
    "mrp_baseline['Cosine_Classification'] = mrp_data.apply(apply_cosine_classification, threshold = 0.45,  axis=1)\n",
    "mrp_baseline_df = pd.DataFrame(mrp_baseline)\n",
    "mrp_baseline_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boon_baseline = {}\n",
    "boon_baseline['Cosine_Classification'] = boon_data.apply(apply_cosine_classification, threshold = 0.45, axis = 1)\n",
    "boon_baseline_df = pd.DataFrame(boon_baseline)\n",
    "boon_baseline_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Measures and Results </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_evaluation_summary(baseline, df):\n",
    "    baseline_accuracy = calculate_accuracy(baseline['Cosine_Classification'], df['is_Paraphrase'])\n",
    "    baseline_recall = calculate_recall(baseline['Cosine_Classification'], df['is_Paraphrase'], 1)\n",
    "    baseline_precision = calculate_precision(baseline['Cosine_Classification'], df['is_Paraphrase'], 1)\n",
    "    \n",
    "    baseline_evaluation_summary = {\"Model\": ['Baseline'],\n",
    "                   \"Accuracy\":[(baseline_accuracy)], \n",
    "                   \"Recall\":[baseline_recall], \n",
    "                   \"Precision\":[baseline_precision]}\n",
    "    results_df = pd.DataFrame(baseline_evaluation_summary)\n",
    "    print (results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Quora Data Baseline Evaluation')\n",
    "baseline_evaluation_summary(quora_baseline, quora_data)\n",
    "print('Microsoft Data Baseline Evaluation')\n",
    "baseline_evaluation_summary(mrp_baseline, mrp_data)\n",
    "print('BoonBot Data Baseline Evaluation')\n",
    "baseline_evaluation_summary(boon_baseline, boon_data)\n",
    "#wherever the cosine similarity is bigger, choose that \n",
    "#where they're equal - reprompt the user to choose what they mean "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Feature Engineering</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Syntactic Similarity</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Edit Distance</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit distance is a measure of similarity between two strings, source and target. It is the minimum number of operations (insertion, deletion, or substitution) required to transform the source string to target string. For example, the edit distance between 'monkey' and 'money' is 1. Deletion of 'k' in 'monkey' will give us 'money'.\n",
    "Here, we have implemented such edit distance but on the word level instead of character level. This was done by transforming the sentences to list and then comparing each element of the source and the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_distance(row):\n",
    "    sentence_one_tokenize = tokenize(row['Sentence_1'])\n",
    "    sentence_two_tokenize = tokenize(row['Sentence_2'])\n",
    "    return nltk.edit_distance(sentence_one_tokenize, sentence_two_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Applying Edit Distance to Quora sentence pairs \n",
    "quora_data['Edit_distance'] = quora_data.apply(edit_distance, axis=1)\n",
    "quora_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Edit Distance to MRP Corpus sentence pairs \n",
    "mrp_data['Edit_distance'] = mrp_data.apply(edit_distance, axis=1)\n",
    "mrp_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boon_data['Edit_distance'] = boon_data.apply(edit_distance, axis=1)\n",
    "boon_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Jaccard Similarity Coefficient</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jaccard Similarity Coefficient is another similarity measure. It is calculated by dividing the intersection (common words) of the two sentences over the union of the two sentences(length of sentence one + length of sentence two - intersection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_sim_coefficient(row):\n",
    "    sentence_one_tokenize = tokenize(row['Sentence_1'])\n",
    "    sentence_two_tokenize = tokenize(row['Sentence_2'])\n",
    "    intersection = 0\n",
    "    for word_in_one in sentence_one_tokenize:\n",
    "        if word_in_one in sentence_two_tokenize:\n",
    "            intersection += 1\n",
    "    union = (len(sentence_one_tokenize) + len(sentence_two_tokenize) - intersection)\n",
    "    return intersection/union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Jaccard Similarity Coefficient to Quora sentence pairs \n",
    "quora_data['Jaccard_similarity'] = quora_data.apply(jaccard_sim_coefficient, axis=1)\n",
    "quora_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Jaccard Similarity Coefficient to MRP Corpus sentence pairs \n",
    "mrp_data['Jaccard_similarity'] = mrp_data.apply(jaccard_sim_coefficient, axis=1)\n",
    "mrp_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boon_data['Jaccard_similarity'] = boon_data.apply(jaccard_sim_coefficient, axis = 1)\n",
    "boon_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Sequence Matcher</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SequenceMatcher is a class in the python module difflib. It finds the length of the longest contiguous matching subsequence. The ratio then divides it by the total length of characeters of both sentences and multiplies it by 2. This returns the similarity score (float in [0,1]). For example, <br>\n",
    "[THANK]S[ FOR][ RESPONSE]and [THANK]ING[ FOR] KIND[ RESPONSE] has 18 characters in the longest subsequence, including spaces. Therefore, the ratio will output 0.8 (2*18/45)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_matcher(row):\n",
    "    return SequenceMatcher(None, row['Sentence_1'], row['Sentence_2']).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Sequence Matcher to Quora sentence pairs \n",
    "quora_data['Sequence_matcher'] = quora_data.apply(sequence_matcher, axis=1)\n",
    "quora_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Sequence Matcher to MRP Corpus sentence pairs \n",
    "mrp_data['Sequence_matcher'] = mrp_data.apply(sequence_matcher, axis=1)\n",
    "mrp_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boon_data['Sequence_matcher'] = boon_data.apply(sequence_matcher, axis = 1)\n",
    "boon_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>N-gram measure</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-gram is a sequence of N words. Here, we create N-grams of both sentences. We then look at the common grams and divide it by the union of grams (in other words perform a jaccard coefficient with the n-grams). Through our research, we found that N = 3 or 4 is commonly used and are optimal at capturing the probability of a given word given the previous words. We chose N = 3 because we felt that this would be able to capture context even for shorter sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_measure(row):\n",
    "    n = 3\n",
    "    common_count = 1\n",
    "    grams_sentence_one = ngrams(row['Sentence_1'].split(), n)\n",
    "    grams_sentence_two = ngrams(row['Sentence_2'].split(), n)\n",
    "    grams_sentence_one_total = sum(1 for x in grams_sentence_one)\n",
    "    grams_sentence_two_total = sum(1 for x in grams_sentence_two)\n",
    "    for gram_in_one in grams_sentence_one:\n",
    "        if gram_in_one in grams_sentence_two:\n",
    "            common_count += 1\n",
    "    union = grams_sentence_one_total + grams_sentence_two_total - common_count\n",
    "    return common_count / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying N-gram Measure to Quora sentence pairs \n",
    "quora_data['N-gram_measure'] = quora_data.apply(ngram_measure, axis=1)\n",
    "quora_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying N-gram Measure to MRP Corpus sentence pairs \n",
    "mrp_data['N-gram_measure'] = mrp_data.apply(ngram_measure, axis=1)\n",
    "mrp_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boon_data['N-gram_measure'] = boon_data.apply(ngram_measure, axis = 1)\n",
    "boon_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Semantic Similarity</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Word Mover's Distance </h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Mover's Distance uses normalized bag of words and word embeddings to calculate the distance between sentences. It retrieves vectors from pre-trained word embeddings models for the words of the sentences. The key assumption with this similarity measure is that similar words should have similar vectors. <br/>\n",
    "For example, 'Obama speaks to the media in Illinois' and 'The president greets the press in Chicago' have the same meaning, however they do not have any words in common. Word Mover's Distance helps with this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_movers_distance(row):\n",
    "    sentence_one_tokenize = tokenize(row['Sentence_1'])\n",
    "    sentence_two_tokenize = tokenize(row['Sentence_2'])\n",
    "    filtered_sentence_one = remove_stop_words(sentence_one_tokenize)\n",
    "    filtered_sentence_two = remove_stop_words(sentence_two_tokenize)\n",
    "    distance = word_vectors.wmdistance(filtered_sentence_one, filtered_sentence_two)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Word Mover's Distance to Quora sentence pairs \n",
    "quora_data['WMD_distance'] = quora_data.apply(word_movers_distance, axis=1)\n",
    "quora_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Word Mover's Distance to MRP Corpus sentence pairs \n",
    "mrp_data['WMD_distance'] = mrp_data.apply(word_movers_distance, axis=1)\n",
    "mrp_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boon_data['WMD_distance'] = boon_data.apply(word_movers_distance, axis=1)\n",
    "boon_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Named Entity Recognition Similarity</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this feature, we first collected NER words for each sentences along with their label. For example, 'Washington' will have a label of 'GPE' for geo-political entities. We computed Jaccard Coefficient by dividing the common NER (with label) over the union of NER of both sentences. \n",
    "In the case where no NER was detected, in neither of the sentences, we simply returned 0, else we returned the Jaccard Coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_measure(row):\n",
    "    ner_sentence_one=[]\n",
    "    ner_sentence_two=[]\n",
    "    count_common_ner = 0\n",
    "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(row['Sentence_1']))):\n",
    "        if hasattr(chunk, 'label'):\n",
    "            ner_sentence_one.append(chunk)\n",
    "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(row['Sentence_2']))):\n",
    "        if hasattr(chunk, 'label'):\n",
    "            ner_sentence_two.append(chunk)\n",
    "    for item in ner_sentence_one:\n",
    "        if item in ner_sentence_two:\n",
    "            count_common_ner += 1\n",
    "    union = len(ner_sentence_one) + len(ner_sentence_two) - count_common_ner\n",
    "    if union == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return count_common_ner / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying NER Measure to Quora sentence pairs \n",
    "quora_data['NER_similarity'] = quora_data.apply(ner_measure, axis=1)\n",
    "quora_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying NER Measure to MRP Corpus sentence pairs \n",
    "mrp_data['NER_similarity'] = mrp_data.apply(ner_measure, axis=1)\n",
    "mrp_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boon_data['NER_similarity'] = boon_data.apply(ner_measure, axis=1)\n",
    "boon_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Word Sense Disambiguation</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Sense Disambiguation finds the best sense of a word from all the given senses of the word. The Lesk algorithm uses WordNet and gets the gloss of all the senses of the word in the sentence and then calculates the maximum overlap with the senses, returning whichever gives the maximum overlap. For example, let's take the phrase 'pine cone'. 'Pine' has two senses. Sense 1: kind of evergreen tree with needle-shaped leaves and Sense 2: waste away through sorrow or illness. 'Cone' has three senses. Sense 1: solid body which narrows to a point. Sense 2: something of this shape whether solid or hollow. Sense 3: fruit of a certain evergreen tree. Comparing the senses of the two words, we can see that 'evergreen tree' is common in one sense of each word. Therefore, Sense 1 of Pine and Sense 3 of Cone are the most appropriate when 'pine' and 'cone' are used together.  \n",
    "\n",
    "Using this knowledge, we created our feature. After Lesk was applied to each sentences, where the most appropriate senses of each word was detected, we looked for the common senses in the two sentences. To normalize our result, we again used Jaccard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wsd(row):\n",
    "    sentence_one_senses = []\n",
    "    sentence_two_senses = []\n",
    "    common_senses = 0\n",
    "    sentence_one_tokenize = tokenize(row['Sentence_1'])\n",
    "    sentence_two_tokenize = tokenize(row['Sentence_2'])\n",
    "    for word in sentence_one_tokenize:\n",
    "        sentence_one_senses.append(lesk(row['Sentence_1'], word))\n",
    "    for word in sentence_two_tokenize:\n",
    "        sentence_two_senses.append(lesk(['Sentenece_2'], word))\n",
    "    sentence_one_senses = (set(sentence_one_senses))\n",
    "    sentence_two_senses = (set(sentence_two_senses))\n",
    "\n",
    "    for sense in sentence_one_senses:\n",
    "        if sense in sentence_two_senses:\n",
    "            common_senses += 1\n",
    "    return common_senses / (len(sentence_one_senses) + len(sentence_two_senses) - common_senses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying WSD Measure to Quora sentence pairs \n",
    "quora_data['WSD'] = quora_data.apply(wsd, axis=1)\n",
    "quora_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying WSD Measure to MRP Corpus sentence pairs \n",
    "mrp_data['WSD'] = mrp_data.apply(wsd, axis=1)\n",
    "mrp_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boon_data['WSD'] = boon_data.apply(wsd, axis=1)\n",
    "boon_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Wordnet Extended Cosine Similarity </h4>\n",
    "Here, we use WordNet's capabilities to extend our sentences and, therefore, extend our dictionary. We extend our semantic reach by including synonyms, hypernyms, and antonyms in our dictionary. We then call our existing calculate_cosine_similarity method to get the similarity of the extended documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_sentence_wordnet(row):\n",
    "    sentence_one_tokenize = tokenize_negation(row['Sentence_1'])\n",
    "    sentence_two_tokenize = tokenize_negation(row['Sentence_2'])\n",
    "    filtered_sentence_one = remove_stop_words(sentence_one_tokenize)\n",
    "    filtered_sentence_two = remove_stop_words(sentence_two_tokenize)\n",
    "    lemmatize_sentence_one = lemmatize(filtered_sentence_one)\n",
    "    lemmatize_sentence_two = lemmatize(filtered_sentence_two)\n",
    "    # get extended synonym list    \n",
    "    extended_dictionary_one = []\n",
    "    extended_dictionary_two = []\n",
    "    for one, two in zip(lemmatize_sentence_one, lemmatize_sentence_two):\n",
    "        synonym_one = add_synonym(one)\n",
    "        synonym_two = add_synonym(two)\n",
    "        hypernym_one = add_hypernym(one)\n",
    "        hypernym_two = add_hypernym(two)\n",
    "        if(synonym_one):\n",
    "            extended_dictionary_one += synonym_one\n",
    "        if(hypernym_one):\n",
    "            extended_dictionary_one += hypernym_one\n",
    "        if(synonym_two):\n",
    "            extended_dictionary_two += synonym_two\n",
    "        if(hypernym_two):\n",
    "            extended_dictionary_two += hypernym_two\n",
    "    lemmatize_sentence_one += extended_dictionary_one\n",
    "    lemmatize_sentence_two += extended_dictionary_two\n",
    "    \n",
    "    #calculate similarity based on the extended list \n",
    "    similarity = calculate_cosine_similarity(lemmatize_sentence_one, lemmatize_sentence_two)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Synonym_Hypernym_Cosine to Quora sentence pairs \n",
    "quora_data['Synonym_Hypernym_Cosine'] = quora_data.apply(extend_sentence_wordnet, axis=1)\n",
    "quora_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Applying Synonym_Hypernym_Cosine to MRP Corpus sentence pairs \n",
    "mrp_data['Synonym_Hypernym_Cosine'] = mrp_data.apply(extend_sentence_wordnet, axis = 1)\n",
    "mrp_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boon_data['Synonym_Hypernym_Cosine'] = boon_data.apply(extend_sentence_wordnet, axis=1)\n",
    "boon_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Neural Network Model </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Post Processing </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed that some of the features contained infinite values, especially the WMD measure. To handle this, we replaced the infinite values with the maximum value of that particular feature. We then normalized the feature so that all values were within the range of 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_mrp = mrp_data.loc[mrp_data['WMD_distance'] != np.nan, 'WMD_distance'].max()\n",
    "mrp_data['WMD_distance'].replace(np.nan, max_mrp, inplace=True)\n",
    "x = mrp_data[['WMD_distance']].values.astype(float)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "mrp_data['WMD_distance'] = pd.DataFrame(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_quora = quora_data.loc[quora_data['WMD_distance'] != np.nan, 'WMD_distance'].max()\n",
    "quora_data['WMD_distance'].replace(np.nan, max_quora, inplace=True)\n",
    "x = quora_data[['WMD_distance']].values.astype(float)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "quora_data['WMD_distance'] = pd.DataFrame(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_boon = boon_data.loc[boon_data['WMD_distance'] != np.nan, 'WMD_distance'].max()\n",
    "boon_data['WMD_distance'].replace(np.nan, max_boon, inplace=True)\n",
    "x = boon_data[['WMD_distance']].values.astype(float)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "boon_data['WMD_distance'] = pd.DataFrame(x_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Multi-layer Perceptron </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_perceptron(df, model):\n",
    "    features = list(df.columns.values)\n",
    "    features.remove('is_Paraphrase')\n",
    "    features.remove('Sentence_1')\n",
    "    features.remove('Sentence_2')\n",
    "    X = df[features]\n",
    "    y = df['is_Paraphrase']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=15, batch_size=1)\n",
    "\n",
    "\n",
    "    test_loss, test_acc  = model.evaluate(X_test, y_test)\n",
    "    print('Test accuracy:{}'.format(test_acc))\n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our research, we found that a common rule of thumb that most researchers use when deciding the number of neurons to implement in the hidden layer is ‘to use a number between the size of the input and size of the output layers’, as this results in the optimal size. Following this rule of thumb, we decided to  keep our number of neurons in the hidden layer to 7 as it in between 9 which is the total number of our inputs and 1 which is the number of our output. We also noticed that there was an improvement in our model, when we increased the number of hidden layers from one to two. However, increasing the hidden layers from two made no significant difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quora data NN\n",
    "model_quora = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(9,)),\n",
    "    keras.layers.Dense(7, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(7, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid),\n",
    "    ])\n",
    "print(\"Multi-Layer Perceptron Results for quora_data\")\n",
    "multilayer_perceptron(quora_data, model_quora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mircosoft data NN\n",
    "model_mrp = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(9,)),\n",
    "    keras.layers.Dense(7, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(7, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid),\n",
    "    ])\n",
    "print(\"Multi-Layer Perceptron Results for mrp_data\")\n",
    "model_mrp = multilayer_perceptron(mrp_data, model_mrp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boon data NN\n",
    "model_boon = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(9,)),\n",
    "    keras.layers.Dense(7, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(7, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid),\n",
    "    ])\n",
    "print(\"Multi-Layer Perceptron Results for boon\")\n",
    "model_boon = multilayer_perceptron(boon_data, model_boon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Feature Testing</h4>\n",
    "We decided to take out each feature and see how the Neural Network performs. The logic for our approach is that if the metrics are higher, then it means that the removed feature is not adding as much value to the overall model. For Quora, we found that the worst feature was WMD distance. For Microsoft, the worst one was the word n-gram. However, we did not find a particular feature that outperformed another or did noticeably worse than the other, since all the metrics were similar to each other. In the future, we plan on looking for more efficient ways to do feature evaluation such as permutation feature importance, where the value of the features are shuffled rather than dropped entirely like we did. This process would also tell us what our best features were in a more logical manner, which is more helpful  to us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_perceptron_feature_testing(df, model, test_feature):\n",
    "    features = list(df.columns.values)\n",
    "    features.remove('is_Paraphrase')\n",
    "    features.remove('Sentence_1')\n",
    "    features.remove('Sentence_2')\n",
    "    features.remove(test_feature)\n",
    "    X = df[features]\n",
    "    y = df['is_Paraphrase']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()])\n",
    "    model.fit(X_train, y_train, epochs=15, batch_size=1)\n",
    "\n",
    "    test_loss, test_acc, test_pre, test_recall = model.evaluate(X_test, y_test)\n",
    "    print('Test accuracy:{}, test recall: {}, test precision: {}'.format(test_acc, test_recall, test_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quora data NN feature testing \n",
    "features = list(quora_data.columns.values)\n",
    "features.remove('is_Paraphrase')\n",
    "features.remove('Sentence_1')\n",
    "features.remove('Sentence_2')\n",
    "\n",
    "for feature in features:\n",
    "    print(\"Removing feature {}\".format(feature))\n",
    "    model_quora_testing = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(8,)),\n",
    "    keras.layers.Dense(7, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(7, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid),\n",
    "    ])\n",
    "    multilayer_perceptron_feature_testing(quora_data, model_quora_testing, feature)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Microsfot data feature testing\n",
    "features = list(mrp_data.columns.values)\n",
    "features.remove('is_Paraphrase')\n",
    "features.remove('Sentence_1')\n",
    "features.remove('Sentence_2')\n",
    "\n",
    "for feature in features:\n",
    "    print(\"Removing feature {}\".format(feature))\n",
    "    model_mrp_testing = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(8,)),\n",
    "    keras.layers.Dense(7, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(7, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid),\n",
    "    ])\n",
    "    multilayer_perceptron_feature_testing(mrp_data, model_mrp_testing, feature)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Evaluation and Conclusion </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, our baseline model performed better on the Quora data set, while our Neural Network performed better on the Microsoft data set. This could be because the Quora data set contains more noisy data and is closer to day-to-day human language in the sense that it contains more slang and the use of emoji, while Microsoft data set is more formal. Our models performed pretty well capturing similarities, however this is not the same as capturing paraphrases. For example, recall the sentences ‘What is the step by step guide to invest in share market in India?’ vs ‘What is the step by step guide to invest in share market?’ that we saw before. These would have a high similarity as calculated by our input features, however, these would not be considered paraphrases because of the one distinguishing word 'india' that alters the overall context. Additionally, sometimes sentences are paraphrases but they don’t have any words in common, meaning that our input measures would not be able to capture this. We thought that our WMD distance measure would aid in this. However, it is very likely that WMD distance measure on its own would not be able to influence the Neural Network. We hypothesized that syntactic and semantic similarity were both important while predicting if a pair of sentences were paraphrases. We built our features based on this hypothesis, creating both syntactic and semantic features. However, in the future, we may have to consider having more semantic features rather than a balance of both semantic and syntactic features. Furthermore, we only deal with 9 input features for our model when ideally we would use way more, encompassing topic features, linguistic features, and much more.  Lastly, we could increase the number of samples and the way we sample our data. For example, using k-fold validation and balancing our classes through over sampling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> References </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.gotrained.com/nltk-edit-distance-jaccard-distance/ <br>\n",
    "http://www.nltk.org/_modules/nltk/model/ngram.html <br>\n",
    "https://medium.com/@nikhiljaiswal_9475/sequencematcher-in-python-6b1e6f3915fc <br>\n",
    "https://towardsdatascience.com/word-distance-between-word-embeddings-cc3e9cf1d632 <br>\n",
    "https://radimrehurek.com/gensim/models/keyedvectors.html <br>\n",
    "https://stackoverflow.com/questions/31836058/nltk-named-entity-recognition-to-a-python-list/48738383 <br>\n",
    "https://www.kaggle.com/antriksh5235/semantic-similarity-using-wordnet <br>\n",
    "https://pdfs.semanticscholar.org/651e/e5def5cabff3cdf03b6c1a44c00aad9ef527.pdf <br>\n",
    "https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw <br>\n",
    "https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/ <br>\n",
    "http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
