{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h1> Paraphrase Detection with Neural Networks - Natural Language Understanding </h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSI4106 Artificial Intelligence<br/>\n",
    "Project Type 3 : In-depth understanding of a solution approach to an AI problem <br/>\n",
    "Prepared by Abha Sharma (8254435) & Rupsi Kaushik (8199148) <br/>\n",
    "Group 33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Background </h2>\n",
    "\n",
    "With the growing trends of virtual assistants and chatbots, Natural Language Processing (NLP) is a topic that is becoming increasingly popular in the recent years. From Google AI's Transformer-based models that consider a word's double-sided context to IBM's training data generator, today we have cutting edge approaches to solving NLP tasks.  However, even with these latest breakthroughs, NLP still faces many challenges, namely the problem of accurately deciphering what humans mean when they express something, regardless of how they express it. This problem falls under Natural Language Understanding (NLU), a subtopic of NLP that aims to increase the proficiency of intelligent systems in exhibiting real knowledge of natural language. Within this field, the task of paraphrase detection - determining whether a pair of sentences convey identical meaning - is considered to be an important one. Through the improvement of paraphrase detection, other NLP tasks that are integral to the efficiency of existing intelligent systems, such as question answering, information retrieval, and text summarization, can also be improved. For this reason, in this report, we propose to enhance the capability of neural networks in the context of paraphrase detection through the use of traditional Information Retrieval (IR) techniques as input features. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Objectives</h2>\n",
    "\n",
    "The main objective of this report is to evaluate the performance of a neural network model given different IR features. Additionally, it will take a look at how the quality and number of features and hidden layers improve the overall performance of the model. These results will be compared among two different training sets that have been annotated for paraphrase detection. Below is the proposed architecture for our particular neural network: \n",
    "<img src=\"CSI4106-NN.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Datasets </h2>\n",
    "\n",
    "We will be working with the Quora Question Pairs and Microsoft Research Paraphrase Corpus datasets for this project. You can find them in this folder labelled as 'msr_train.csv' and 'questions_train.csv'. Each dataset contains pairs of sentences (Sentence_1 and Sentence_2), which have been annotated by humans to indicate whether these sentences capture a semantic equivalence (is_Paraphrase = 1) or not (is_Paraphrase = 0).\n",
    "\n",
    "The Quora Question Pairs dataset was obtained from: https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs . \n",
    "And\n",
    "the Microsoft Research Corpus dataset was obtained from: https://www.microsoft.com/en-ca/download/details.aspx?id=52398\n",
    "\n",
    "Due to the computational and speed limitations of our machines, we decided to only look at 4500 samples from the Quora Question Pairs dataset. Similarly, although, Microsoft has provided the public with both the train and test dataset, we will only be using the train dataset. This train data set will later be split into train and test sets for our neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure to import all these modules\n",
    "import pandas as pd\n",
    "pd.options.mode.use_inf_as_na = True\n",
    "from pyemd import emd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.wsd import lesk\n",
    "from nltk import ngrams\n",
    "from difflib import SequenceMatcher\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "word_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_1</th>\n",
       "      <th>Sentence_2</th>\n",
       "      <th>is_Paraphrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Should I buy tiago?</td>\n",
       "      <td>What keeps childern active and far from phone ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>What should I do to be a great geologist?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>When do you use シ instead of し?</td>\n",
       "      <td>When do you use \"&amp;\" instead of \"and\"?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Motorola (company): Can I hack my Charter Moto...</td>\n",
       "      <td>How do I hack Motorola DCX3400 for free internet?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Sentence_1  \\\n",
       "0  What is the step by step guide to invest in sh...   \n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2  How can I increase the speed of my internet co...   \n",
       "3  Why am I mentally very lonely? How can I solve...   \n",
       "4  Which one dissolve in water quikly sugar, salt...   \n",
       "5  Astrology: I am a Capricorn Sun Cap moon and c...   \n",
       "6                                Should I buy tiago?   \n",
       "7                     How can I be a good geologist?   \n",
       "8                    When do you use シ instead of し?   \n",
       "9  Motorola (company): Can I hack my Charter Moto...   \n",
       "\n",
       "                                          Sentence_2  is_Paraphrase  \n",
       "0  What is the step by step guide to invest in sh...              0  \n",
       "1  What would happen if the Indian government sto...              0  \n",
       "2  How can Internet speed be increased by hacking...              0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...              0  \n",
       "4            Which fish would survive in salt water?              0  \n",
       "5  I'm a triple Capricorn (Sun, Moon and ascendan...              1  \n",
       "6  What keeps childern active and far from phone ...              0  \n",
       "7          What should I do to be a great geologist?              1  \n",
       "8              When do you use \"&\" instead of \"and\"?              0  \n",
       "9  How do I hack Motorola DCX3400 for free internet?              0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking a look at the Quora Question Pairs dataset\n",
    "quora_data = pd.read_csv(\"questions_train.csv\", error_bad_lines=False)\n",
    "quora_data.Sentence_1 = quora_data.Sentence_1.astype(str)\n",
    "quora_data.Sentence_2 = quora_data.Sentence_2.astype(str)\n",
    "quora_data = quora_data[:4500]\n",
    "quora_data.is_Paraphrase = quora_data.is_Paraphrase.astype(int)\n",
    "quora_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_1</th>\n",
       "      <th>Sentence_2</th>\n",
       "      <th>is_Paraphrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amrozi accused his brother, whom he called \"th...</td>\n",
       "      <td>Referring to him as only \"the witness\", Amrozi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yucaipa owned Dominick's before selling the ch...</td>\n",
       "      <td>Yucaipa bought Dominick's in 1995 for $693 mil...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They had published an advertisement on the Int...</td>\n",
       "      <td>On June 10, the ship's owners had published an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Around 0335 GMT, Tab shares were up 19 cents, ...</td>\n",
       "      <td>Tab shares jumped 20 cents, or 4.6%, to set a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The stock rose $2.11, or about 11 percent, to ...</td>\n",
       "      <td>PG&amp;E Corp. shares jumped $1.63 or 8 percent to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Revenue in the first quarter of the year dropp...</td>\n",
       "      <td>With the scandal hanging over Stewart's compan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Nasdaq had a weekly gain of 17.27, or 1.2 ...</td>\n",
       "      <td>The tech-laced Nasdaq Composite .IXIC rallied ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The DVD-CCA then appealed to the state Supreme...</td>\n",
       "      <td>The DVD CCA appealed that decision to the U.S....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>That compared with $35.18 million, or 24 cents...</td>\n",
       "      <td>Earnings were affected by a non-recurring $8 m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>He said the foodservice pie business doesn't f...</td>\n",
       "      <td>The foodservice pie business does not fit our ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Sentence_1  \\\n",
       "0  Amrozi accused his brother, whom he called \"th...   \n",
       "1  Yucaipa owned Dominick's before selling the ch...   \n",
       "2  They had published an advertisement on the Int...   \n",
       "3  Around 0335 GMT, Tab shares were up 19 cents, ...   \n",
       "4  The stock rose $2.11, or about 11 percent, to ...   \n",
       "5  Revenue in the first quarter of the year dropp...   \n",
       "6  The Nasdaq had a weekly gain of 17.27, or 1.2 ...   \n",
       "7  The DVD-CCA then appealed to the state Supreme...   \n",
       "8  That compared with $35.18 million, or 24 cents...   \n",
       "9  He said the foodservice pie business doesn't f...   \n",
       "\n",
       "                                          Sentence_2  is_Paraphrase  \n",
       "0  Referring to him as only \"the witness\", Amrozi...              1  \n",
       "1  Yucaipa bought Dominick's in 1995 for $693 mil...              0  \n",
       "2  On June 10, the ship's owners had published an...              1  \n",
       "3  Tab shares jumped 20 cents, or 4.6%, to set a ...              0  \n",
       "4  PG&E Corp. shares jumped $1.63 or 8 percent to...              1  \n",
       "5  With the scandal hanging over Stewart's compan...              1  \n",
       "6  The tech-laced Nasdaq Composite .IXIC rallied ...              0  \n",
       "7  The DVD CCA appealed that decision to the U.S....              1  \n",
       "8  Earnings were affected by a non-recurring $8 m...              0  \n",
       "9  The foodservice pie business does not fit our ...              1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Taking a look at the Microsoft Research Paraphrase dataset \n",
    "mrp_data = pd.read_csv(\"msr_train.csv\")\n",
    "mrp_data.Sentence_1 = mrp_data.Sentence_1.astype(str)\n",
    "mrp_data.Sentence_2 = mrp_data.Sentence_2.astype(str)\n",
    "mrp_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Dataset Quality </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both datasets are slightly imbalanced. The Quora dataset contains more of the ‘not a paraphrase’ sample examples while the Microsoft dataset contains more of the ‘is a paraphrase’ sample examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quora Data:\n",
      "Total samples in each class:\n",
      "0    2789\n",
      "1    1711\n",
      "Name: is_Paraphrase, dtype: int64\n",
      "Number of columns:3\n",
      "Number of rows: 4500\n",
      "\n",
      "\n",
      "Mrp Data:\n",
      "Total samples in each class:\n",
      "1    2668\n",
      "0    1281\n",
      "Name: is_Paraphrase, dtype: int64\n",
      "Number of columns: 3\n",
      "Number of rows: 3949\n"
     ]
    }
   ],
   "source": [
    "print(\"Quora Data:\\nTotal samples in each class:\\n{}\".format(quora_data['is_Paraphrase'].value_counts()))\n",
    "print(\"Number of columns:{}\".format(quora_data.shape[1]))\n",
    "print(\"Number of rows: {}\".format(quora_data.shape[0]))\n",
    "print(\"\\n\")\n",
    "print(\"Mrp Data:\\nTotal samples in each class:\\n{}\".format(mrp_data['is_Paraphrase'].value_counts()))\n",
    "print(\"Number of columns: {}\".format(mrp_data.shape[1]))\n",
    "print(\"Number of rows: {}\".format(mrp_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Preprocessing & Transformation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing common words that provide little to no value to semantic information within a sentence.\n",
    "def remove_stop_words(sentence):\n",
    "    stop_words = stopwords.words('english')\n",
    "    processed_sentence = [re.sub(r\"[,.!?&$]+\",'', word) for word in sentence if not word in stop_words]\n",
    "    return processed_sentence     \n",
    "#Tokenize the sentence for further text processing.\n",
    "def tokenize(sentence):\n",
    "    tokenized_sentence = sentence.lower().split()\n",
    "    return tokenized_sentence\n",
    "#Lemmatization captures the root form of a word but ensures that it is a valid word in the language.\n",
    "def lemmatize(sentence):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = [lemmatizer.lemmatize(word) for word in sentence]\n",
    "    return lemmatized_sentence\n",
    "#Gets synonym for a given word. This lets us capture more semantic information than string matching does.\n",
    "def add_synonym(word):\n",
    "    synonym_list = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for name in syn.lemma_names():\n",
    "             synonym_list.append(name.split(\".\")[0].replace('_',' '))\n",
    "    return list(set(synonym_list))\n",
    "\n",
    "'''Gets antonym for a given word. This helps us later when we get sentences like \"I'm not happy.\"\n",
    "    When we detect a negation, we are now able to better capture the semantic value.\n",
    "'''\n",
    "def add_antonym(word):\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            if(lemma.antonyms()):\n",
    "                return lemma.antonyms()[0].name()\n",
    "            else:\n",
    "                return None     \n",
    "#Gets the hypernym (the broader category that a word belongs to) of a given word.(ie, clothing is a hypernym of shirt).  \n",
    "def add_hypernym(word):\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for hypernym in syn.hypernyms():\n",
    "            return hypernym.name().split('.')[0]\n",
    "#Handles negation term and includes antonyms of this given term. \n",
    "def tokenize_negation(sentence):\n",
    "    negation_adverbs = [\"no\", \"without\",\"not\", \"n't\", \"never\", \"neith\", \"nor\"]\n",
    "    tokens_with_negation = []\n",
    "    tokenized_sentence = tokenize(sentence)\n",
    "    i = 0\n",
    "    while i < (len(tokenized_sentence)):\n",
    "        if (i != len(tokenized_sentence)-1) and (tokenized_sentence[i] in negation_adverbs):\n",
    "            negation_token = add_antonym(tokenized_sentence[i+1])\n",
    "            if(negation_token):\n",
    "                tokens_with_negation.append(negation_token)\n",
    "                i += 2 \n",
    "            else:\n",
    "                tokens_with_negation.append(tokenized_sentence[i])\n",
    "                i +=1\n",
    "        else:\n",
    "            tokens_with_negation.append(tokenized_sentence[i])\n",
    "            i += 1\n",
    "    return tokens_with_negation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Baseline Model </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Pairwise Similarity </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Description </h4>\n",
    "\n",
    "Cosine Similarity is a simple IR measure that calculates similarity among pairwise input vectors projected in a multi-dimensional space, based on their cosine angle. In its core, this method is syntactic as it is purely based on common word occurrences/counts and does not take into account word order or other semantic information. However, it is more than enough to capture similarity for a baseline model. *explain why we chose this*. Let's do an example by hand with Quora dataset in order to illustrate this measure. \n",
    "<h4> Illustration </h4>\n",
    "<br /> Sentence_1: \"What is the step by step guide to invest in share market in India?\" <br/> Sentence_2: \"What is the step by step guide to invest in share market?\" <br />\n",
    "After tokenization, stopword removal, and lemmatization, our sentences would look something like this: <br/>\n",
    "Sentence_1: ['step', 'step', 'guide','invest','share','market','india'] <br />\n",
    "Sentence_2: ['step', 'step', 'guide', 'invest', 'share', 'market'] <br />\n",
    "Now we calculate <b>term frequency</b> and <b>inverse document frequency(tf-idf)</b>. Term frequency counts the frequency of word occurrence in each sentence: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Document  step  guide  invest  share  market  india\n",
      "0  Sentence_1     2      1       1      1       1      1\n",
      "1  Sentence_2     2      1       1      1       1      0\n"
     ]
    }
   ],
   "source": [
    "#The number of times a word occurs in each sentence\n",
    "term_document_matrix = {\"Document\": ['Sentence_1', 'Sentence_2'],\n",
    "                   \"step\":[2,2], \n",
    "                   \"guide\":[1,1], \n",
    "                   \"invest\":[1,1],\n",
    "                   \"share\":[1,1],\n",
    "                   \"market\":[1,1],\n",
    "                   \"india\":[1,0]}\n",
    "tdm_df = pd.DataFrame(term_document_matrix)\n",
    "print(tdm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term frequency can be further normalized to take into account sentence length (ie, the tf for 'step' would now be 2/7 for Sentence_1 and 1/3 for Sentence_2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Document  step  guide  invest  share  market  india\n",
      "0  Sentence_1  0.29   0.14    0.14   0.14    0.14   0.14\n",
      "1  Sentence_2  0.33   0.16    0.16   0.16    0.16   0.00\n"
     ]
    }
   ],
   "source": [
    "#Normalized term frequency by accounting for total number of words in each sentence\n",
    "extended_tdm = {\"Document\": ['Sentence_1', 'Sentence_2'],\n",
    "                   \"step\":[0.29, 0.33], \n",
    "                   \"guide\":[0.14,0.16], \n",
    "                   \"invest\":[0.14,0.16],\n",
    "                   \"share\":[0.14,0.16],\n",
    "                   \"market\":[0.14,0.16],\n",
    "                   \"india\":[0.14,0]}\n",
    "extended_tdm_df = pd.DataFrame(extended_tdm)\n",
    "print(extended_tdm_df)                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverse document frequency lets us to put more value on the occurrence of rare terms and put less value on the frequently occurring terms throughout the sentences. This is done to acknowledge that the frequent occurrence of a word like 'step' in both documents is less distinguishing than the occurrence of a word like 'india'. This concept of idf can be captured through the equation: <b> log(N/df(w))</b>, where N is the total number of sentences and df (aka document frequency) is the number of sentences with word w in it. In our case, N is 2 at each comparison. This means that the df at word w in our case is:\n",
    "df(step): 2, df(guide): 2, df(invest): 2, df(share): 2 , df(market): 2 , df(india): 1 <br /> \n",
    "The idf can, thus, be calculated through our equation as: log(2/2) +1 = 1, 1, 1, 1, 1, 1.3, respectively (adding 1 to accommodate for 0's). <br /> \n",
    "Then the tf-idf becomes tf * idf = 1*0.29 = 0.29, 0.14, 0.14, 0.14, 0.14, 0.182 for Sentence_1, and 0.33, 0.16, 0.16, 0.16,0.16, 0 for Sentence_2, respectively. <br />\n",
    "Now we can calculate the cosine similarity through the dot product: <img src=\"cosine.png\"> <br />\n",
    "Where d2 is the tf-idf vector for Sentence_1 and q is the tf-idf vector for Sentence_2 that we previously calculated. <br/>\n",
    " = (0.29 * 0.33 + 0.14 * 0.16 + ...) / (0.29 * 0.29 + 0.14 * 0.14 + ...) (0.33 * 0.33 + 0.16 * 0.16 + ...) <br />\n",
    " = 0.182/0.203 <br />\n",
    " = approx 0.89 or 0.46 radians <br/> \n",
    " Therefore, the cosine similarity of Sentence_1 and Sentence_2 is approximately 0.89, making them highly similar to one another.  <br />\n",
    "This process has been coded below with the help of sklearn in order to define our baseline approach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(sentence_one, sentence_two):\n",
    "    #handles the calculation of tfidf and building of term document matrices for us\n",
    "    tfidf = TfidfVectorizer(preprocessor=' '.join)\n",
    "    tfidf_matrix = tfidf.fit_transform([sentence_one, sentence_two])\n",
    "    #calculates the cosine similarity of the pairwise sentences\n",
    "    similarity = cosine_similarity(tfidf_matrix)[0,1]\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_similarity(row):\n",
    "    sentence_one_tokenize = tokenize_negation(row['Sentence_1'])\n",
    "    sentence_two_tokenize = tokenize_negation(row['Sentence_2'])\n",
    "    filtered_sentence_one = remove_stop_words(sentence_one_tokenize)\n",
    "    filtered_sentence_two = remove_stop_words(sentence_two_tokenize)\n",
    "    lemmatize_sentence_one = lemmatize(filtered_sentence_one)\n",
    "    lemmatize_sentence_two = lemmatize(filtered_sentence_two)\n",
    "    return calculate_cosine_similarity(lemmatize_sentence_one, lemmatize_sentence_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_1</th>\n",
       "      <th>Sentence_2</th>\n",
       "      <th>is_Paraphrase</th>\n",
       "      <th>Cosine_Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.895532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.410995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.225765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.168368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.422795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Should I buy tiago?</td>\n",
       "      <td>What keeps childern active and far from phone ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>What should I do to be a great geologist?</td>\n",
       "      <td>1</td>\n",
       "      <td>0.336097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>When do you use シ instead of し?</td>\n",
       "      <td>When do you use \"&amp;\" instead of \"and\"?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.709297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Motorola (company): Can I hack my Charter Moto...</td>\n",
       "      <td>How do I hack Motorola DCX3400 for free internet?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.380873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Sentence_1  \\\n",
       "0  What is the step by step guide to invest in sh...   \n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2  How can I increase the speed of my internet co...   \n",
       "3  Why am I mentally very lonely? How can I solve...   \n",
       "4  Which one dissolve in water quikly sugar, salt...   \n",
       "5  Astrology: I am a Capricorn Sun Cap moon and c...   \n",
       "6                                Should I buy tiago?   \n",
       "7                     How can I be a good geologist?   \n",
       "8                    When do you use シ instead of し?   \n",
       "9  Motorola (company): Can I hack my Charter Moto...   \n",
       "\n",
       "                                          Sentence_2  is_Paraphrase  \\\n",
       "0  What is the step by step guide to invest in sh...              0   \n",
       "1  What would happen if the Indian government sto...              0   \n",
       "2  How can Internet speed be increased by hacking...              0   \n",
       "3  Find the remainder when [math]23^{24}[/math] i...              0   \n",
       "4            Which fish would survive in salt water?              0   \n",
       "5  I'm a triple Capricorn (Sun, Moon and ascendan...              1   \n",
       "6  What keeps childern active and far from phone ...              0   \n",
       "7          What should I do to be a great geologist?              1   \n",
       "8              When do you use \"&\" instead of \"and\"?              0   \n",
       "9  How do I hack Motorola DCX3400 for free internet?              0   \n",
       "\n",
       "   Cosine_Similarity  \n",
       "0           0.895532  \n",
       "1           0.410995  \n",
       "2           0.225765  \n",
       "3           0.000000  \n",
       "4           0.168368  \n",
       "5           0.422795  \n",
       "6           0.000000  \n",
       "7           0.336097  \n",
       "8           0.709297  \n",
       "9           0.380873  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying Cosine Similaritye to Quora sentence pairs \n",
    "quora_data['Cosine_Similarity'] = quora_data.apply(get_cosine_similarity, axis=1)\n",
    "quora_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_1</th>\n",
       "      <th>Sentence_2</th>\n",
       "      <th>is_Paraphrase</th>\n",
       "      <th>Cosine_Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amrozi accused his brother, whom he called \"th...</td>\n",
       "      <td>Referring to him as only \"the witness\", Amrozi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.801978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yucaipa owned Dominick's before selling the ch...</td>\n",
       "      <td>Yucaipa bought Dominick's in 1995 for $693 mil...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.339099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They had published an advertisement on the Int...</td>\n",
       "      <td>On June 10, the ship's owners had published an...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.588364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Around 0335 GMT, Tab shares were up 19 cents, ...</td>\n",
       "      <td>Tab shares jumped 20 cents, or 4.6%, to set a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.397346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The stock rose $2.11, or about 11 percent, to ...</td>\n",
       "      <td>PG&amp;E Corp. shares jumped $1.63 or 8 percent to...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.381322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Revenue in the first quarter of the year dropp...</td>\n",
       "      <td>With the scandal hanging over Stewart's compan...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.776515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Nasdaq had a weekly gain of 17.27, or 1.2 ...</td>\n",
       "      <td>The tech-laced Nasdaq Composite .IXIC rallied ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.179523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The DVD-CCA then appealed to the state Supreme...</td>\n",
       "      <td>The DVD CCA appealed that decision to the U.S....</td>\n",
       "      <td>1</td>\n",
       "      <td>0.716812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>That compared with $35.18 million, or 24 cents...</td>\n",
       "      <td>Earnings were affected by a non-recurring $8 m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.252334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>He said the foodservice pie business doesn't f...</td>\n",
       "      <td>The foodservice pie business does not fit our ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.818180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Sentence_1  \\\n",
       "0  Amrozi accused his brother, whom he called \"th...   \n",
       "1  Yucaipa owned Dominick's before selling the ch...   \n",
       "2  They had published an advertisement on the Int...   \n",
       "3  Around 0335 GMT, Tab shares were up 19 cents, ...   \n",
       "4  The stock rose $2.11, or about 11 percent, to ...   \n",
       "5  Revenue in the first quarter of the year dropp...   \n",
       "6  The Nasdaq had a weekly gain of 17.27, or 1.2 ...   \n",
       "7  The DVD-CCA then appealed to the state Supreme...   \n",
       "8  That compared with $35.18 million, or 24 cents...   \n",
       "9  He said the foodservice pie business doesn't f...   \n",
       "\n",
       "                                          Sentence_2  is_Paraphrase  \\\n",
       "0  Referring to him as only \"the witness\", Amrozi...              1   \n",
       "1  Yucaipa bought Dominick's in 1995 for $693 mil...              0   \n",
       "2  On June 10, the ship's owners had published an...              1   \n",
       "3  Tab shares jumped 20 cents, or 4.6%, to set a ...              0   \n",
       "4  PG&E Corp. shares jumped $1.63 or 8 percent to...              1   \n",
       "5  With the scandal hanging over Stewart's compan...              1   \n",
       "6  The tech-laced Nasdaq Composite .IXIC rallied ...              0   \n",
       "7  The DVD CCA appealed that decision to the U.S....              1   \n",
       "8  Earnings were affected by a non-recurring $8 m...              0   \n",
       "9  The foodservice pie business does not fit our ...              1   \n",
       "\n",
       "   Cosine_Similarity  \n",
       "0           0.801978  \n",
       "1           0.339099  \n",
       "2           0.588364  \n",
       "3           0.397346  \n",
       "4           0.381322  \n",
       "5           0.776515  \n",
       "6           0.179523  \n",
       "7           0.716812  \n",
       "8           0.252334  \n",
       "9           0.818180  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying Cosine Similarity Coefficient to MRP Corpus sentence pairs \n",
    "mrp_data['Cosine_Similarity'] = mrp_data.apply(get_cosine_similarity, axis=1)\n",
    "mrp_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Evaluation Measures </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wil be using accuracy, recall, and precision in order to evaluate our models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate how many were right predictions out of the total predicted\n",
    "def calculate_accuracy(model, actual_tags):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for prediction, actual in zip(model, actual_tags):\n",
    "        total += 1\n",
    "        if prediction == actual:\n",
    "            correct += 1 \n",
    "    accuracy = correct / total \n",
    "    return '{0:.1%}'.format(accuracy)\n",
    "#Keeps track of true positives, true negatives, false positives, and false negatives.\n",
    "def build_confusion_matrix(actual_tags, model, classOfInterest):\n",
    "    confusion_matrix = {}\n",
    "    truePositives = len([p for p, a in zip(model, actual_tags) if p == a and p == classOfInterest])\n",
    "    trueNegatives = len([p for p, a in zip(model, actual_tags) if p == a and p != classOfInterest])\n",
    "    falsePositives = len([p for p, a in zip(model, actual_tags) if p != a and p == classOfInterest])\n",
    "    falseNegatives = len([p for p, a in zip(model, actual_tags) if p != a and p != classOfInterest])\n",
    "    confusion_matrix[\"tp\"] = truePositives\n",
    "    confusion_matrix[\"tn\"] = trueNegatives\n",
    "    confusion_matrix[\"fp\"] = falsePositives \n",
    "    confusion_matrix[\"fn\"] = falseNegatives\n",
    "    return confusion_matrix\n",
    "#Calculate how many relevant predictions are retrieved in general\n",
    "def calculate_recall(model, actual_tags, classOfInterest):\n",
    "    matrix = build_confusion_matrix(model, actual_tags, classOfInterest)\n",
    "    recall = matrix[\"tp\"] / ( matrix[\"tp\"] + matrix [\"fn\"])\n",
    "    return '{0:.1%}'.format(recall)\n",
    "#Calculate how many retrieved predictions are relevant\n",
    "def calculate_precision(model, actual_tags, classOfInterest):\n",
    "    matrix = build_confusion_matrix(model, actual_tags, classOfInterest)\n",
    "    precision = matrix[\"tp\"]/ (matrix[\"tp\"] + matrix[\"fp\"])\n",
    "    return '{0:.1%}'.format(precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Baseline Model Threshold </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to decide what similarity value is adequate to determine whether a pair of sentences are semantically equivalent. Are we going to accept them as a paraphrase if the measure is above 0.5? Above 0.8? Instead of randomly picking a threshold for the baseline method, we are going to 'learn' a threshold that yields relatively best results for us in terms of accuracy, recall, and precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quora Accuracy: {0.1: '49.7%', 0.15: '54.0%', 0.2: '58.9%', 0.25: '62.6%', 0.3: '65.4%', 0.35: '67.0%', 0.4: '67.5%', 0.45: '67.8%', 0.5: '67.6%', 0.55: '66.4%', 0.6: '65.8%', 0.65: '66.0%', 0.7: '65.6%', 0.75: '65.5%', 0.8: '65.4%', 0.85: '65.2%', 0.9: '65.4%'}, \n",
      " Quora Recall: {0.1: '43.1%', 0.15: '45.3%', 0.2: '48.0%', 0.25: '50.4%', 0.3: '52.7%', 0.35: '54.5%', 0.4: '55.4%', 0.45: '56.6%', 0.5: '56.7%', 0.55: '56.6%', 0.6: '56.6%', 0.65: '58.2%', 0.7: '58.7%', 0.75: '61.4%', 0.8: '66.2%', 0.85: '71.6%', 0.9: '75.9%'},\n",
      "Quora Precision: {0.1: '100.0%', 0.15: '99.6%', 0.2: '98.7%', 0.25: '95.1%', 0.3: '88.3%', 0.35: '80.3%', 0.4: '75.4%', 0.45: '66.5%', 0.5: '62.4%', 0.55: '49.5%', 0.6: '42.8%', 0.65: '37.4%', 0.7: '32.1%', 0.75: '24.8%', 0.8: '18.5%', 0.85: '13.9%', 0.9: '13.0%'}\n",
      "MRP Accuracy: {0.1: '67.4%', 0.15: '67.9%', 0.2: '68.7%', 0.25: '69.1%', 0.3: '69.6%', 0.35: '70.2%', 0.4: '70.9%', 0.45: '69.5%', 0.5: '67.1%', 0.55: '63.8%', 0.6: '59.3%', 0.65: '53.8%', 0.7: '49.3%', 0.75: '43.4%', 0.8: '39.3%', 0.85: '36.2%', 0.9: '34.4%'}, \n",
      "MRP Recall: {0.1: '67.6%', 0.15: '67.9%', 0.2: '68.6%', 0.25: '69.3%', 0.3: '70.7%', 0.35: '73.1%', 0.4: '76.2%', 0.45: '78.9%', 0.5: '81.5%', 0.55: '85.3%', 0.6: '88.3%', 0.65: '89.9%', 0.7: '91.6%', 0.75: '94.5%', 0.8: '95.3%', 0.85: '98.7%', 0.9: '98.8%'},\n",
      "MRP Precision: {0.1: '99.4%', 0.15: '99.4%', 0.2: '98.9%', 0.25: '97.6%', 0.3: '93.9%', 0.35: '88.5%', 0.4: '82.8%', 0.45: '75.0%', 0.5: '66.3%', 0.55: '56.0%', 0.6: '45.9%', 0.65: '35.5%', 0.7: '27.5%', 0.75: '17.3%', 0.8: '10.6%', 0.85: '5.7%', 0.9: '3.0%'}\n"
     ]
    }
   ],
   "source": [
    "def apply_cosine_classification(row, **kwargs):\n",
    "    if(row['Cosine_Similarity'] > kwargs['threshold']):\n",
    "        classification = 1\n",
    "    else:\n",
    "        classification = 0\n",
    "    return classification\n",
    "\n",
    "thresholds = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9]\n",
    "quora_data_thresholds = {}\n",
    "mrp_data_thresholds = {}\n",
    "accuracies = {}\n",
    "recalls = {}\n",
    "precisions = {}\n",
    "for threshold in thresholds:\n",
    "    quora_data_thresholds[threshold] = quora_data.apply(apply_cosine_classification, threshold = threshold, axis = 1)\n",
    "    accuracies[threshold] = calculate_accuracy(quora_data_thresholds[threshold], quora_data['is_Paraphrase'])\n",
    "    recalls[threshold] = calculate_recall(quora_data_thresholds[threshold], quora_data['is_Paraphrase'], 1)\n",
    "    precisions[threshold] = calculate_precision(quora_data_thresholds[threshold], quora_data['is_Paraphrase'], 1)\n",
    "print(\"Quora Accuracy: {}, \\n Quora Recall: {},\\nQuora Precision: {}\" .format(accuracies, recalls, precisions))\n",
    "\n",
    "for threshold in thresholds:\n",
    "    mrp_data_thresholds[threshold] = mrp_data.apply(apply_cosine_classification, threshold = threshold, axis = 1)\n",
    "    accuracies[threshold] = calculate_accuracy(mrp_data_thresholds[threshold], mrp_data['is_Paraphrase'])\n",
    "    recalls[threshold] = calculate_recall(mrp_data_thresholds[threshold], mrp_data['is_Paraphrase'],1)\n",
    "    precisions[threshold] = calculate_precision(mrp_data_thresholds[threshold], mrp_data['is_Paraphrase'], 1)\n",
    "print(\"MRP Accuracy: {}, \\nMRP Recall: {},\\nMRP Precision: {}\" .format(accuracies, recalls, precisions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Picking a Threshold </h4>\n",
    "We chose 0.45 as our threshold. We take into account all three metrics and the results of both datasets but put a higher emphasis on accuracy and precision. A high accuracy can indicate that the model is great, however, all the other parameters must be taken into account. We might have higher accuracies at other thresholds but taking a look at the confusion matrix at each threshold, there is an imbalance of classes at these thresholds with higher accuracies, which is not necessarily good. For example, let’s say our dataset has 99% of one class and 1% of the other, then our accuracy is pretty high but this  clearly shows that there is equal cost to false positives and false negatives.  Therefore, metrics need to also be relative to each class. The precision at 0.45 is overall decent for both datasets, indicating that at this threshold the specificity is pretty good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Cosine_Classification\n",
      "0                         1\n",
      "1                         0\n",
      "2                         0\n",
      "3                         0\n",
      "4                         0\n",
      "...                     ...\n",
      "4495                      1\n",
      "4496                      0\n",
      "4497                      0\n",
      "4498                      1\n",
      "4499                      1\n",
      "\n",
      "[4500 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "quora_baseline = {}\n",
    "quora_baseline['Cosine_Classification'] = quora_data.apply(apply_cosine_classification, threshold=0.45, axis=1)\n",
    "quora_baseline_df = pd.DataFrame(quora_baseline)\n",
    "print(quora_baseline_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Cosine_Classification\n",
      "0                         1\n",
      "1                         0\n",
      "2                         1\n",
      "3                         0\n",
      "4                         0\n",
      "...                     ...\n",
      "3944                      0\n",
      "3945                      1\n",
      "3946                      1\n",
      "3947                      0\n",
      "3948                      1\n",
      "\n",
      "[3949 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "mrp_baseline = {}\n",
    "mrp_baseline['Cosine_Classification'] = mrp_data.apply(apply_cosine_classification, threshold = 0.45,  axis=1)\n",
    "mrp_baseline_df = pd.DataFrame(mrp_baseline)\n",
    "print(mrp_baseline_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Measures and Results </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_evaluation_summary(baseline, df):\n",
    "    baseline_accuracy = calculate_accuracy(baseline['Cosine_Classification'], df['is_Paraphrase'])\n",
    "    baseline_recall = calculate_recall(baseline['Cosine_Classification'], df['is_Paraphrase'], 1)\n",
    "    baseline_precision = calculate_precision(baseline['Cosine_Classification'], df['is_Paraphrase'], 1)\n",
    "    \n",
    "    baseline_evaluation_summary = {\"Model\": ['Baseline'],\n",
    "                   \"Accuracy\":[(baseline_accuracy)], \n",
    "                   \"Recall\":[baseline_recall], \n",
    "                   \"Precision\":[baseline_precision]}\n",
    "    results_df = pd.DataFrame(baseline_evaluation_summary)\n",
    "    print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quora Data Baseline Evaluation\n",
      "      Model Accuracy Recall Precision\n",
      "0  Baseline    67.8%  56.6%     66.5%\n",
      "Microsoft Data Baseline Evaluation\n",
      "      Model Accuracy Recall Precision\n",
      "0  Baseline    69.5%  78.9%     75.0%\n"
     ]
    }
   ],
   "source": [
    "print('Quora Data Baseline Evaluation')\n",
    "baseline_evaluation_summary(quora_baseline, quora_data)\n",
    "print('Microsoft Data Baseline Evaluation')\n",
    "baseline_evaluation_summary(mrp_baseline, mrp_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Feature Engineering</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Syntactic Similarity</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Edit Distance</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit distance is a measure of similarity between two strings, source and target. It is the minimum number of operations (insertion, deletion, or substitution) required to transform the source string to target string. For example, the edit distance between 'monkey' and 'money' is 1. Deletion of 'k' in 'monkey' will give us 'money'.\n",
    "Here, we have implemented such edit distance but on the word level instead of character level. This was done by transforming the sentences to list and then comparing each element of the source and the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_distance(row):\n",
    "    sentence_one_tokenize = tokenize(row['Sentence_1'])\n",
    "    sentence_two_tokenize = tokenize(row['Sentence_2'])\n",
    "    return nltk.edit_distance(sentence_one_tokenize, sentence_two_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_1</th>\n",
       "      <th>Sentence_2</th>\n",
       "      <th>is_Paraphrase</th>\n",
       "      <th>Cosine_Similarity</th>\n",
       "      <th>Edit_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.895532</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.410995</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.225765</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.168368</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.422795</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Should I buy tiago?</td>\n",
       "      <td>What keeps childern active and far from phone ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>What should I do to be a great geologist?</td>\n",
       "      <td>1</td>\n",
       "      <td>0.336097</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>When do you use シ instead of し?</td>\n",
       "      <td>When do you use \"&amp;\" instead of \"and\"?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.709297</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Motorola (company): Can I hack my Charter Moto...</td>\n",
       "      <td>How do I hack Motorola DCX3400 for free internet?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.380873</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Sentence_1  \\\n",
       "0  What is the step by step guide to invest in sh...   \n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2  How can I increase the speed of my internet co...   \n",
       "3  Why am I mentally very lonely? How can I solve...   \n",
       "4  Which one dissolve in water quikly sugar, salt...   \n",
       "5  Astrology: I am a Capricorn Sun Cap moon and c...   \n",
       "6                                Should I buy tiago?   \n",
       "7                     How can I be a good geologist?   \n",
       "8                    When do you use シ instead of し?   \n",
       "9  Motorola (company): Can I hack my Charter Moto...   \n",
       "\n",
       "                                          Sentence_2  is_Paraphrase  \\\n",
       "0  What is the step by step guide to invest in sh...              0   \n",
       "1  What would happen if the Indian government sto...              0   \n",
       "2  How can Internet speed be increased by hacking...              0   \n",
       "3  Find the remainder when [math]23^{24}[/math] i...              0   \n",
       "4            Which fish would survive in salt water?              0   \n",
       "5  I'm a triple Capricorn (Sun, Moon and ascendan...              1   \n",
       "6  What keeps childern active and far from phone ...              0   \n",
       "7          What should I do to be a great geologist?              1   \n",
       "8              When do you use \"&\" instead of \"and\"?              0   \n",
       "9  How do I hack Motorola DCX3400 for free internet?              0   \n",
       "\n",
       "   Cosine_Similarity  Edit_distance  \n",
       "0           0.895532              3  \n",
       "1           0.410995              9  \n",
       "2           0.225765             11  \n",
       "3           0.000000             11  \n",
       "4           0.168368             12  \n",
       "5           0.422795             11  \n",
       "6           0.000000             11  \n",
       "7           0.336097              5  \n",
       "8           0.709297              2  \n",
       "9           0.380873              8  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying Edit Distance to Quora sentence pairs \n",
    "quora_data['Edit_distance'] = quora_data.apply(edit_distance, axis=1)\n",
    "quora_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_1</th>\n",
       "      <th>Sentence_2</th>\n",
       "      <th>is_Paraphrase</th>\n",
       "      <th>Cosine_Similarity</th>\n",
       "      <th>Edit_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amrozi accused his brother, whom he called \"th...</td>\n",
       "      <td>Referring to him as only \"the witness\", Amrozi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.801978</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yucaipa owned Dominick's before selling the ch...</td>\n",
       "      <td>Yucaipa bought Dominick's in 1995 for $693 mil...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.339099</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They had published an advertisement on the Int...</td>\n",
       "      <td>On June 10, the ship's owners had published an...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.588364</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Around 0335 GMT, Tab shares were up 19 cents, ...</td>\n",
       "      <td>Tab shares jumped 20 cents, or 4.6%, to set a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.397346</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The stock rose $2.11, or about 11 percent, to ...</td>\n",
       "      <td>PG&amp;E Corp. shares jumped $1.63 or 8 percent to...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.381322</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Revenue in the first quarter of the year dropp...</td>\n",
       "      <td>With the scandal hanging over Stewart's compan...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.776515</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Nasdaq had a weekly gain of 17.27, or 1.2 ...</td>\n",
       "      <td>The tech-laced Nasdaq Composite .IXIC rallied ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.179523</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The DVD-CCA then appealed to the state Supreme...</td>\n",
       "      <td>The DVD CCA appealed that decision to the U.S....</td>\n",
       "      <td>1</td>\n",
       "      <td>0.716812</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>That compared with $35.18 million, or 24 cents...</td>\n",
       "      <td>Earnings were affected by a non-recurring $8 m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.252334</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>He said the foodservice pie business doesn't f...</td>\n",
       "      <td>The foodservice pie business does not fit our ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.818180</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Sentence_1  \\\n",
       "0  Amrozi accused his brother, whom he called \"th...   \n",
       "1  Yucaipa owned Dominick's before selling the ch...   \n",
       "2  They had published an advertisement on the Int...   \n",
       "3  Around 0335 GMT, Tab shares were up 19 cents, ...   \n",
       "4  The stock rose $2.11, or about 11 percent, to ...   \n",
       "5  Revenue in the first quarter of the year dropp...   \n",
       "6  The Nasdaq had a weekly gain of 17.27, or 1.2 ...   \n",
       "7  The DVD-CCA then appealed to the state Supreme...   \n",
       "8  That compared with $35.18 million, or 24 cents...   \n",
       "9  He said the foodservice pie business doesn't f...   \n",
       "\n",
       "                                          Sentence_2  is_Paraphrase  \\\n",
       "0  Referring to him as only \"the witness\", Amrozi...              1   \n",
       "1  Yucaipa bought Dominick's in 1995 for $693 mil...              0   \n",
       "2  On June 10, the ship's owners had published an...              1   \n",
       "3  Tab shares jumped 20 cents, or 4.6%, to set a ...              0   \n",
       "4  PG&E Corp. shares jumped $1.63 or 8 percent to...              1   \n",
       "5  With the scandal hanging over Stewart's compan...              1   \n",
       "6  The tech-laced Nasdaq Composite .IXIC rallied ...              0   \n",
       "7  The DVD CCA appealed that decision to the U.S....              1   \n",
       "8  Earnings were affected by a non-recurring $8 m...              0   \n",
       "9  The foodservice pie business does not fit our ...              1   \n",
       "\n",
       "   Cosine_Similarity  Edit_distance  \n",
       "0           0.801978             11  \n",
       "1           0.339099             14  \n",
       "2           0.588364             14  \n",
       "3           0.397346             13  \n",
       "4           0.381322             15  \n",
       "5           0.776515              8  \n",
       "6           0.179523             13  \n",
       "7           0.716812              5  \n",
       "8           0.252334             10  \n",
       "9           0.818180              6  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying Edit Distance to MRP Corpus sentence pairs \n",
    "mrp_data['Edit_distance'] = mrp_data.apply(edit_distance, axis=1)\n",
    "mrp_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Jaccard Similarity Coefficient</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jaccard Similarity Coefficient is another similarity measure. It is calculated by dividing the intersection (common words) of the two sentences over the union of the two sentences(length of sentence one + length of sentence two - intersection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_sim_coefficient(row):\n",
    "    sentence_one_tokenize = tokenize(row['Sentence_1'])\n",
    "    sentence_two_tokenize = tokenize(row['Sentence_2'])\n",
    "    intersection = 0\n",
    "    for word_in_one in sentence_one_tokenize:\n",
    "        if word_in_one in sentence_two_tokenize:\n",
    "            intersection += 1\n",
    "    union = (len(sentence_one_tokenize) + len(sentence_two_tokenize) - intersection)\n",
    "    return intersection/union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_1</th>\n",
       "      <th>Sentence_2</th>\n",
       "      <th>is_Paraphrase</th>\n",
       "      <th>Cosine_Similarity</th>\n",
       "      <th>Edit_distance</th>\n",
       "      <th>Jaccard_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.895532</td>\n",
       "      <td>3</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.410995</td>\n",
       "      <td>9</td>\n",
       "      <td>0.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.225765</td>\n",
       "      <td>11</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.168368</td>\n",
       "      <td>12</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.422795</td>\n",
       "      <td>11</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Should I buy tiago?</td>\n",
       "      <td>What keeps childern active and far from phone ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>What should I do to be a great geologist?</td>\n",
       "      <td>1</td>\n",
       "      <td>0.336097</td>\n",
       "      <td>5</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>When do you use シ instead of し?</td>\n",
       "      <td>When do you use \"&amp;\" instead of \"and\"?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.709297</td>\n",
       "      <td>2</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Motorola (company): Can I hack my Charter Moto...</td>\n",
       "      <td>How do I hack Motorola DCX3400 for free internet?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.380873</td>\n",
       "      <td>8</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Sentence_1  \\\n",
       "0  What is the step by step guide to invest in sh...   \n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2  How can I increase the speed of my internet co...   \n",
       "3  Why am I mentally very lonely? How can I solve...   \n",
       "4  Which one dissolve in water quikly sugar, salt...   \n",
       "5  Astrology: I am a Capricorn Sun Cap moon and c...   \n",
       "6                                Should I buy tiago?   \n",
       "7                     How can I be a good geologist?   \n",
       "8                    When do you use シ instead of し?   \n",
       "9  Motorola (company): Can I hack my Charter Moto...   \n",
       "\n",
       "                                          Sentence_2  is_Paraphrase  \\\n",
       "0  What is the step by step guide to invest in sh...              0   \n",
       "1  What would happen if the Indian government sto...              0   \n",
       "2  How can Internet speed be increased by hacking...              0   \n",
       "3  Find the remainder when [math]23^{24}[/math] i...              0   \n",
       "4            Which fish would survive in salt water?              0   \n",
       "5  I'm a triple Capricorn (Sun, Moon and ascendan...              1   \n",
       "6  What keeps childern active and far from phone ...              0   \n",
       "7          What should I do to be a great geologist?              1   \n",
       "8              When do you use \"&\" instead of \"and\"?              0   \n",
       "9  How do I hack Motorola DCX3400 for free internet?              0   \n",
       "\n",
       "   Cosine_Similarity  Edit_distance  Jaccard_similarity  \n",
       "0           0.895532              3            0.857143  \n",
       "1           0.410995              9            0.235294  \n",
       "2           0.225765             11            0.200000  \n",
       "3           0.000000             11            0.000000  \n",
       "4           0.168368             12            0.111111  \n",
       "5           0.422795             11            0.333333  \n",
       "6           0.000000             11            0.000000  \n",
       "7           0.336097              5            0.333333  \n",
       "8           0.709297              2            0.600000  \n",
       "9           0.380873              8            0.200000  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying Jaccard Similarity Coefficient to Quora sentence pairs \n",
    "quora_data['Jaccard_similarity'] = quora_data.apply(jaccard_sim_coefficient, axis=1)\n",
    "quora_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_1</th>\n",
       "      <th>Sentence_2</th>\n",
       "      <th>is_Paraphrase</th>\n",
       "      <th>Cosine_Similarity</th>\n",
       "      <th>Edit_distance</th>\n",
       "      <th>Jaccard_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amrozi accused his brother, whom he called \"th...</td>\n",
       "      <td>Referring to him as only \"the witness\", Amrozi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.801978</td>\n",
       "      <td>11</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yucaipa owned Dominick's before selling the ch...</td>\n",
       "      <td>Yucaipa bought Dominick's in 1995 for $693 mil...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.339099</td>\n",
       "      <td>14</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They had published an advertisement on the Int...</td>\n",
       "      <td>On June 10, the ship's owners had published an...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.588364</td>\n",
       "      <td>14</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Around 0335 GMT, Tab shares were up 19 cents, ...</td>\n",
       "      <td>Tab shares jumped 20 cents, or 4.6%, to set a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.397346</td>\n",
       "      <td>13</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The stock rose $2.11, or about 11 percent, to ...</td>\n",
       "      <td>PG&amp;E Corp. shares jumped $1.63 or 8 percent to...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.381322</td>\n",
       "      <td>15</td>\n",
       "      <td>0.321429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Revenue in the first quarter of the year dropp...</td>\n",
       "      <td>With the scandal hanging over Stewart's compan...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.776515</td>\n",
       "      <td>8</td>\n",
       "      <td>0.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Nasdaq had a weekly gain of 17.27, or 1.2 ...</td>\n",
       "      <td>The tech-laced Nasdaq Composite .IXIC rallied ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.179523</td>\n",
       "      <td>13</td>\n",
       "      <td>0.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The DVD-CCA then appealed to the state Supreme...</td>\n",
       "      <td>The DVD CCA appealed that decision to the U.S....</td>\n",
       "      <td>1</td>\n",
       "      <td>0.716812</td>\n",
       "      <td>5</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>That compared with $35.18 million, or 24 cents...</td>\n",
       "      <td>Earnings were affected by a non-recurring $8 m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.252334</td>\n",
       "      <td>10</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>He said the foodservice pie business doesn't f...</td>\n",
       "      <td>The foodservice pie business does not fit our ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.818180</td>\n",
       "      <td>6</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Sentence_1  \\\n",
       "0  Amrozi accused his brother, whom he called \"th...   \n",
       "1  Yucaipa owned Dominick's before selling the ch...   \n",
       "2  They had published an advertisement on the Int...   \n",
       "3  Around 0335 GMT, Tab shares were up 19 cents, ...   \n",
       "4  The stock rose $2.11, or about 11 percent, to ...   \n",
       "5  Revenue in the first quarter of the year dropp...   \n",
       "6  The Nasdaq had a weekly gain of 17.27, or 1.2 ...   \n",
       "7  The DVD-CCA then appealed to the state Supreme...   \n",
       "8  That compared with $35.18 million, or 24 cents...   \n",
       "9  He said the foodservice pie business doesn't f...   \n",
       "\n",
       "                                          Sentence_2  is_Paraphrase  \\\n",
       "0  Referring to him as only \"the witness\", Amrozi...              1   \n",
       "1  Yucaipa bought Dominick's in 1995 for $693 mil...              0   \n",
       "2  On June 10, the ship's owners had published an...              1   \n",
       "3  Tab shares jumped 20 cents, or 4.6%, to set a ...              0   \n",
       "4  PG&E Corp. shares jumped $1.63 or 8 percent to...              1   \n",
       "5  With the scandal hanging over Stewart's compan...              1   \n",
       "6  The tech-laced Nasdaq Composite .IXIC rallied ...              0   \n",
       "7  The DVD CCA appealed that decision to the U.S....              1   \n",
       "8  Earnings were affected by a non-recurring $8 m...              0   \n",
       "9  The foodservice pie business does not fit our ...              1   \n",
       "\n",
       "   Cosine_Similarity  Edit_distance  Jaccard_similarity  \n",
       "0           0.801978             11            0.500000  \n",
       "1           0.339099             14            0.230769  \n",
       "2           0.588364             14            0.500000  \n",
       "3           0.397346             13            0.384615  \n",
       "4           0.381322             15            0.321429  \n",
       "5           0.776515              8            0.680000  \n",
       "6           0.179523             13            0.160000  \n",
       "7           0.716812              5            0.428571  \n",
       "8           0.252334             10            0.166667  \n",
       "9           0.818180              6            0.600000  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying Jaccard Similarity Coefficient to MRP Corpus sentence pairs \n",
    "mrp_data['Jaccard_similarity'] = mrp_data.apply(jaccard_sim_coefficient, axis=1)\n",
    "mrp_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Sequence Matcher</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SequenceMatcher is a class in the python module difflib. It finds the length of the longest contiguous matching subsequence. The ratio then divides it by the total length of characeters of both sentences and multiplies it by 2. This returns the similarity score (float in [0,1]). For example, <br>\n",
    "[THANK]S[ FOR][ RESPONSE]and [THANK]ING[ FOR] KIND[ RESPONSE] has 18 characters in the longest subsequence, including spaces. Therefore, the ratio will output 0.8 (2*18/45)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_matcher(row):\n",
    "    return SequenceMatcher(None, row['Sentence_1'], row['Sentence_2']).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_1</th>\n",
       "      <th>Sentence_2</th>\n",
       "      <th>is_Paraphrase</th>\n",
       "      <th>Cosine_Similarity</th>\n",
       "      <th>Edit_distance</th>\n",
       "      <th>Jaccard_similarity</th>\n",
       "      <th>Sequence_matcher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.895532</td>\n",
       "      <td>3</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.926829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.410995</td>\n",
       "      <td>9</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.647482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.225765</td>\n",
       "      <td>11</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.454545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.168368</td>\n",
       "      <td>12</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.365217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.422795</td>\n",
       "      <td>11</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.659091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Should I buy tiago?</td>\n",
       "      <td>What keeps childern active and far from phone ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.172840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>What should I do to be a great geologist?</td>\n",
       "      <td>1</td>\n",
       "      <td>0.336097</td>\n",
       "      <td>5</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.591549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>When do you use シ instead of し?</td>\n",
       "      <td>When do you use \"&amp;\" instead of \"and\"?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.709297</td>\n",
       "      <td>2</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.852941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Motorola (company): Can I hack my Charter Moto...</td>\n",
       "      <td>How do I hack Motorola DCX3400 for free internet?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.380873</td>\n",
       "      <td>8</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.495413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Sentence_1  \\\n",
       "0  What is the step by step guide to invest in sh...   \n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2  How can I increase the speed of my internet co...   \n",
       "3  Why am I mentally very lonely? How can I solve...   \n",
       "4  Which one dissolve in water quikly sugar, salt...   \n",
       "5  Astrology: I am a Capricorn Sun Cap moon and c...   \n",
       "6                                Should I buy tiago?   \n",
       "7                     How can I be a good geologist?   \n",
       "8                    When do you use シ instead of し?   \n",
       "9  Motorola (company): Can I hack my Charter Moto...   \n",
       "\n",
       "                                          Sentence_2  is_Paraphrase  \\\n",
       "0  What is the step by step guide to invest in sh...              0   \n",
       "1  What would happen if the Indian government sto...              0   \n",
       "2  How can Internet speed be increased by hacking...              0   \n",
       "3  Find the remainder when [math]23^{24}[/math] i...              0   \n",
       "4            Which fish would survive in salt water?              0   \n",
       "5  I'm a triple Capricorn (Sun, Moon and ascendan...              1   \n",
       "6  What keeps childern active and far from phone ...              0   \n",
       "7          What should I do to be a great geologist?              1   \n",
       "8              When do you use \"&\" instead of \"and\"?              0   \n",
       "9  How do I hack Motorola DCX3400 for free internet?              0   \n",
       "\n",
       "   Cosine_Similarity  Edit_distance  Jaccard_similarity  Sequence_matcher  \n",
       "0           0.895532              3            0.857143          0.926829  \n",
       "1           0.410995              9            0.235294          0.647482  \n",
       "2           0.225765             11            0.200000          0.454545  \n",
       "3           0.000000             11            0.000000          0.069565  \n",
       "4           0.168368             12            0.111111          0.365217  \n",
       "5           0.422795             11            0.333333          0.659091  \n",
       "6           0.000000             11            0.000000          0.172840  \n",
       "7           0.336097              5            0.333333          0.591549  \n",
       "8           0.709297              2            0.600000          0.852941  \n",
       "9           0.380873              8            0.200000          0.495413  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying Sequence Matcher to Quora sentence pairs \n",
    "quora_data['Sequence_matcher'] = quora_data.apply(sequence_matcher, axis=1)\n",
    "quora_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_1</th>\n",
       "      <th>Sentence_2</th>\n",
       "      <th>is_Paraphrase</th>\n",
       "      <th>Cosine_Similarity</th>\n",
       "      <th>Edit_distance</th>\n",
       "      <th>Jaccard_similarity</th>\n",
       "      <th>Sequence_matcher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amrozi accused his brother, whom he called \"th...</td>\n",
       "      <td>Referring to him as only \"the witness\", Amrozi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.801978</td>\n",
       "      <td>11</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.653659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yucaipa owned Dominick's before selling the ch...</td>\n",
       "      <td>Yucaipa bought Dominick's in 1995 for $693 mil...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.339099</td>\n",
       "      <td>14</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.627027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They had published an advertisement on the Int...</td>\n",
       "      <td>On June 10, the ship's owners had published an...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.588364</td>\n",
       "      <td>14</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.704225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Around 0335 GMT, Tab shares were up 19 cents, ...</td>\n",
       "      <td>Tab shares jumped 20 cents, or 4.6%, to set a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.397346</td>\n",
       "      <td>13</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.616216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The stock rose $2.11, or about 11 percent, to ...</td>\n",
       "      <td>PG&amp;E Corp. shares jumped $1.63 or 8 percent to...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.381322</td>\n",
       "      <td>15</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.605128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Revenue in the first quarter of the year dropp...</td>\n",
       "      <td>With the scandal hanging over Stewart's compan...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.776515</td>\n",
       "      <td>8</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.773109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Nasdaq had a weekly gain of 17.27, or 1.2 ...</td>\n",
       "      <td>The tech-laced Nasdaq Composite .IXIC rallied ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.179523</td>\n",
       "      <td>13</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.505747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The DVD-CCA then appealed to the state Supreme...</td>\n",
       "      <td>The DVD CCA appealed that decision to the U.S....</td>\n",
       "      <td>1</td>\n",
       "      <td>0.716812</td>\n",
       "      <td>5</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.736842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>That compared with $35.18 million, or 24 cents...</td>\n",
       "      <td>Earnings were affected by a non-recurring $8 m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.252334</td>\n",
       "      <td>10</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.544379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>He said the foodservice pie business doesn't f...</td>\n",
       "      <td>The foodservice pie business does not fit our ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.818180</td>\n",
       "      <td>6</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.832298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Sentence_1  \\\n",
       "0  Amrozi accused his brother, whom he called \"th...   \n",
       "1  Yucaipa owned Dominick's before selling the ch...   \n",
       "2  They had published an advertisement on the Int...   \n",
       "3  Around 0335 GMT, Tab shares were up 19 cents, ...   \n",
       "4  The stock rose $2.11, or about 11 percent, to ...   \n",
       "5  Revenue in the first quarter of the year dropp...   \n",
       "6  The Nasdaq had a weekly gain of 17.27, or 1.2 ...   \n",
       "7  The DVD-CCA then appealed to the state Supreme...   \n",
       "8  That compared with $35.18 million, or 24 cents...   \n",
       "9  He said the foodservice pie business doesn't f...   \n",
       "\n",
       "                                          Sentence_2  is_Paraphrase  \\\n",
       "0  Referring to him as only \"the witness\", Amrozi...              1   \n",
       "1  Yucaipa bought Dominick's in 1995 for $693 mil...              0   \n",
       "2  On June 10, the ship's owners had published an...              1   \n",
       "3  Tab shares jumped 20 cents, or 4.6%, to set a ...              0   \n",
       "4  PG&E Corp. shares jumped $1.63 or 8 percent to...              1   \n",
       "5  With the scandal hanging over Stewart's compan...              1   \n",
       "6  The tech-laced Nasdaq Composite .IXIC rallied ...              0   \n",
       "7  The DVD CCA appealed that decision to the U.S....              1   \n",
       "8  Earnings were affected by a non-recurring $8 m...              0   \n",
       "9  The foodservice pie business does not fit our ...              1   \n",
       "\n",
       "   Cosine_Similarity  Edit_distance  Jaccard_similarity  Sequence_matcher  \n",
       "0           0.801978             11            0.500000          0.653659  \n",
       "1           0.339099             14            0.230769          0.627027  \n",
       "2           0.588364             14            0.500000          0.704225  \n",
       "3           0.397346             13            0.384615          0.616216  \n",
       "4           0.381322             15            0.321429          0.605128  \n",
       "5           0.776515              8            0.680000          0.773109  \n",
       "6           0.179523             13            0.160000          0.505747  \n",
       "7           0.716812              5            0.428571          0.736842  \n",
       "8           0.252334             10            0.166667          0.544379  \n",
       "9           0.818180              6            0.600000          0.832298  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying Sequence Matcher to MRP Corpus sentence pairs \n",
    "mrp_data['Sequence_matcher'] = mrp_data.apply(sequence_matcher, axis=1)\n",
    "mrp_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>N-gram measure</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-gram is a sequence of N words. Here, we create N-grams of both sentences. We then look at the common grams and divide it by the union of grams (in other words perform a jaccard coefficient with the n-grams). Through our research, we found that N = 3 or 4 is commonly used and are optimal at capturing the probability of a given word given the previous words. We chose N = 3 because we felt that this would be able to capture context even for shorter sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_measure(row):\n",
    "    n = 3\n",
    "    common_count = 1\n",
    "    grams_sentence_one = ngrams(row['Sentence_1'].split(), n)\n",
    "    grams_sentence_two = ngrams(row['Sentence_2'].split(), n)\n",
    "    grams_sentence_one_total = sum(1 for x in grams_sentence_one)\n",
    "    grams_sentence_two_total = sum(1 for x in grams_sentence_two)\n",
    "    for gram_in_one in grams_sentence_one:\n",
    "        if gram_in_one in grams_sentence_two:\n",
    "            common_count += 1\n",
    "    union = grams_sentence_one_total + grams_sentence_two_total - common_count\n",
    "    return common_count / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_1</th>\n",
       "      <th>Sentence_2</th>\n",
       "      <th>is_Paraphrase</th>\n",
       "      <th>Cosine_Similarity</th>\n",
       "      <th>Edit_distance</th>\n",
       "      <th>Jaccard_similarity</th>\n",
       "      <th>Sequence_matcher</th>\n",
       "      <th>N-gram_measure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.895532</td>\n",
       "      <td>3</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.047619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.410995</td>\n",
       "      <td>9</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.647482</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.225765</td>\n",
       "      <td>11</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069565</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.168368</td>\n",
       "      <td>12</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.365217</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.422795</td>\n",
       "      <td>11</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.659091</td>\n",
       "      <td>0.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Should I buy tiago?</td>\n",
       "      <td>What keeps childern active and far from phone ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.172840</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>What should I do to be a great geologist?</td>\n",
       "      <td>1</td>\n",
       "      <td>0.336097</td>\n",
       "      <td>5</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.591549</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>When do you use シ instead of し?</td>\n",
       "      <td>When do you use \"&amp;\" instead of \"and\"?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.709297</td>\n",
       "      <td>2</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Motorola (company): Can I hack my Charter Moto...</td>\n",
       "      <td>How do I hack Motorola DCX3400 for free internet?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.380873</td>\n",
       "      <td>8</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.495413</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Sentence_1  \\\n",
       "0  What is the step by step guide to invest in sh...   \n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2  How can I increase the speed of my internet co...   \n",
       "3  Why am I mentally very lonely? How can I solve...   \n",
       "4  Which one dissolve in water quikly sugar, salt...   \n",
       "5  Astrology: I am a Capricorn Sun Cap moon and c...   \n",
       "6                                Should I buy tiago?   \n",
       "7                     How can I be a good geologist?   \n",
       "8                    When do you use シ instead of し?   \n",
       "9  Motorola (company): Can I hack my Charter Moto...   \n",
       "\n",
       "                                          Sentence_2  is_Paraphrase  \\\n",
       "0  What is the step by step guide to invest in sh...              0   \n",
       "1  What would happen if the Indian government sto...              0   \n",
       "2  How can Internet speed be increased by hacking...              0   \n",
       "3  Find the remainder when [math]23^{24}[/math] i...              0   \n",
       "4            Which fish would survive in salt water?              0   \n",
       "5  I'm a triple Capricorn (Sun, Moon and ascendan...              1   \n",
       "6  What keeps childern active and far from phone ...              0   \n",
       "7          What should I do to be a great geologist?              1   \n",
       "8              When do you use \"&\" instead of \"and\"?              0   \n",
       "9  How do I hack Motorola DCX3400 for free internet?              0   \n",
       "\n",
       "   Cosine_Similarity  Edit_distance  Jaccard_similarity  Sequence_matcher  \\\n",
       "0           0.895532              3            0.857143          0.926829   \n",
       "1           0.410995              9            0.235294          0.647482   \n",
       "2           0.225765             11            0.200000          0.454545   \n",
       "3           0.000000             11            0.000000          0.069565   \n",
       "4           0.168368             12            0.111111          0.365217   \n",
       "5           0.422795             11            0.333333          0.659091   \n",
       "6           0.000000             11            0.000000          0.172840   \n",
       "7           0.336097              5            0.333333          0.591549   \n",
       "8           0.709297              2            0.600000          0.852941   \n",
       "9           0.380873              8            0.200000          0.495413   \n",
       "\n",
       "   N-gram_measure  \n",
       "0        0.047619  \n",
       "1        0.062500  \n",
       "2        0.052632  \n",
       "3        0.066667  \n",
       "4        0.066667  \n",
       "5        0.037037  \n",
       "6        0.100000  \n",
       "7        0.090909  \n",
       "8        0.090909  \n",
       "9        0.076923  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying N-gram Measure to Quora sentence pairs \n",
    "quora_data['N-gram_measure'] = quora_data.apply(ngram_measure, axis=1)\n",
    "quora_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_1</th>\n",
       "      <th>Sentence_2</th>\n",
       "      <th>is_Paraphrase</th>\n",
       "      <th>Cosine_Similarity</th>\n",
       "      <th>Edit_distance</th>\n",
       "      <th>Jaccard_similarity</th>\n",
       "      <th>Sequence_matcher</th>\n",
       "      <th>N-gram_measure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amrozi accused his brother, whom he called \"th...</td>\n",
       "      <td>Referring to him as only \"the witness\", Amrozi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.801978</td>\n",
       "      <td>11</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.653659</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yucaipa owned Dominick's before selling the ch...</td>\n",
       "      <td>Yucaipa bought Dominick's in 1995 for $693 mil...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.339099</td>\n",
       "      <td>14</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.627027</td>\n",
       "      <td>0.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They had published an advertisement on the Int...</td>\n",
       "      <td>On June 10, the ship's owners had published an...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.588364</td>\n",
       "      <td>14</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.704225</td>\n",
       "      <td>0.032258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Around 0335 GMT, Tab shares were up 19 cents, ...</td>\n",
       "      <td>Tab shares jumped 20 cents, or 4.6%, to set a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.397346</td>\n",
       "      <td>13</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.616216</td>\n",
       "      <td>0.032258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The stock rose $2.11, or about 11 percent, to ...</td>\n",
       "      <td>PG&amp;E Corp. shares jumped $1.63 or 8 percent to...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.381322</td>\n",
       "      <td>15</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.605128</td>\n",
       "      <td>0.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Revenue in the first quarter of the year dropp...</td>\n",
       "      <td>With the scandal hanging over Stewart's compan...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.776515</td>\n",
       "      <td>8</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.773109</td>\n",
       "      <td>0.027027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Nasdaq had a weekly gain of 17.27, or 1.2 ...</td>\n",
       "      <td>The tech-laced Nasdaq Composite .IXIC rallied ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.179523</td>\n",
       "      <td>13</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.505747</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The DVD-CCA then appealed to the state Supreme...</td>\n",
       "      <td>The DVD CCA appealed that decision to the U.S....</td>\n",
       "      <td>1</td>\n",
       "      <td>0.716812</td>\n",
       "      <td>5</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>That compared with $35.18 million, or 24 cents...</td>\n",
       "      <td>Earnings were affected by a non-recurring $8 m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.252334</td>\n",
       "      <td>10</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.544379</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>He said the foodservice pie business doesn't f...</td>\n",
       "      <td>The foodservice pie business does not fit our ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.818180</td>\n",
       "      <td>6</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.832298</td>\n",
       "      <td>0.052632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Sentence_1  \\\n",
       "0  Amrozi accused his brother, whom he called \"th...   \n",
       "1  Yucaipa owned Dominick's before selling the ch...   \n",
       "2  They had published an advertisement on the Int...   \n",
       "3  Around 0335 GMT, Tab shares were up 19 cents, ...   \n",
       "4  The stock rose $2.11, or about 11 percent, to ...   \n",
       "5  Revenue in the first quarter of the year dropp...   \n",
       "6  The Nasdaq had a weekly gain of 17.27, or 1.2 ...   \n",
       "7  The DVD-CCA then appealed to the state Supreme...   \n",
       "8  That compared with $35.18 million, or 24 cents...   \n",
       "9  He said the foodservice pie business doesn't f...   \n",
       "\n",
       "                                          Sentence_2  is_Paraphrase  \\\n",
       "0  Referring to him as only \"the witness\", Amrozi...              1   \n",
       "1  Yucaipa bought Dominick's in 1995 for $693 mil...              0   \n",
       "2  On June 10, the ship's owners had published an...              1   \n",
       "3  Tab shares jumped 20 cents, or 4.6%, to set a ...              0   \n",
       "4  PG&E Corp. shares jumped $1.63 or 8 percent to...              1   \n",
       "5  With the scandal hanging over Stewart's compan...              1   \n",
       "6  The tech-laced Nasdaq Composite .IXIC rallied ...              0   \n",
       "7  The DVD CCA appealed that decision to the U.S....              1   \n",
       "8  Earnings were affected by a non-recurring $8 m...              0   \n",
       "9  The foodservice pie business does not fit our ...              1   \n",
       "\n",
       "   Cosine_Similarity  Edit_distance  Jaccard_similarity  Sequence_matcher  \\\n",
       "0           0.801978             11            0.500000          0.653659   \n",
       "1           0.339099             14            0.230769          0.627027   \n",
       "2           0.588364             14            0.500000          0.704225   \n",
       "3           0.397346             13            0.384615          0.616216   \n",
       "4           0.381322             15            0.321429          0.605128   \n",
       "5           0.776515              8            0.680000          0.773109   \n",
       "6           0.179523             13            0.160000          0.505747   \n",
       "7           0.716812              5            0.428571          0.736842   \n",
       "8           0.252334             10            0.166667          0.544379   \n",
       "9           0.818180              6            0.600000          0.832298   \n",
       "\n",
       "   N-gram_measure  \n",
       "0        0.040000  \n",
       "1        0.037037  \n",
       "2        0.032258  \n",
       "3        0.032258  \n",
       "4        0.031250  \n",
       "5        0.027027  \n",
       "6        0.041667  \n",
       "7        0.066667  \n",
       "8        0.043478  \n",
       "9        0.052632  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying N-gram Measure to MRP Corpus sentence pairs \n",
    "mrp_data['N-gram_measure'] = mrp_data.apply(ngram_measure, axis=1)\n",
    "mrp_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Semantic Similarity</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Word Mover's Distance </h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Mover's Distance uses normalized bag of words and word embeddings to calculate the distance between sentences. It retrieves vectors from pre-trained word embeddings models for the words of the sentences. The key assumption with this similarity measure is that similar words should have similar vectors. <br/>\n",
    "For example, 'Obama speaks to the media in Illinois' and 'The president greets the press in Chicago' have the same meaning, however they do not have any words in common. Word Mover's Distance helps with this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_movers_distance(row):\n",
    "    sentence_one_tokenize = tokenize(row['Sentence_1'])\n",
    "    sentence_two_tokenize = tokenize(row['Sentence_2'])\n",
    "    filtered_sentence_one = remove_stop_words(sentence_one_tokenize)\n",
    "    filtered_sentence_two = remove_stop_words(sentence_two_tokenize)\n",
    "    distance = word_vectors.wmdistance(filtered_sentence_one, filtered_sentence_two)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_1</th>\n",
       "      <th>Sentence_2</th>\n",
       "      <th>is_Paraphrase</th>\n",
       "      <th>Cosine_Similarity</th>\n",
       "      <th>Edit_distance</th>\n",
       "      <th>Jaccard_similarity</th>\n",
       "      <th>Sequence_matcher</th>\n",
       "      <th>N-gram_measure</th>\n",
       "      <th>WMD_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.895532</td>\n",
       "      <td>3</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.960645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.410995</td>\n",
       "      <td>9</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.647482</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>4.814004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.225765</td>\n",
       "      <td>11</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>3.439465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069565</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>5.829829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.168368</td>\n",
       "      <td>12</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.365217</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>5.070934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.422795</td>\n",
       "      <td>11</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.659091</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>2.846745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Should I buy tiago?</td>\n",
       "      <td>What keeps childern active and far from phone ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.172840</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>6.943961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>What should I do to be a great geologist?</td>\n",
       "      <td>1</td>\n",
       "      <td>0.336097</td>\n",
       "      <td>5</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.591549</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>1.980018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>When do you use シ instead of し?</td>\n",
       "      <td>When do you use \"&amp;\" instead of \"and\"?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.709297</td>\n",
       "      <td>2</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Motorola (company): Can I hack my Charter Moto...</td>\n",
       "      <td>How do I hack Motorola DCX3400 for free internet?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.380873</td>\n",
       "      <td>8</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.495413</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>3.169096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Sentence_1  \\\n",
       "0  What is the step by step guide to invest in sh...   \n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2  How can I increase the speed of my internet co...   \n",
       "3  Why am I mentally very lonely? How can I solve...   \n",
       "4  Which one dissolve in water quikly sugar, salt...   \n",
       "5  Astrology: I am a Capricorn Sun Cap moon and c...   \n",
       "6                                Should I buy tiago?   \n",
       "7                     How can I be a good geologist?   \n",
       "8                    When do you use シ instead of し?   \n",
       "9  Motorola (company): Can I hack my Charter Moto...   \n",
       "\n",
       "                                          Sentence_2  is_Paraphrase  \\\n",
       "0  What is the step by step guide to invest in sh...              0   \n",
       "1  What would happen if the Indian government sto...              0   \n",
       "2  How can Internet speed be increased by hacking...              0   \n",
       "3  Find the remainder when [math]23^{24}[/math] i...              0   \n",
       "4            Which fish would survive in salt water?              0   \n",
       "5  I'm a triple Capricorn (Sun, Moon and ascendan...              1   \n",
       "6  What keeps childern active and far from phone ...              0   \n",
       "7          What should I do to be a great geologist?              1   \n",
       "8              When do you use \"&\" instead of \"and\"?              0   \n",
       "9  How do I hack Motorola DCX3400 for free internet?              0   \n",
       "\n",
       "   Cosine_Similarity  Edit_distance  Jaccard_similarity  Sequence_matcher  \\\n",
       "0           0.895532              3            0.857143          0.926829   \n",
       "1           0.410995              9            0.235294          0.647482   \n",
       "2           0.225765             11            0.200000          0.454545   \n",
       "3           0.000000             11            0.000000          0.069565   \n",
       "4           0.168368             12            0.111111          0.365217   \n",
       "5           0.422795             11            0.333333          0.659091   \n",
       "6           0.000000             11            0.000000          0.172840   \n",
       "7           0.336097              5            0.333333          0.591549   \n",
       "8           0.709297              2            0.600000          0.852941   \n",
       "9           0.380873              8            0.200000          0.495413   \n",
       "\n",
       "   N-gram_measure  WMD_distance  \n",
       "0        0.047619      0.960645  \n",
       "1        0.062500      4.814004  \n",
       "2        0.052632      3.439465  \n",
       "3        0.066667      5.829829  \n",
       "4        0.066667      5.070934  \n",
       "5        0.037037      2.846745  \n",
       "6        0.100000      6.943961  \n",
       "7        0.090909      1.980018  \n",
       "8        0.090909      0.000000  \n",
       "9        0.076923      3.169096  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying Word Mover's Distance to Quora sentence pairs \n",
    "quora_data['WMD_distance'] = quora_data.apply(word_movers_distance, axis=1)\n",
    "quora_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_1</th>\n",
       "      <th>Sentence_2</th>\n",
       "      <th>is_Paraphrase</th>\n",
       "      <th>Cosine_Similarity</th>\n",
       "      <th>Edit_distance</th>\n",
       "      <th>Jaccard_similarity</th>\n",
       "      <th>Sequence_matcher</th>\n",
       "      <th>N-gram_measure</th>\n",
       "      <th>WMD_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amrozi accused his brother, whom he called \"th...</td>\n",
       "      <td>Referring to him as only \"the witness\", Amrozi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.801978</td>\n",
       "      <td>11</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.653659</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.491286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yucaipa owned Dominick's before selling the ch...</td>\n",
       "      <td>Yucaipa bought Dominick's in 1995 for $693 mil...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.339099</td>\n",
       "      <td>14</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.627027</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>2.723919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They had published an advertisement on the Int...</td>\n",
       "      <td>On June 10, the ship's owners had published an...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.588364</td>\n",
       "      <td>14</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.704225</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>1.386057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Around 0335 GMT, Tab shares were up 19 cents, ...</td>\n",
       "      <td>Tab shares jumped 20 cents, or 4.6%, to set a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.397346</td>\n",
       "      <td>13</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.616216</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>2.483421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The stock rose $2.11, or about 11 percent, to ...</td>\n",
       "      <td>PG&amp;E Corp. shares jumped $1.63 or 8 percent to...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.381322</td>\n",
       "      <td>15</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.605128</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>2.149025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Revenue in the first quarter of the year dropp...</td>\n",
       "      <td>With the scandal hanging over Stewart's compan...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.776515</td>\n",
       "      <td>8</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.773109</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>1.475687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Nasdaq had a weekly gain of 17.27, or 1.2 ...</td>\n",
       "      <td>The tech-laced Nasdaq Composite .IXIC rallied ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.179523</td>\n",
       "      <td>13</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.505747</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>4.475639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The DVD-CCA then appealed to the state Supreme...</td>\n",
       "      <td>The DVD CCA appealed that decision to the U.S....</td>\n",
       "      <td>1</td>\n",
       "      <td>0.716812</td>\n",
       "      <td>5</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>3.584339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>That compared with $35.18 million, or 24 cents...</td>\n",
       "      <td>Earnings were affected by a non-recurring $8 m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.252334</td>\n",
       "      <td>10</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.544379</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>4.282460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>He said the foodservice pie business doesn't f...</td>\n",
       "      <td>The foodservice pie business does not fit our ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.818180</td>\n",
       "      <td>6</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.832298</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.907146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Sentence_1  \\\n",
       "0  Amrozi accused his brother, whom he called \"th...   \n",
       "1  Yucaipa owned Dominick's before selling the ch...   \n",
       "2  They had published an advertisement on the Int...   \n",
       "3  Around 0335 GMT, Tab shares were up 19 cents, ...   \n",
       "4  The stock rose $2.11, or about 11 percent, to ...   \n",
       "5  Revenue in the first quarter of the year dropp...   \n",
       "6  The Nasdaq had a weekly gain of 17.27, or 1.2 ...   \n",
       "7  The DVD-CCA then appealed to the state Supreme...   \n",
       "8  That compared with $35.18 million, or 24 cents...   \n",
       "9  He said the foodservice pie business doesn't f...   \n",
       "\n",
       "                                          Sentence_2  is_Paraphrase  \\\n",
       "0  Referring to him as only \"the witness\", Amrozi...              1   \n",
       "1  Yucaipa bought Dominick's in 1995 for $693 mil...              0   \n",
       "2  On June 10, the ship's owners had published an...              1   \n",
       "3  Tab shares jumped 20 cents, or 4.6%, to set a ...              0   \n",
       "4  PG&E Corp. shares jumped $1.63 or 8 percent to...              1   \n",
       "5  With the scandal hanging over Stewart's compan...              1   \n",
       "6  The tech-laced Nasdaq Composite .IXIC rallied ...              0   \n",
       "7  The DVD CCA appealed that decision to the U.S....              1   \n",
       "8  Earnings were affected by a non-recurring $8 m...              0   \n",
       "9  The foodservice pie business does not fit our ...              1   \n",
       "\n",
       "   Cosine_Similarity  Edit_distance  Jaccard_similarity  Sequence_matcher  \\\n",
       "0           0.801978             11            0.500000          0.653659   \n",
       "1           0.339099             14            0.230769          0.627027   \n",
       "2           0.588364             14            0.500000          0.704225   \n",
       "3           0.397346             13            0.384615          0.616216   \n",
       "4           0.381322             15            0.321429          0.605128   \n",
       "5           0.776515              8            0.680000          0.773109   \n",
       "6           0.179523             13            0.160000          0.505747   \n",
       "7           0.716812              5            0.428571          0.736842   \n",
       "8           0.252334             10            0.166667          0.544379   \n",
       "9           0.818180              6            0.600000          0.832298   \n",
       "\n",
       "   N-gram_measure  WMD_distance  \n",
       "0        0.040000      0.491286  \n",
       "1        0.037037      2.723919  \n",
       "2        0.032258      1.386057  \n",
       "3        0.032258      2.483421  \n",
       "4        0.031250      2.149025  \n",
       "5        0.027027      1.475687  \n",
       "6        0.041667      4.475639  \n",
       "7        0.066667      3.584339  \n",
       "8        0.043478      4.282460  \n",
       "9        0.052632      0.907146  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying Word Mover's Distance to MRP Corpus sentence pairs \n",
    "mrp_data['WMD_distance'] = mrp_data.apply(word_movers_distance, axis=1)\n",
    "mrp_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Named Entity Recognition Similarity</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this feature, we first collected NER words for each sentences along with their label. For example, 'Washington' will have a label of 'GPE' for geo-political entities. We computed Jaccard Coefficient by dividing the common NER (with label) over the union of NER of both sentences. \n",
    "In the case where no NER was detected, in neither of the sentences, we simply returned 0, else we returned the Jaccard Coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_measure(row):\n",
    "    ner_sentence_one=[]\n",
    "    ner_sentence_two=[]\n",
    "    count_common_ner = 0\n",
    "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(row['Sentence_1']))):\n",
    "        if hasattr(chunk, 'label'):\n",
    "            ner_sentence_one.append(chunk)\n",
    "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(row['Sentence_2']))):\n",
    "        if hasattr(chunk, 'label'):\n",
    "            ner_sentence_two.append(chunk)\n",
    "    for item in ner_sentence_one:\n",
    "        if item in ner_sentence_two:\n",
    "            count_common_ner += 1\n",
    "    union = len(ner_sentence_one) + len(ner_sentence_two) - count_common_ner\n",
    "    if union == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return count_common_ner / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_1</th>\n",
       "      <th>Sentence_2</th>\n",
       "      <th>is_Paraphrase</th>\n",
       "      <th>Cosine_Similarity</th>\n",
       "      <th>Edit_distance</th>\n",
       "      <th>Jaccard_similarity</th>\n",
       "      <th>Sequence_matcher</th>\n",
       "      <th>N-gram_measure</th>\n",
       "      <th>WMD_distance</th>\n",
       "      <th>NER_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.895532</td>\n",
       "      <td>3</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.960645</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.410995</td>\n",
       "      <td>9</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.647482</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>4.814004</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.225765</td>\n",
       "      <td>11</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>3.439465</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069565</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>5.829829</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.168368</td>\n",
       "      <td>12</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.365217</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>5.070934</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.422795</td>\n",
       "      <td>11</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.659091</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>2.846745</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Should I buy tiago?</td>\n",
       "      <td>What keeps childern active and far from phone ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.172840</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>6.943961</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>What should I do to be a great geologist?</td>\n",
       "      <td>1</td>\n",
       "      <td>0.336097</td>\n",
       "      <td>5</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.591549</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>1.980018</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>When do you use シ instead of し?</td>\n",
       "      <td>When do you use \"&amp;\" instead of \"and\"?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.709297</td>\n",
       "      <td>2</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Motorola (company): Can I hack my Charter Moto...</td>\n",
       "      <td>How do I hack Motorola DCX3400 for free internet?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.380873</td>\n",
       "      <td>8</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.495413</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>3.169096</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Sentence_1  \\\n",
       "0  What is the step by step guide to invest in sh...   \n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2  How can I increase the speed of my internet co...   \n",
       "3  Why am I mentally very lonely? How can I solve...   \n",
       "4  Which one dissolve in water quikly sugar, salt...   \n",
       "5  Astrology: I am a Capricorn Sun Cap moon and c...   \n",
       "6                                Should I buy tiago?   \n",
       "7                     How can I be a good geologist?   \n",
       "8                    When do you use シ instead of し?   \n",
       "9  Motorola (company): Can I hack my Charter Moto...   \n",
       "\n",
       "                                          Sentence_2  is_Paraphrase  \\\n",
       "0  What is the step by step guide to invest in sh...              0   \n",
       "1  What would happen if the Indian government sto...              0   \n",
       "2  How can Internet speed be increased by hacking...              0   \n",
       "3  Find the remainder when [math]23^{24}[/math] i...              0   \n",
       "4            Which fish would survive in salt water?              0   \n",
       "5  I'm a triple Capricorn (Sun, Moon and ascendan...              1   \n",
       "6  What keeps childern active and far from phone ...              0   \n",
       "7          What should I do to be a great geologist?              1   \n",
       "8              When do you use \"&\" instead of \"and\"?              0   \n",
       "9  How do I hack Motorola DCX3400 for free internet?              0   \n",
       "\n",
       "   Cosine_Similarity  Edit_distance  Jaccard_similarity  Sequence_matcher  \\\n",
       "0           0.895532              3            0.857143          0.926829   \n",
       "1           0.410995              9            0.235294          0.647482   \n",
       "2           0.225765             11            0.200000          0.454545   \n",
       "3           0.000000             11            0.000000          0.069565   \n",
       "4           0.168368             12            0.111111          0.365217   \n",
       "5           0.422795             11            0.333333          0.659091   \n",
       "6           0.000000             11            0.000000          0.172840   \n",
       "7           0.336097              5            0.333333          0.591549   \n",
       "8           0.709297              2            0.600000          0.852941   \n",
       "9           0.380873              8            0.200000          0.495413   \n",
       "\n",
       "   N-gram_measure  WMD_distance  NER_similarity  \n",
       "0        0.047619      0.960645        0.000000  \n",
       "1        0.062500      4.814004        0.333333  \n",
       "2        0.052632      3.439465        0.000000  \n",
       "3        0.066667      5.829829        0.000000  \n",
       "4        0.066667      5.070934        0.000000  \n",
       "5        0.037037      2.846745        0.000000  \n",
       "6        0.100000      6.943961        0.000000  \n",
       "7        0.090909      1.980018        0.000000  \n",
       "8        0.090909      0.000000        0.000000  \n",
       "9        0.076923      3.169096        0.000000  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying NER Measure to Quora sentence pairs \n",
    "quora_data['NER_similarity'] = quora_data.apply(ner_measure, axis=1)\n",
    "quora_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_1</th>\n",
       "      <th>Sentence_2</th>\n",
       "      <th>is_Paraphrase</th>\n",
       "      <th>Cosine_Similarity</th>\n",
       "      <th>Edit_distance</th>\n",
       "      <th>Jaccard_similarity</th>\n",
       "      <th>Sequence_matcher</th>\n",
       "      <th>N-gram_measure</th>\n",
       "      <th>WMD_distance</th>\n",
       "      <th>NER_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amrozi accused his brother, whom he called \"th...</td>\n",
       "      <td>Referring to him as only \"the witness\", Amrozi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.801978</td>\n",
       "      <td>11</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.653659</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.491286</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yucaipa owned Dominick's before selling the ch...</td>\n",
       "      <td>Yucaipa bought Dominick's in 1995 for $693 mil...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.339099</td>\n",
       "      <td>14</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.627027</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>2.723919</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They had published an advertisement on the Int...</td>\n",
       "      <td>On June 10, the ship's owners had published an...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.588364</td>\n",
       "      <td>14</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.704225</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>1.386057</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Around 0335 GMT, Tab shares were up 19 cents, ...</td>\n",
       "      <td>Tab shares jumped 20 cents, or 4.6%, to set a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.397346</td>\n",
       "      <td>13</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.616216</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>2.483421</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The stock rose $2.11, or about 11 percent, to ...</td>\n",
       "      <td>PG&amp;E Corp. shares jumped $1.63 or 8 percent to...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.381322</td>\n",
       "      <td>15</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.605128</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>2.149025</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Revenue in the first quarter of the year dropp...</td>\n",
       "      <td>With the scandal hanging over Stewart's compan...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.776515</td>\n",
       "      <td>8</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.773109</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>1.475687</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Nasdaq had a weekly gain of 17.27, or 1.2 ...</td>\n",
       "      <td>The tech-laced Nasdaq Composite .IXIC rallied ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.179523</td>\n",
       "      <td>13</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.505747</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>4.475639</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The DVD-CCA then appealed to the state Supreme...</td>\n",
       "      <td>The DVD CCA appealed that decision to the U.S....</td>\n",
       "      <td>1</td>\n",
       "      <td>0.716812</td>\n",
       "      <td>5</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>3.584339</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>That compared with $35.18 million, or 24 cents...</td>\n",
       "      <td>Earnings were affected by a non-recurring $8 m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.252334</td>\n",
       "      <td>10</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.544379</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>4.282460</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>He said the foodservice pie business doesn't f...</td>\n",
       "      <td>The foodservice pie business does not fit our ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.818180</td>\n",
       "      <td>6</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.832298</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.907146</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Sentence_1  \\\n",
       "0  Amrozi accused his brother, whom he called \"th...   \n",
       "1  Yucaipa owned Dominick's before selling the ch...   \n",
       "2  They had published an advertisement on the Int...   \n",
       "3  Around 0335 GMT, Tab shares were up 19 cents, ...   \n",
       "4  The stock rose $2.11, or about 11 percent, to ...   \n",
       "5  Revenue in the first quarter of the year dropp...   \n",
       "6  The Nasdaq had a weekly gain of 17.27, or 1.2 ...   \n",
       "7  The DVD-CCA then appealed to the state Supreme...   \n",
       "8  That compared with $35.18 million, or 24 cents...   \n",
       "9  He said the foodservice pie business doesn't f...   \n",
       "\n",
       "                                          Sentence_2  is_Paraphrase  \\\n",
       "0  Referring to him as only \"the witness\", Amrozi...              1   \n",
       "1  Yucaipa bought Dominick's in 1995 for $693 mil...              0   \n",
       "2  On June 10, the ship's owners had published an...              1   \n",
       "3  Tab shares jumped 20 cents, or 4.6%, to set a ...              0   \n",
       "4  PG&E Corp. shares jumped $1.63 or 8 percent to...              1   \n",
       "5  With the scandal hanging over Stewart's compan...              1   \n",
       "6  The tech-laced Nasdaq Composite .IXIC rallied ...              0   \n",
       "7  The DVD CCA appealed that decision to the U.S....              1   \n",
       "8  Earnings were affected by a non-recurring $8 m...              0   \n",
       "9  The foodservice pie business does not fit our ...              1   \n",
       "\n",
       "   Cosine_Similarity  Edit_distance  Jaccard_similarity  Sequence_matcher  \\\n",
       "0           0.801978             11            0.500000          0.653659   \n",
       "1           0.339099             14            0.230769          0.627027   \n",
       "2           0.588364             14            0.500000          0.704225   \n",
       "3           0.397346             13            0.384615          0.616216   \n",
       "4           0.381322             15            0.321429          0.605128   \n",
       "5           0.776515              8            0.680000          0.773109   \n",
       "6           0.179523             13            0.160000          0.505747   \n",
       "7           0.716812              5            0.428571          0.736842   \n",
       "8           0.252334             10            0.166667          0.544379   \n",
       "9           0.818180              6            0.600000          0.832298   \n",
       "\n",
       "   N-gram_measure  WMD_distance  NER_similarity  \n",
       "0        0.040000      0.491286        1.000000  \n",
       "1        0.037037      2.723919        0.500000  \n",
       "2        0.032258      1.386057        0.000000  \n",
       "3        0.032258      2.483421        0.000000  \n",
       "4        0.031250      2.149025        1.000000  \n",
       "5        0.027027      1.475687        0.000000  \n",
       "6        0.041667      4.475639        0.000000  \n",
       "7        0.066667      3.584339        0.333333  \n",
       "8        0.043478      4.282460        0.000000  \n",
       "9        0.052632      0.907146        0.000000  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying NER Measure to MRP Corpus sentence pairs \n",
    "mrp_data['NER_similarity'] = mrp_data.apply(ner_measure, axis=1)\n",
    "mrp_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Word Sense Disambiguation</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Sense Disambiguation finds the best sense of a word from all the given senses of the word. The Lesk algorithm uses WordNet and gets the gloss of all the senses of the word in the sentence and then calculates the maximum overlap with the senses, returning whichever gives the maximum overlap. For example, let's take the phrase 'pine cone'. 'Pine' has two senses. Sense 1: kind of evergreen tree with needle-shaped leaves and Sense 2: waste away through sorrow or illness. 'Cone' has three senses. Sense 1: solid body which narrows to a point. Sense 2: something of this shape whether solid or hollow. Sense 3: fruit of a certain evergreen tree. Comparing the senses of the two words, we can see that 'evergreen tree' is common in one sense of each word. Therefore, Sense 1 of Pine and Sense 3 of Cone are the most appropriate when 'pine' and 'cone' are used together.  \n",
    "\n",
    "Using this knowledge, we created our feature. After Lesk was applied to each sentences, where the most appropriate senses of each word was detected, we looked for the common senses in the two sentences. To normalize our result, we again used Jaccard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wsd(row):\n",
    "    sentence_one_senses = []\n",
    "    sentence_two_senses = []\n",
    "    common_senses = 0\n",
    "    sentence_one_tokenize = tokenize(row['Sentence_1'])\n",
    "    sentence_two_tokenize = tokenize(row['Sentence_2'])\n",
    "    for word in sentence_one_tokenize:\n",
    "        sentence_one_senses.append(lesk(row['Sentence_1'], word))\n",
    "    for word in sentence_two_tokenize:\n",
    "        sentence_two_senses.append(lesk(['Sentenece_2'], word))\n",
    "    sentence_one_senses = (set(sentence_one_senses))\n",
    "    sentence_two_senses = (set(sentence_two_senses))\n",
    "\n",
    "    for sense in sentence_one_senses:\n",
    "        if sense in sentence_two_senses:\n",
    "            common_senses += 1\n",
    "    return common_senses / (len(sentence_one_senses) + len(sentence_two_senses) - common_senses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_1</th>\n",
       "      <th>Sentence_2</th>\n",
       "      <th>is_Paraphrase</th>\n",
       "      <th>Cosine_Similarity</th>\n",
       "      <th>Edit_distance</th>\n",
       "      <th>Jaccard_similarity</th>\n",
       "      <th>Sequence_matcher</th>\n",
       "      <th>N-gram_measure</th>\n",
       "      <th>WMD_distance</th>\n",
       "      <th>NER_similarity</th>\n",
       "      <th>WSD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.895532</td>\n",
       "      <td>3</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.960645</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.410995</td>\n",
       "      <td>9</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.647482</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>4.814004</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.225765</td>\n",
       "      <td>11</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>3.439465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069565</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>5.829829</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.168368</td>\n",
       "      <td>12</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.365217</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>5.070934</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.422795</td>\n",
       "      <td>11</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.659091</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>2.846745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Should I buy tiago?</td>\n",
       "      <td>What keeps childern active and far from phone ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.172840</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>6.943961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>What should I do to be a great geologist?</td>\n",
       "      <td>1</td>\n",
       "      <td>0.336097</td>\n",
       "      <td>5</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.591549</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>1.980018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>When do you use シ instead of し?</td>\n",
       "      <td>When do you use \"&amp;\" instead of \"and\"?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.709297</td>\n",
       "      <td>2</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Motorola (company): Can I hack my Charter Moto...</td>\n",
       "      <td>How do I hack Motorola DCX3400 for free internet?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.380873</td>\n",
       "      <td>8</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.495413</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>3.169096</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Sentence_1  \\\n",
       "0  What is the step by step guide to invest in sh...   \n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2  How can I increase the speed of my internet co...   \n",
       "3  Why am I mentally very lonely? How can I solve...   \n",
       "4  Which one dissolve in water quikly sugar, salt...   \n",
       "5  Astrology: I am a Capricorn Sun Cap moon and c...   \n",
       "6                                Should I buy tiago?   \n",
       "7                     How can I be a good geologist?   \n",
       "8                    When do you use シ instead of し?   \n",
       "9  Motorola (company): Can I hack my Charter Moto...   \n",
       "\n",
       "                                          Sentence_2  is_Paraphrase  \\\n",
       "0  What is the step by step guide to invest in sh...              0   \n",
       "1  What would happen if the Indian government sto...              0   \n",
       "2  How can Internet speed be increased by hacking...              0   \n",
       "3  Find the remainder when [math]23^{24}[/math] i...              0   \n",
       "4            Which fish would survive in salt water?              0   \n",
       "5  I'm a triple Capricorn (Sun, Moon and ascendan...              1   \n",
       "6  What keeps childern active and far from phone ...              0   \n",
       "7          What should I do to be a great geologist?              1   \n",
       "8              When do you use \"&\" instead of \"and\"?              0   \n",
       "9  How do I hack Motorola DCX3400 for free internet?              0   \n",
       "\n",
       "   Cosine_Similarity  Edit_distance  Jaccard_similarity  Sequence_matcher  \\\n",
       "0           0.895532              3            0.857143          0.926829   \n",
       "1           0.410995              9            0.235294          0.647482   \n",
       "2           0.225765             11            0.200000          0.454545   \n",
       "3           0.000000             11            0.000000          0.069565   \n",
       "4           0.168368             12            0.111111          0.365217   \n",
       "5           0.422795             11            0.333333          0.659091   \n",
       "6           0.000000             11            0.000000          0.172840   \n",
       "7           0.336097              5            0.333333          0.591549   \n",
       "8           0.709297              2            0.600000          0.852941   \n",
       "9           0.380873              8            0.200000          0.495413   \n",
       "\n",
       "   N-gram_measure  WMD_distance  NER_similarity       WSD  \n",
       "0        0.047619      0.960645        0.000000  0.416667  \n",
       "1        0.062500      4.814004        0.333333  0.125000  \n",
       "2        0.052632      3.439465        0.000000  0.187500  \n",
       "3        0.066667      5.829829        0.000000  0.076923  \n",
       "4        0.066667      5.070934        0.000000  0.200000  \n",
       "5        0.037037      2.846745        0.000000  0.166667  \n",
       "6        0.100000      6.943961        0.000000  0.125000  \n",
       "7        0.090909      1.980018        0.000000  0.200000  \n",
       "8        0.090909      0.000000        0.000000  0.333333  \n",
       "9        0.076923      3.169096        0.000000  0.428571  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying WSD Measure to Quora sentence pairs \n",
    "quora_data['WSD'] = quora_data.apply(wsd, axis=1)\n",
    "quora_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_1</th>\n",
       "      <th>Sentence_2</th>\n",
       "      <th>is_Paraphrase</th>\n",
       "      <th>Cosine_Similarity</th>\n",
       "      <th>Edit_distance</th>\n",
       "      <th>Jaccard_similarity</th>\n",
       "      <th>Sequence_matcher</th>\n",
       "      <th>N-gram_measure</th>\n",
       "      <th>WMD_distance</th>\n",
       "      <th>NER_similarity</th>\n",
       "      <th>WSD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amrozi accused his brother, whom he called \"th...</td>\n",
       "      <td>Referring to him as only \"the witness\", Amrozi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.801978</td>\n",
       "      <td>11</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.653659</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.491286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yucaipa owned Dominick's before selling the ch...</td>\n",
       "      <td>Yucaipa bought Dominick's in 1995 for $693 mil...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.339099</td>\n",
       "      <td>14</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.627027</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>2.723919</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They had published an advertisement on the Int...</td>\n",
       "      <td>On June 10, the ship's owners had published an...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.588364</td>\n",
       "      <td>14</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.704225</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>1.386057</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Around 0335 GMT, Tab shares were up 19 cents, ...</td>\n",
       "      <td>Tab shares jumped 20 cents, or 4.6%, to set a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.397346</td>\n",
       "      <td>13</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.616216</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>2.483421</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.173913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The stock rose $2.11, or about 11 percent, to ...</td>\n",
       "      <td>PG&amp;E Corp. shares jumped $1.63 or 8 percent to...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.381322</td>\n",
       "      <td>15</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.605128</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>2.149025</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.277778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Revenue in the first quarter of the year dropp...</td>\n",
       "      <td>With the scandal hanging over Stewart's compan...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.776515</td>\n",
       "      <td>8</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.773109</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>1.475687</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Nasdaq had a weekly gain of 17.27, or 1.2 ...</td>\n",
       "      <td>The tech-laced Nasdaq Composite .IXIC rallied ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.179523</td>\n",
       "      <td>13</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.505747</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>4.475639</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The DVD-CCA then appealed to the state Supreme...</td>\n",
       "      <td>The DVD CCA appealed that decision to the U.S....</td>\n",
       "      <td>1</td>\n",
       "      <td>0.716812</td>\n",
       "      <td>5</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>3.584339</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>That compared with $35.18 million, or 24 cents...</td>\n",
       "      <td>Earnings were affected by a non-recurring $8 m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.252334</td>\n",
       "      <td>10</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.544379</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>4.282460</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>He said the foodservice pie business doesn't f...</td>\n",
       "      <td>The foodservice pie business does not fit our ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.818180</td>\n",
       "      <td>6</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.832298</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.907146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Sentence_1  \\\n",
       "0  Amrozi accused his brother, whom he called \"th...   \n",
       "1  Yucaipa owned Dominick's before selling the ch...   \n",
       "2  They had published an advertisement on the Int...   \n",
       "3  Around 0335 GMT, Tab shares were up 19 cents, ...   \n",
       "4  The stock rose $2.11, or about 11 percent, to ...   \n",
       "5  Revenue in the first quarter of the year dropp...   \n",
       "6  The Nasdaq had a weekly gain of 17.27, or 1.2 ...   \n",
       "7  The DVD-CCA then appealed to the state Supreme...   \n",
       "8  That compared with $35.18 million, or 24 cents...   \n",
       "9  He said the foodservice pie business doesn't f...   \n",
       "\n",
       "                                          Sentence_2  is_Paraphrase  \\\n",
       "0  Referring to him as only \"the witness\", Amrozi...              1   \n",
       "1  Yucaipa bought Dominick's in 1995 for $693 mil...              0   \n",
       "2  On June 10, the ship's owners had published an...              1   \n",
       "3  Tab shares jumped 20 cents, or 4.6%, to set a ...              0   \n",
       "4  PG&E Corp. shares jumped $1.63 or 8 percent to...              1   \n",
       "5  With the scandal hanging over Stewart's compan...              1   \n",
       "6  The tech-laced Nasdaq Composite .IXIC rallied ...              0   \n",
       "7  The DVD CCA appealed that decision to the U.S....              1   \n",
       "8  Earnings were affected by a non-recurring $8 m...              0   \n",
       "9  The foodservice pie business does not fit our ...              1   \n",
       "\n",
       "   Cosine_Similarity  Edit_distance  Jaccard_similarity  Sequence_matcher  \\\n",
       "0           0.801978             11            0.500000          0.653659   \n",
       "1           0.339099             14            0.230769          0.627027   \n",
       "2           0.588364             14            0.500000          0.704225   \n",
       "3           0.397346             13            0.384615          0.616216   \n",
       "4           0.381322             15            0.321429          0.605128   \n",
       "5           0.776515              8            0.680000          0.773109   \n",
       "6           0.179523             13            0.160000          0.505747   \n",
       "7           0.716812              5            0.428571          0.736842   \n",
       "8           0.252334             10            0.166667          0.544379   \n",
       "9           0.818180              6            0.600000          0.832298   \n",
       "\n",
       "   N-gram_measure  WMD_distance  NER_similarity       WSD  \n",
       "0        0.040000      0.491286        1.000000  0.400000  \n",
       "1        0.037037      2.723919        0.500000  0.181818  \n",
       "2        0.032258      1.386057        0.000000  0.400000  \n",
       "3        0.032258      2.483421        0.000000  0.173913  \n",
       "4        0.031250      2.149025        1.000000  0.277778  \n",
       "5        0.027027      1.475687        0.000000  0.300000  \n",
       "6        0.041667      4.475639        0.000000  0.250000  \n",
       "7        0.066667      3.584339        0.333333  0.222222  \n",
       "8        0.043478      4.282460        0.000000  0.142857  \n",
       "9        0.052632      0.907146        0.000000  0.333333  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying WSD Measure to MRP Corpus sentence pairs \n",
    "mrp_data['WSD'] = mrp_data.apply(wsd, axis=1)\n",
    "mrp_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Wordnet Extended Cosine Similarity </h4>\n",
    "Here, we use WordNet's capabilities to extend our sentences and, therefore, extend our dictionary. We extend our semantic reach by including synonyms, hypernyms, and antonyms in our dictionary. We then call our existing calculate_cosine_similarity method to get the similarity of the extended documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_sentence_wordnet(row):\n",
    "    sentence_one_tokenize = tokenize_negation(row['Sentence_1'])\n",
    "    sentence_two_tokenize = tokenize_negation(row['Sentence_2'])\n",
    "    filtered_sentence_one = remove_stop_words(sentence_one_tokenize)\n",
    "    filtered_sentence_two = remove_stop_words(sentence_two_tokenize)\n",
    "    lemmatize_sentence_one = lemmatize(filtered_sentence_one)\n",
    "    lemmatize_sentence_two = lemmatize(filtered_sentence_two)\n",
    "    # get extended synonym list    \n",
    "    extended_dictionary_one = []\n",
    "    extended_dictionary_two = []\n",
    "    for one, two in zip(lemmatize_sentence_one, lemmatize_sentence_two):\n",
    "        synonym_one = add_synonym(one)\n",
    "        synonym_two = add_synonym(two)\n",
    "        hypernym_one = add_hypernym(one)\n",
    "        hypernym_two = add_hypernym(two)\n",
    "        if(synonym_one):\n",
    "            extended_dictionary_one += synonym_one\n",
    "        if(hypernym_one):\n",
    "            extended_dictionary_one += hypernym_one\n",
    "        if(synonym_two):\n",
    "            extended_dictionary_two += synonym_two\n",
    "        if(hypernym_two):\n",
    "            extended_dictionary_two += hypernym_two\n",
    "    lemmatize_sentence_one += extended_dictionary_one\n",
    "    lemmatize_sentence_two += extended_dictionary_two\n",
    "    \n",
    "    #calculate similarity based on the extended list \n",
    "    similarity = calculate_cosine_similarity(lemmatize_sentence_one, lemmatize_sentence_two)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_1</th>\n",
       "      <th>Sentence_2</th>\n",
       "      <th>is_Paraphrase</th>\n",
       "      <th>Cosine_Similarity</th>\n",
       "      <th>Edit_distance</th>\n",
       "      <th>Jaccard_similarity</th>\n",
       "      <th>Sequence_matcher</th>\n",
       "      <th>N-gram_measure</th>\n",
       "      <th>WMD_distance</th>\n",
       "      <th>NER_similarity</th>\n",
       "      <th>WSD</th>\n",
       "      <th>Synonym_Hypernym_Cosine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.895532</td>\n",
       "      <td>3</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.960645</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.996588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.410995</td>\n",
       "      <td>9</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.647482</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>4.814004</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.050784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.225765</td>\n",
       "      <td>11</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>3.439465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.640131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069565</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>5.829829</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.047164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.168368</td>\n",
       "      <td>12</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.365217</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>5.070934</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.239692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.422795</td>\n",
       "      <td>11</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.659091</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>2.846745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.367553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Should I buy tiago?</td>\n",
       "      <td>What keeps childern active and far from phone ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.172840</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>6.943961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>What should I do to be a great geologist?</td>\n",
       "      <td>1</td>\n",
       "      <td>0.336097</td>\n",
       "      <td>5</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.591549</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>1.980018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.047231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>When do you use シ instead of し?</td>\n",
       "      <td>When do you use \"&amp;\" instead of \"and\"?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.709297</td>\n",
       "      <td>2</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.989762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Motorola (company): Can I hack my Charter Moto...</td>\n",
       "      <td>How do I hack Motorola DCX3400 for free internet?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.380873</td>\n",
       "      <td>8</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.495413</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>3.169096</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.547755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Sentence_1  \\\n",
       "0  What is the step by step guide to invest in sh...   \n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2  How can I increase the speed of my internet co...   \n",
       "3  Why am I mentally very lonely? How can I solve...   \n",
       "4  Which one dissolve in water quikly sugar, salt...   \n",
       "5  Astrology: I am a Capricorn Sun Cap moon and c...   \n",
       "6                                Should I buy tiago?   \n",
       "7                     How can I be a good geologist?   \n",
       "8                    When do you use シ instead of し?   \n",
       "9  Motorola (company): Can I hack my Charter Moto...   \n",
       "\n",
       "                                          Sentence_2  is_Paraphrase  \\\n",
       "0  What is the step by step guide to invest in sh...              0   \n",
       "1  What would happen if the Indian government sto...              0   \n",
       "2  How can Internet speed be increased by hacking...              0   \n",
       "3  Find the remainder when [math]23^{24}[/math] i...              0   \n",
       "4            Which fish would survive in salt water?              0   \n",
       "5  I'm a triple Capricorn (Sun, Moon and ascendan...              1   \n",
       "6  What keeps childern active and far from phone ...              0   \n",
       "7          What should I do to be a great geologist?              1   \n",
       "8              When do you use \"&\" instead of \"and\"?              0   \n",
       "9  How do I hack Motorola DCX3400 for free internet?              0   \n",
       "\n",
       "   Cosine_Similarity  Edit_distance  Jaccard_similarity  Sequence_matcher  \\\n",
       "0           0.895532              3            0.857143          0.926829   \n",
       "1           0.410995              9            0.235294          0.647482   \n",
       "2           0.225765             11            0.200000          0.454545   \n",
       "3           0.000000             11            0.000000          0.069565   \n",
       "4           0.168368             12            0.111111          0.365217   \n",
       "5           0.422795             11            0.333333          0.659091   \n",
       "6           0.000000             11            0.000000          0.172840   \n",
       "7           0.336097              5            0.333333          0.591549   \n",
       "8           0.709297              2            0.600000          0.852941   \n",
       "9           0.380873              8            0.200000          0.495413   \n",
       "\n",
       "   N-gram_measure  WMD_distance  NER_similarity       WSD  \\\n",
       "0        0.047619      0.960645        0.000000  0.416667   \n",
       "1        0.062500      4.814004        0.333333  0.125000   \n",
       "2        0.052632      3.439465        0.000000  0.187500   \n",
       "3        0.066667      5.829829        0.000000  0.076923   \n",
       "4        0.066667      5.070934        0.000000  0.200000   \n",
       "5        0.037037      2.846745        0.000000  0.166667   \n",
       "6        0.100000      6.943961        0.000000  0.125000   \n",
       "7        0.090909      1.980018        0.000000  0.200000   \n",
       "8        0.090909      0.000000        0.000000  0.333333   \n",
       "9        0.076923      3.169096        0.000000  0.428571   \n",
       "\n",
       "   Synonym_Hypernym_Cosine  \n",
       "0                 0.996588  \n",
       "1                 0.050784  \n",
       "2                 0.640131  \n",
       "3                 0.047164  \n",
       "4                 0.239692  \n",
       "5                 0.367553  \n",
       "6                 0.000000  \n",
       "7                 0.047231  \n",
       "8                 0.989762  \n",
       "9                 0.547755  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying Synonym_Hypernym_Cosine to Quora sentence pairs \n",
    "quora_data['Synonym_Hypernym_Cosine'] = quora_data.apply(extend_sentence_wordnet, axis=1)\n",
    "quora_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_1</th>\n",
       "      <th>Sentence_2</th>\n",
       "      <th>is_Paraphrase</th>\n",
       "      <th>Cosine_Similarity</th>\n",
       "      <th>Edit_distance</th>\n",
       "      <th>Jaccard_similarity</th>\n",
       "      <th>Sequence_matcher</th>\n",
       "      <th>N-gram_measure</th>\n",
       "      <th>WMD_distance</th>\n",
       "      <th>NER_similarity</th>\n",
       "      <th>WSD</th>\n",
       "      <th>Synonym_Hypernym_Cosine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amrozi accused his brother, whom he called \"th...</td>\n",
       "      <td>Referring to him as only \"the witness\", Amrozi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.801978</td>\n",
       "      <td>11</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.653659</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.491286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.543801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yucaipa owned Dominick's before selling the ch...</td>\n",
       "      <td>Yucaipa bought Dominick's in 1995 for $693 mil...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.339099</td>\n",
       "      <td>14</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.627027</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>2.723919</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.241053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They had published an advertisement on the Int...</td>\n",
       "      <td>On June 10, the ship's owners had published an...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.588364</td>\n",
       "      <td>14</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.704225</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>1.386057</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.577762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Around 0335 GMT, Tab shares were up 19 cents, ...</td>\n",
       "      <td>Tab shares jumped 20 cents, or 4.6%, to set a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.397346</td>\n",
       "      <td>13</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.616216</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>2.483421</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.467429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The stock rose $2.11, or about 11 percent, to ...</td>\n",
       "      <td>PG&amp;E Corp. shares jumped $1.63 or 8 percent to...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.381322</td>\n",
       "      <td>15</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.605128</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>2.149025</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.541388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Revenue in the first quarter of the year dropp...</td>\n",
       "      <td>With the scandal hanging over Stewart's compan...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.776515</td>\n",
       "      <td>8</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.773109</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>1.475687</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.759142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Nasdaq had a weekly gain of 17.27, or 1.2 ...</td>\n",
       "      <td>The tech-laced Nasdaq Composite .IXIC rallied ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.179523</td>\n",
       "      <td>13</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.505747</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>4.475639</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.117006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The DVD-CCA then appealed to the state Supreme...</td>\n",
       "      <td>The DVD CCA appealed that decision to the U.S....</td>\n",
       "      <td>1</td>\n",
       "      <td>0.716812</td>\n",
       "      <td>5</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>3.584339</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.140652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>That compared with $35.18 million, or 24 cents...</td>\n",
       "      <td>Earnings were affected by a non-recurring $8 m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.252334</td>\n",
       "      <td>10</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.544379</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>4.282460</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.328852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>He said the foodservice pie business doesn't f...</td>\n",
       "      <td>The foodservice pie business does not fit our ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.818180</td>\n",
       "      <td>6</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.832298</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.907146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.824734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Sentence_1  \\\n",
       "0  Amrozi accused his brother, whom he called \"th...   \n",
       "1  Yucaipa owned Dominick's before selling the ch...   \n",
       "2  They had published an advertisement on the Int...   \n",
       "3  Around 0335 GMT, Tab shares were up 19 cents, ...   \n",
       "4  The stock rose $2.11, or about 11 percent, to ...   \n",
       "5  Revenue in the first quarter of the year dropp...   \n",
       "6  The Nasdaq had a weekly gain of 17.27, or 1.2 ...   \n",
       "7  The DVD-CCA then appealed to the state Supreme...   \n",
       "8  That compared with $35.18 million, or 24 cents...   \n",
       "9  He said the foodservice pie business doesn't f...   \n",
       "\n",
       "                                          Sentence_2  is_Paraphrase  \\\n",
       "0  Referring to him as only \"the witness\", Amrozi...              1   \n",
       "1  Yucaipa bought Dominick's in 1995 for $693 mil...              0   \n",
       "2  On June 10, the ship's owners had published an...              1   \n",
       "3  Tab shares jumped 20 cents, or 4.6%, to set a ...              0   \n",
       "4  PG&E Corp. shares jumped $1.63 or 8 percent to...              1   \n",
       "5  With the scandal hanging over Stewart's compan...              1   \n",
       "6  The tech-laced Nasdaq Composite .IXIC rallied ...              0   \n",
       "7  The DVD CCA appealed that decision to the U.S....              1   \n",
       "8  Earnings were affected by a non-recurring $8 m...              0   \n",
       "9  The foodservice pie business does not fit our ...              1   \n",
       "\n",
       "   Cosine_Similarity  Edit_distance  Jaccard_similarity  Sequence_matcher  \\\n",
       "0           0.801978             11            0.500000          0.653659   \n",
       "1           0.339099             14            0.230769          0.627027   \n",
       "2           0.588364             14            0.500000          0.704225   \n",
       "3           0.397346             13            0.384615          0.616216   \n",
       "4           0.381322             15            0.321429          0.605128   \n",
       "5           0.776515              8            0.680000          0.773109   \n",
       "6           0.179523             13            0.160000          0.505747   \n",
       "7           0.716812              5            0.428571          0.736842   \n",
       "8           0.252334             10            0.166667          0.544379   \n",
       "9           0.818180              6            0.600000          0.832298   \n",
       "\n",
       "   N-gram_measure  WMD_distance  NER_similarity       WSD  \\\n",
       "0        0.040000      0.491286        1.000000  0.400000   \n",
       "1        0.037037      2.723919        0.500000  0.181818   \n",
       "2        0.032258      1.386057        0.000000  0.400000   \n",
       "3        0.032258      2.483421        0.000000  0.173913   \n",
       "4        0.031250      2.149025        1.000000  0.277778   \n",
       "5        0.027027      1.475687        0.000000  0.300000   \n",
       "6        0.041667      4.475639        0.000000  0.250000   \n",
       "7        0.066667      3.584339        0.333333  0.222222   \n",
       "8        0.043478      4.282460        0.000000  0.142857   \n",
       "9        0.052632      0.907146        0.000000  0.333333   \n",
       "\n",
       "   Synonym_Hypernym_Cosine  \n",
       "0                 0.543801  \n",
       "1                 0.241053  \n",
       "2                 0.577762  \n",
       "3                 0.467429  \n",
       "4                 0.541388  \n",
       "5                 0.759142  \n",
       "6                 0.117006  \n",
       "7                 0.140652  \n",
       "8                 0.328852  \n",
       "9                 0.824734  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying Synonym_Hypernym_Cosine to MRP Corpus sentence pairs \n",
    "mrp_data['Synonym_Hypernym_Cosine'] = mrp_data.apply(extend_sentence_wordnet, axis = 1)\n",
    "mrp_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Neural Network Model </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Post Processing </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed that some of the features contained infinite values, especially the WMD measure. To handle this, we replaced the infinite values with the maximum value of that particular feature. We then normalized the feature so that all values were within the range of 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_mrp = mrp_data.loc[mrp_data['WMD_distance'] != np.nan, 'WMD_distance'].max()\n",
    "mrp_data['WMD_distance'].replace(np.nan, max_mrp, inplace=True)\n",
    "x = mrp_data[['WMD_distance']].values.astype(float)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "mrp_data['WMD_distance'] = pd.DataFrame(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_quora = quora_data.loc[quora_data['WMD_distance'] != np.nan, 'WMD_distance'].max()\n",
    "quora_data['WMD_distance'].replace(np.nan, max_quora, inplace=True)\n",
    "x = quora_data[['WMD_distance']].values.astype(float)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "quora_data['WMD_distance'] = pd.DataFrame(x_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Multi-layer Perceptron </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_perceptron(df, model):\n",
    "    features = list(df.columns.values)\n",
    "    features.remove('is_Paraphrase')\n",
    "    features.remove('Sentence_1')\n",
    "    features.remove('Sentence_2')\n",
    "    X = df[features]\n",
    "    y = df['is_Paraphrase']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()])\n",
    "    model.fit(X_train, y_train, epochs=15, batch_size=1)\n",
    "\n",
    "\n",
    "    test_loss, test_acc, test_pre, test_recall = model.evaluate(X_test, y_test)\n",
    "    print('Test accuracy:{}, test recall: {}, test precision: {}'.format(test_acc, test_recall, test_pre))\n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our research, we found that a common rule of thumb that most researchers use when deciding the number of neurons to implement in the hidden layer is ‘to use a number between the size of the input and size of the output layers’, as this results in the optimal size. Following this rule of thumb, we decided to  keep our number of neurons in the hidden layer to 7 as it in between 9 which is the total number of our inputs and 1 which is the number of our output. We also noticed that there was an improvement in our model, when we increased the number of hidden layers from one to two. However, increasing the hidden layers from two made no significant difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Layer Perceptron Results for quora_data\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 3150 samples\n",
      "Epoch 1/15\n",
      "3150/3150 [==============================] - 3s 855us/sample - loss: 0.5853 - accuracy: 0.6600 - precision: 0.5636 - recall: 0.4193\n",
      "Epoch 2/15\n",
      "3150/3150 [==============================] - 3s 818us/sample - loss: 0.5525 - accuracy: 0.6816 - precision: 0.5683 - recall: 0.6331\n",
      "Epoch 3/15\n",
      "3150/3150 [==============================] - 3s 800us/sample - loss: 0.5319 - accuracy: 0.6911 - precision: 0.5748 - recall: 0.6822\n",
      "Epoch 4/15\n",
      "3150/3150 [==============================] - 3s 817us/sample - loss: 0.5206 - accuracy: 0.6952 - precision: 0.5792 - recall: 0.6889\n",
      "Epoch 5/15\n",
      "3150/3150 [==============================] - 3s 834us/sample - loss: 0.5171 - accuracy: 0.7006 - precision: 0.5821 - recall: 0.7194\n",
      "Epoch 6/15\n",
      "3150/3150 [==============================] - 2s 765us/sample - loss: 0.5126 - accuracy: 0.6940 - precision: 0.5715 - recall: 0.7396\n",
      "Epoch 7/15\n",
      "3150/3150 [==============================] - 2s 781us/sample - loss: 0.5085 - accuracy: 0.7035 - precision: 0.5848 - recall: 0.7261\n",
      "Epoch 8/15\n",
      "3150/3150 [==============================] - 2s 768us/sample - loss: 0.5079 - accuracy: 0.7019 - precision: 0.5774 - recall: 0.7692\n",
      "Epoch 9/15\n",
      "3150/3150 [==============================] - 2s 777us/sample - loss: 0.5062 - accuracy: 0.7041 - precision: 0.5806 - recall: 0.7642\n",
      "Epoch 10/15\n",
      "3150/3150 [==============================] - 3s 817us/sample - loss: 0.5030 - accuracy: 0.7000 - precision: 0.5781 - recall: 0.7447\n",
      "Epoch 11/15\n",
      "3150/3150 [==============================] - 3s 812us/sample - loss: 0.5005 - accuracy: 0.7083 - precision: 0.5849 - recall: 0.7684\n",
      "Epoch 12/15\n",
      "3150/3150 [==============================] - 3s 806us/sample - loss: 0.5028 - accuracy: 0.7067 - precision: 0.5847 - recall: 0.7557\n",
      "Epoch 13/15\n",
      "3150/3150 [==============================] - 3s 808us/sample - loss: 0.4995 - accuracy: 0.7130 - precision: 0.5939 - recall: 0.7456\n",
      "Epoch 14/15\n",
      "3150/3150 [==============================] - 3s 834us/sample - loss: 0.5012 - accuracy: 0.7108 - precision: 0.5876 - recall: 0.7709\n",
      "Epoch 15/15\n",
      "3150/3150 [==============================] - 3s 819us/sample - loss: 0.5001 - accuracy: 0.7076 - precision: 0.5857 - recall: 0.7566\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "1350/1350 [==============================] - 0s 45us/sample - loss: 0.5128 - accuracy: 0.7015 - precision: 0.5792 - recall: 0.8655\n",
      "Test accuracy:0.7014814615249634, test recall: 0.8655303120613098, test precision: 0.5792142152786255\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x165d59150>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quora data NN\n",
    "model_quora = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(9,)),\n",
    "    keras.layers.Dense(7, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(7, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid),\n",
    "    ])\n",
    "print(\"Multi-Layer Perceptron Results for quora_data\")\n",
    "multilayer_perceptron(quora_data, model_quora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Layer Perceptron Results for mrp_data\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 2764 samples\n",
      "Epoch 1/15\n",
      "2764/2764 [==============================] - 2s 850us/sample - loss: 0.5828 - accuracy: 0.6899 - precision_2: 0.7154 - recall_2: 0.9031\n",
      "Epoch 2/15\n",
      "2764/2764 [==============================] - 2s 819us/sample - loss: 0.5525 - accuracy: 0.6899 - precision_2: 0.7255 - recall_2: 0.8749\n",
      "Epoch 3/15\n",
      "2764/2764 [==============================] - 2s 829us/sample - loss: 0.5501 - accuracy: 0.7051 - precision_2: 0.7449 - recall_2: 0.8611\n",
      "Epoch 4/15\n",
      "2764/2764 [==============================] - 2s 795us/sample - loss: 0.5440 - accuracy: 0.6993 - precision_2: 0.7446 - recall_2: 0.8489\n",
      "Epoch 5/15\n",
      "2764/2764 [==============================] - 2s 766us/sample - loss: 0.5426 - accuracy: 0.7001 - precision_2: 0.7488 - recall_2: 0.8409\n",
      "Epoch 6/15\n",
      "2764/2764 [==============================] - 2s 767us/sample - loss: 0.5416 - accuracy: 0.7145 - precision_2: 0.7618 - recall_2: 0.8441\n",
      "Epoch 7/15\n",
      "2764/2764 [==============================] - 2s 770us/sample - loss: 0.5423 - accuracy: 0.7095 - precision_2: 0.7526 - recall_2: 0.8531\n",
      "Epoch 8/15\n",
      "2764/2764 [==============================] - 2s 801us/sample - loss: 0.5352 - accuracy: 0.6975 - precision_2: 0.7463 - recall_2: 0.8409\n",
      "Epoch 9/15\n",
      "2764/2764 [==============================] - 2s 810us/sample - loss: 0.5339 - accuracy: 0.7142 - precision_2: 0.7622 - recall_2: 0.8425\n",
      "Epoch 10/15\n",
      "2764/2764 [==============================] - 2s 844us/sample - loss: 0.5367 - accuracy: 0.7149 - precision_2: 0.7699 - recall_2: 0.8281\n",
      "Epoch 11/15\n",
      "2764/2764 [==============================] - 2s 859us/sample - loss: 0.5332 - accuracy: 0.7109 - precision_2: 0.7689 - recall_2: 0.8217\n",
      "Epoch 12/15\n",
      "2764/2764 [==============================] - 2s 823us/sample - loss: 0.5370 - accuracy: 0.7124 - precision_2: 0.7606 - recall_2: 0.8419\n",
      "Epoch 13/15\n",
      "2764/2764 [==============================] - 2s 788us/sample - loss: 0.5324 - accuracy: 0.7127 - precision_2: 0.7653 - recall_2: 0.8329\n",
      "Epoch 14/15\n",
      "2764/2764 [==============================] - 2s 792us/sample - loss: 0.5320 - accuracy: 0.7138 - precision_2: 0.7651 - recall_2: 0.8356\n",
      "Epoch 15/15\n",
      "2764/2764 [==============================] - 2s 792us/sample - loss: 0.5327 - accuracy: 0.6968 - precision_2: 0.7612 - recall_2: 0.8073\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "1185/1185 [==============================] - 0s 50us/sample - loss: 0.5444 - accuracy: 0.7046 - precision_2: 0.7768 - recall_2: 0.7807\n",
      "Test accuracy:0.7046413421630859, test recall: 0.7807351350784302, test precision: 0.7767969965934753\n"
     ]
    }
   ],
   "source": [
    "# Mircosoft data NN\n",
    "model_mrp = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(9,)),\n",
    "    keras.layers.Dense(7, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(7, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid),\n",
    "    ])\n",
    "print(\"Multi-Layer Perceptron Results for mrp_data\")\n",
    "model_mrp = multilayer_perceptron(mrp_data, model_mrp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Feature Testing</h4>\n",
    "We decided to take out each feature and see how the Neural Network performs. The logic for our approach is that if the metrics are higher, then it means that the removed feature is not adding as much value to the overall model. For Quora, we found that the worst feature was WMD distance. For Microsoft, the worst one was the word n-gram. However, we did not find a particular feature that outperformed another or did noticeably worse than the other, since all the metrics were similar to each other. In the future, we plan on looking for more efficient ways to do feature evaluation such as permutation feature importance, where the value of the features are shuffled rather than dropped entirely like we did. This process would also tell us what our best features were in a more logical manner, which is more helpful  to us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_perceptron_feature_testing(df, model, test_feature):\n",
    "    features = list(df.columns.values)\n",
    "    features.remove('is_Paraphrase')\n",
    "    features.remove('Sentence_1')\n",
    "    features.remove('Sentence_2')\n",
    "    features.remove(test_feature)\n",
    "    X = df[features]\n",
    "    y = df['is_Paraphrase']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()])\n",
    "    model.fit(X_train, y_train, epochs=15, batch_size=1)\n",
    "\n",
    "    test_loss, test_acc, test_pre, test_recall = model.evaluate(X_test, y_test)\n",
    "    print('Test accuracy:{}, test recall: {}, test precision: {}'.format(test_acc, test_recall, test_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing feature Cosine_Similarity\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 3150 samples\n",
      "Epoch 1/15\n",
      "3150/3150 [==============================] - 3s 859us/sample - loss: 0.6762 - accuracy: 0.6108 - precision_3: 0.4474 - recall_3: 0.1547\n",
      "Epoch 2/15\n",
      "3150/3150 [==============================] - 3s 814us/sample - loss: 0.5779 - accuracy: 0.6594 - precision_3: 0.5503 - recall_3: 0.5089\n",
      "Epoch 3/15\n",
      "3150/3150 [==============================] - 3s 826us/sample - loss: 0.5576 - accuracy: 0.6835 - precision_3: 0.5663 - recall_3: 0.6712\n",
      "Epoch 4/15\n",
      "3150/3150 [==============================] - 3s 813us/sample - loss: 0.5453 - accuracy: 0.6867 - precision_3: 0.5654 - recall_3: 0.7160\n",
      "Epoch 5/15\n",
      "3150/3150 [==============================] - 3s 813us/sample - loss: 0.5345 - accuracy: 0.6940 - precision_3: 0.5712 - recall_3: 0.7422\n",
      "Epoch 6/15\n",
      "3150/3150 [==============================] - 3s 825us/sample - loss: 0.5280 - accuracy: 0.6965 - precision_3: 0.5814 - recall_3: 0.6855\n",
      "Epoch 7/15\n",
      "3150/3150 [==============================] - 3s 826us/sample - loss: 0.5225 - accuracy: 0.6946 - precision_3: 0.5782 - recall_3: 0.6906\n",
      "Epoch 8/15\n",
      "3150/3150 [==============================] - 3s 821us/sample - loss: 0.5185 - accuracy: 0.7003 - precision_3: 0.5794 - recall_3: 0.7371\n",
      "Epoch 9/15\n",
      "3150/3150 [==============================] - 3s 821us/sample - loss: 0.5155 - accuracy: 0.7013 - precision_3: 0.5858 - recall_3: 0.6982\n",
      "Epoch 10/15\n",
      "3150/3150 [==============================] - 2s 770us/sample - loss: 0.5134 - accuracy: 0.7000 - precision_3: 0.5782 - recall_3: 0.7439\n",
      "Epoch 11/15\n",
      "3150/3150 [==============================] - 2s 777us/sample - loss: 0.5121 - accuracy: 0.6990 - precision_3: 0.5764 - recall_3: 0.7489\n",
      "Epoch 12/15\n",
      "3150/3150 [==============================] - 3s 803us/sample - loss: 0.5106 - accuracy: 0.7041 - precision_3: 0.5806 - recall_3: 0.7642\n",
      "Epoch 13/15\n",
      "3150/3150 [==============================] - 3s 816us/sample - loss: 0.5081 - accuracy: 0.6997 - precision_3: 0.5742 - recall_3: 0.7751\n",
      "Epoch 14/15\n",
      "3150/3150 [==============================] - 3s 824us/sample - loss: 0.5108 - accuracy: 0.7057 - precision_3: 0.5810 - recall_3: 0.7760\n",
      "Epoch 15/15\n",
      "3150/3150 [==============================] - 3s 816us/sample - loss: 0.5078 - accuracy: 0.7022 - precision_3: 0.5751 - recall_3: 0.7929\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "1350/1350 [==============================] - 0s 45us/sample - loss: 0.5278 - accuracy: 0.7037 - precision_3: 0.5950 - recall_3: 0.7595\n",
      "Test accuracy:0.7037037014961243, test recall: 0.7594696879386902, test precision: 0.5949555039405823\n",
      "\n",
      "\n",
      "Removing feature Edit_distance\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 3150 samples\n",
      "Epoch 1/15\n",
      "3150/3150 [==============================] - 3s 855us/sample - loss: 0.6033 - accuracy: 0.6495 - precision_4: 0.5632 - recall_4: 0.2975\n",
      "Epoch 2/15\n",
      "3150/3150 [==============================] - 3s 825us/sample - loss: 0.5391 - accuracy: 0.6756 - precision_4: 0.5725 - recall_4: 0.5376\n",
      "Epoch 3/15\n",
      "3150/3150 [==============================] - 3s 828us/sample - loss: 0.5236 - accuracy: 0.6737 - precision_4: 0.5645 - recall_4: 0.5731s - loss: 0.5269 - accuracy: 0.6709 - precision_4: 0.56\n",
      "Epoch 4/15\n",
      "3150/3150 [==============================] - 3s 850us/sample - loss: 0.5158 - accuracy: 0.6816 - precision_4: 0.5677 - recall_4: 0.6382s - loss: 0.4957 - accuracy: 0.7059 \n",
      "Epoch 5/15\n",
      "3150/3150 [==============================] - 3s 840us/sample - loss: 0.5097 - accuracy: 0.6933 - precision_4: 0.5795 - recall_4: 0.6686\n",
      "Epoch 6/15\n",
      "3150/3150 [==============================] - 3s 837us/sample - loss: 0.5050 - accuracy: 0.6981 - precision_4: 0.5807 - recall_4: 0.7058\n",
      "Epoch 7/15\n",
      "3150/3150 [==============================] - 3s 858us/sample - loss: 0.5041 - accuracy: 0.7006 - precision_4: 0.5837 - recall_4: 0.7075\n",
      "Epoch 8/15\n",
      "3150/3150 [==============================] - 3s 824us/sample - loss: 0.5021 - accuracy: 0.7022 - precision_4: 0.5795 - recall_4: 0.7549\n",
      "Epoch 9/15\n",
      "3150/3150 [==============================] - 3s 825us/sample - loss: 0.5000 - accuracy: 0.7098 - precision_4: 0.5908 - recall_4: 0.7396\n",
      "Epoch 10/15\n",
      "3150/3150 [==============================] - 3s 807us/sample - loss: 0.5019 - accuracy: 0.7032 - precision_4: 0.5797 - recall_4: 0.7625\n",
      "Epoch 11/15\n",
      "3150/3150 [==============================] - 3s 835us/sample - loss: 0.5011 - accuracy: 0.7048 - precision_4: 0.5812 - recall_4: 0.7650\n",
      "Epoch 12/15\n",
      "3150/3150 [==============================] - 3s 811us/sample - loss: 0.5010 - accuracy: 0.7079 - precision_4: 0.5831 - recall_4: 0.7802\n",
      "Epoch 13/15\n",
      "3150/3150 [==============================] - 3s 808us/sample - loss: 0.5002 - accuracy: 0.7108 - precision_4: 0.5859 - recall_4: 0.7844\n",
      "Epoch 14/15\n",
      "3150/3150 [==============================] - 3s 830us/sample - loss: 0.4993 - accuracy: 0.7095 - precision_4: 0.5877 - recall_4: 0.7591s - loss: 0.4987 - accuracy: 0.7097 - precision_4: 0.5884 - recall_4: 0.760\n",
      "Epoch 15/15\n",
      "3150/3150 [==============================] - 3s 824us/sample - loss: 0.4987 - accuracy: 0.7156 - precision_4: 0.5943 - recall_4: 0.7642\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "1350/1350 [==============================] - 0s 46us/sample - loss: 0.5095 - accuracy: 0.7074 - precision_4: 0.6012 - recall_4: 0.7481\n",
      "Test accuracy:0.7074074149131775, test recall: 0.748106062412262, test precision: 0.6012176275253296\n",
      "\n",
      "\n",
      "Removing feature Jaccard_similarity\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 3150 samples\n",
      "Epoch 1/15\n",
      "3150/3150 [==============================] - 3s 834us/sample - loss: 0.6020 - accuracy: 0.6514 - precision_5: 0.5407 - recall_5: 0.4776\n",
      "Epoch 2/15\n",
      "3150/3150 [==============================] - 3s 815us/sample - loss: 0.5645 - accuracy: 0.6806 - precision_5: 0.5652 - recall_5: 0.6484\n",
      "Epoch 3/15\n",
      "3150/3150 [==============================] - 3s 809us/sample - loss: 0.5438 - accuracy: 0.6794 - precision_5: 0.5627 - recall_5: 0.6560\n",
      "Epoch 4/15\n",
      "3150/3150 [==============================] - 3s 819us/sample - loss: 0.5316 - accuracy: 0.6883 - precision_5: 0.5718 - recall_5: 0.6762\n",
      "Epoch 5/15\n",
      "3150/3150 [==============================] - 3s 814us/sample - loss: 0.5225 - accuracy: 0.6908 - precision_5: 0.5757 - recall_5: 0.6720s - loss: 0.5177 - accuracy: 0.6948 - precision_5\n",
      "Epoch 6/15\n",
      "3150/3150 [==============================] - 3s 810us/sample - loss: 0.5177 - accuracy: 0.6990 - precision_5: 0.5846 - recall_5: 0.6864\n",
      "Epoch 7/15\n",
      "3150/3150 [==============================] - 3s 805us/sample - loss: 0.5139 - accuracy: 0.7063 - precision_5: 0.5910 - recall_5: 0.7084\n",
      "Epoch 8/15\n",
      "3150/3150 [==============================] - 3s 816us/sample - loss: 0.5106 - accuracy: 0.7070 - precision_5: 0.5915 - recall_5: 0.7101\n",
      "Epoch 9/15\n",
      "3150/3150 [==============================] - 3s 814us/sample - loss: 0.5098 - accuracy: 0.7063 - precision_5: 0.5887 - recall_5: 0.7236\n",
      "Epoch 10/15\n",
      "3150/3150 [==============================] - 3s 813us/sample - loss: 0.5084 - accuracy: 0.7022 - precision_5: 0.5867 - recall_5: 0.7008\n",
      "Epoch 11/15\n",
      "3150/3150 [==============================] - 3s 840us/sample - loss: 0.5069 - accuracy: 0.7079 - precision_5: 0.5904 - recall_5: 0.7261\n",
      "Epoch 12/15\n",
      "3150/3150 [==============================] - 3s 834us/sample - loss: 0.5058 - accuracy: 0.7108 - precision_5: 0.5980 - recall_5: 0.7016\n",
      "Epoch 13/15\n",
      "3150/3150 [==============================] - 3s 800us/sample - loss: 0.5059 - accuracy: 0.7140 - precision_5: 0.5986 - recall_5: 0.7236\n",
      "Epoch 14/15\n",
      "3150/3150 [==============================] - 3s 798us/sample - loss: 0.5034 - accuracy: 0.7098 - precision_5: 0.5966 - recall_5: 0.7025\n",
      "Epoch 15/15\n",
      "3150/3150 [==============================] - 3s 805us/sample - loss: 0.5024 - accuracy: 0.7149 - precision_5: 0.6041 - recall_5: 0.6991\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "1350/1350 [==============================] - 0s 48us/sample - loss: 0.5125 - accuracy: 0.7022 - precision_5: 0.5870 - recall_5: 0.8049\n",
      "Test accuracy:0.7022222280502319, test recall: 0.8049242496490479, test precision: 0.5870165824890137\n",
      "\n",
      "\n",
      "Removing feature Sequence_matcher\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 3150 samples\n",
      "Epoch 1/15\n",
      "3150/3150 [==============================] - 3s 915us/sample - loss: 0.5948 - accuracy: 0.6549 - precision_6: 0.5462 - recall_6: 0.4801s - loss: 0.5965 - accuracy: 0.6552 - precision_6: 0.5515 - recall_6: 0.47\n",
      "Epoch 2/15\n",
      "3150/3150 [==============================] - 3s 819us/sample - loss: 0.5611 - accuracy: 0.6797 - precision_6: 0.5627 - recall_6: 0.6602\n",
      "Epoch 3/15\n",
      "3150/3150 [==============================] - 3s 821us/sample - loss: 0.5400 - accuracy: 0.6810 - precision_6: 0.5624 - recall_6: 0.6779 - ETA: 2s - loss: 0.5452 - accuracy: \n",
      "Epoch 4/15\n",
      "3150/3150 [==============================] - 3s 834us/sample - loss: 0.5236 - accuracy: 0.6889 - precision_6: 0.5661 - recall_6: 0.7346\n",
      "Epoch 5/15\n",
      "3150/3150 [==============================] - 3s 830us/sample - loss: 0.5178 - accuracy: 0.6987 - precision_6: 0.5767 - recall_6: 0.7439\n",
      "Epoch 6/15\n",
      "3150/3150 [==============================] - 3s 836us/sample - loss: 0.5131 - accuracy: 0.6987 - precision_6: 0.5793 - recall_6: 0.7227\n",
      "Epoch 7/15\n",
      "3150/3150 [==============================] - 3s 827us/sample - loss: 0.5095 - accuracy: 0.7079 - precision_6: 0.5888 - recall_6: 0.7371\n",
      "Epoch 8/15\n",
      "3150/3150 [==============================] - 3s 824us/sample - loss: 0.5053 - accuracy: 0.7044 - precision_6: 0.5817 - recall_6: 0.7582\n",
      "Epoch 9/15\n",
      "3150/3150 [==============================] - 3s 831us/sample - loss: 0.5059 - accuracy: 0.6975 - precision_6: 0.5816 - recall_6: 0.6932\n",
      "Epoch 10/15\n",
      "3150/3150 [==============================] - 3s 828us/sample - loss: 0.5064 - accuracy: 0.7016 - precision_6: 0.5829 - recall_6: 0.7219\n",
      "Epoch 11/15\n",
      "3150/3150 [==============================] - 3s 831us/sample - loss: 0.5038 - accuracy: 0.7067 - precision_6: 0.5863 - recall_6: 0.7439\n",
      "Epoch 12/15\n",
      "3150/3150 [==============================] - 3s 829us/sample - loss: 0.5011 - accuracy: 0.7168 - precision_6: 0.5988 - recall_6: 0.7456\n",
      "Epoch 13/15\n",
      "3150/3150 [==============================] - 3s 832us/sample - loss: 0.5022 - accuracy: 0.7076 - precision_6: 0.5929 - recall_6: 0.7067\n",
      "Epoch 14/15\n",
      "3150/3150 [==============================] - 3s 831us/sample - loss: 0.5019 - accuracy: 0.7114 - precision_6: 0.5986 - recall_6: 0.7033\n",
      "Epoch 15/15\n",
      "3150/3150 [==============================] - 3s 832us/sample - loss: 0.4985 - accuracy: 0.7146 - precision_6: 0.5975 - recall_6: 0.7354\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "1350/1350 [==============================] - 0s 46us/sample - loss: 0.5071 - accuracy: 0.7207 - precision_6: 0.6135 - recall_6: 0.7727\n",
      "Test accuracy:0.7207407355308533, test recall: 0.7727272510528564, test precision: 0.6135338544845581\n",
      "\n",
      "\n",
      "Removing feature N-gram_measure\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 3150 samples\n",
      "Epoch 1/15\n",
      "3150/3150 [==============================] - 3s 866us/sample - loss: 0.5869 - accuracy: 0.6581 - precision_7: 0.5518 - recall_7: 0.4776\n",
      "Epoch 2/15\n",
      "3150/3150 [==============================] - 3s 825us/sample - loss: 0.5574 - accuracy: 0.6708 - precision_7: 0.5623 - recall_7: 0.5571\n",
      "Epoch 3/15\n",
      "3150/3150 [==============================] - 3s 795us/sample - loss: 0.5437 - accuracy: 0.6860 - precision_7: 0.5814 - recall_7: 0.5858\n",
      "Epoch 4/15\n",
      "3150/3150 [==============================] - 3s 799us/sample - loss: 0.5376 - accuracy: 0.6921 - precision_7: 0.5893 - recall_7: 0.5943\n",
      "Epoch 5/15\n",
      "3150/3150 [==============================] - 2s 792us/sample - loss: 0.5286 - accuracy: 0.6940 - precision_7: 0.5943 - recall_7: 0.5833\n",
      "Epoch 6/15\n",
      "3150/3150 [==============================] - 3s 797us/sample - loss: 0.5237 - accuracy: 0.6987 - precision_7: 0.5965 - recall_7: 0.6112\n",
      "Epoch 7/15\n",
      "3150/3150 [==============================] - 3s 803us/sample - loss: 0.5187 - accuracy: 0.7057 - precision_7: 0.6070 - recall_7: 0.6137\n",
      "Epoch 8/15\n",
      "3150/3150 [==============================] - 3s 813us/sample - loss: 0.5152 - accuracy: 0.7108 - precision_7: 0.6120 - recall_7: 0.6281\n",
      "Epoch 9/15\n",
      "3150/3150 [==============================] - 3s 804us/sample - loss: 0.5115 - accuracy: 0.7165 - precision_7: 0.6185 - recall_7: 0.6399\n",
      "Epoch 10/15\n",
      "3150/3150 [==============================] - 3s 838us/sample - loss: 0.5085 - accuracy: 0.7206 - precision_7: 0.6166 - recall_7: 0.6771\n",
      "Epoch 11/15\n",
      "3150/3150 [==============================] - 3s 805us/sample - loss: 0.5067 - accuracy: 0.7079 - precision_7: 0.6008 - recall_7: 0.6627\n",
      "Epoch 12/15\n",
      "3150/3150 [==============================] - 3s 797us/sample - loss: 0.5051 - accuracy: 0.7105 - precision_7: 0.6048 - recall_7: 0.6610\n",
      "Epoch 13/15\n",
      "3150/3150 [==============================] - 2s 788us/sample - loss: 0.5050 - accuracy: 0.7152 - precision_7: 0.6075 - recall_7: 0.6830\n",
      "Epoch 14/15\n",
      "3150/3150 [==============================] - 3s 806us/sample - loss: 0.5049 - accuracy: 0.7133 - precision_7: 0.6061 - recall_7: 0.6762\n",
      "Epoch 15/15\n",
      "3150/3150 [==============================] - 3s 818us/sample - loss: 0.5037 - accuracy: 0.7162 - precision_7: 0.6074 - recall_7: 0.6906\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "1350/1350 [==============================] - 0s 46us/sample - loss: 0.5129 - accuracy: 0.7119 - precision_7: 0.5964 - recall_7: 0.8144\n",
      "Test accuracy:0.7118518352508545, test recall: 0.814393937587738, test precision: 0.596393883228302\n",
      "\n",
      "\n",
      "Removing feature WMD_distance\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 3150 samples\n",
      "Epoch 1/15\n",
      "3150/3150 [==============================] - 3s 849us/sample - loss: 0.6570 - accuracy: 0.6378 - precision_8: 0.5292 - recall_8: 0.3221\n",
      "Epoch 2/15\n",
      "3150/3150 [==============================] - 3s 803us/sample - loss: 0.6048 - accuracy: 0.6714 - precision_8: 0.5642 - recall_8: 0.5495\n",
      "Epoch 3/15\n",
      "3150/3150 [==============================] - 3s 812us/sample - loss: 0.5803 - accuracy: 0.6832 - precision_8: 0.5804 - recall_8: 0.5647\n",
      "Epoch 4/15\n",
      "3150/3150 [==============================] - 3s 797us/sample - loss: 0.5652 - accuracy: 0.6883 - precision_8: 0.5852 - recall_8: 0.5833\n",
      "Epoch 5/15\n",
      "3150/3150 [==============================] - 3s 863us/sample - loss: 0.5544 - accuracy: 0.6892 - precision_8: 0.5859 - recall_8: 0.5883\n",
      "Epoch 6/15\n",
      "3150/3150 [==============================] - 3s 804us/sample - loss: 0.5472 - accuracy: 0.6949 - precision_8: 0.5884 - recall_8: 0.6247\n",
      "Epoch 7/15\n",
      "3150/3150 [==============================] - 3s 806us/sample - loss: 0.5402 - accuracy: 0.6946 - precision_8: 0.5904 - recall_8: 0.6103\n",
      "Epoch 8/15\n",
      "3150/3150 [==============================] - 3s 802us/sample - loss: 0.5367 - accuracy: 0.6933 - precision_8: 0.5894 - recall_8: 0.6044\n",
      "Epoch 9/15\n",
      "3150/3150 [==============================] - 3s 807us/sample - loss: 0.5317 - accuracy: 0.7025 - precision_8: 0.6003 - recall_8: 0.6221\n",
      "Epoch 10/15\n",
      "3150/3150 [==============================] - 3s 803us/sample - loss: 0.5271 - accuracy: 0.7035 - precision_8: 0.6003 - recall_8: 0.6298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15\n",
      "3150/3150 [==============================] - 3s 802us/sample - loss: 0.5251 - accuracy: 0.7032 - precision_8: 0.6011 - recall_8: 0.6230\n",
      "Epoch 12/15\n",
      "3150/3150 [==============================] - 3s 815us/sample - loss: 0.5255 - accuracy: 0.7038 - precision_8: 0.5998 - recall_8: 0.6348\n",
      "Epoch 13/15\n",
      "3150/3150 [==============================] - 3s 815us/sample - loss: 0.5197 - accuracy: 0.7130 - precision_8: 0.6154 - recall_8: 0.6289\n",
      "Epoch 14/15\n",
      "3150/3150 [==============================] - 3s 820us/sample - loss: 0.5197 - accuracy: 0.7187 - precision_8: 0.6178 - recall_8: 0.6585\n",
      "Epoch 15/15\n",
      "3150/3150 [==============================] - 3s 818us/sample - loss: 0.5164 - accuracy: 0.7137 - precision_8: 0.6107 - recall_8: 0.6551\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "1350/1350 [==============================] - 0s 45us/sample - loss: 0.5225 - accuracy: 0.7148 - precision_8: 0.6252 - recall_8: 0.6761\n",
      "Test accuracy:0.7148148417472839, test recall: 0.6761363744735718, test precision: 0.62521892786026\n",
      "\n",
      "\n",
      "Removing feature NER_similarity\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 3150 samples\n",
      "Epoch 1/15\n",
      "3150/3150 [==============================] - 3s 820us/sample - loss: 0.5864 - accuracy: 0.6568 - precision_9: 0.5488 - recall_9: 0.4852\n",
      "Epoch 2/15\n",
      "3150/3150 [==============================] - 2s 763us/sample - loss: 0.5546 - accuracy: 0.6854 - precision_9: 0.5656 - recall_9: 0.6999\n",
      "Epoch 3/15\n",
      "3150/3150 [==============================] - 2s 768us/sample - loss: 0.5350 - accuracy: 0.6854 - precision_9: 0.5626 - recall_9: 0.7295\n",
      "Epoch 4/15\n",
      "3150/3150 [==============================] - 2s 779us/sample - loss: 0.5188 - accuracy: 0.7035 - precision_9: 0.5803 - recall_9: 0.7608\n",
      "Epoch 5/15\n",
      "3150/3150 [==============================] - 2s 777us/sample - loss: 0.5120 - accuracy: 0.6927 - precision_9: 0.5716 - recall_9: 0.7253s - loss: 0.5124 - accuracy: 0.6909 - precision_9: 0.5682 - recall_9: 0.7\n",
      "Epoch 6/15\n",
      "3150/3150 [==============================] - 2s 779us/sample - loss: 0.5064 - accuracy: 0.7003 - precision_9: 0.5757 - recall_9: 0.7684\n",
      "Epoch 7/15\n",
      "3150/3150 [==============================] - 2s 773us/sample - loss: 0.5082 - accuracy: 0.6971 - precision_9: 0.5738 - recall_9: 0.7523\n",
      "Epoch 8/15\n",
      "3150/3150 [==============================] - 3s 801us/sample - loss: 0.5062 - accuracy: 0.7006 - precision_9: 0.5799 - recall_9: 0.7363\n",
      "Epoch 9/15\n",
      "3150/3150 [==============================] - 2s 764us/sample - loss: 0.5050 - accuracy: 0.7149 - precision_9: 0.5952 - recall_9: 0.7532\n",
      "Epoch 10/15\n",
      "3150/3150 [==============================] - 3s 806us/sample - loss: 0.5056 - accuracy: 0.7044 - precision_9: 0.5819 - recall_9: 0.7566\n",
      "Epoch 11/15\n",
      "3150/3150 [==============================] - 3s 803us/sample - loss: 0.5063 - accuracy: 0.6997 - precision_9: 0.5813 - recall_9: 0.7160\n",
      "Epoch 12/15\n",
      "3150/3150 [==============================] - 2s 767us/sample - loss: 0.5030 - accuracy: 0.7124 - precision_9: 0.5929 - recall_9: 0.7473\n",
      "Epoch 13/15\n",
      "3150/3150 [==============================] - 2s 788us/sample - loss: 0.5033 - accuracy: 0.6994 - precision_9: 0.5771 - recall_9: 0.7464\n",
      "Epoch 14/15\n",
      "3150/3150 [==============================] - 2s 779us/sample - loss: 0.5033 - accuracy: 0.7149 - precision_9: 0.5994 - recall_9: 0.7261\n",
      "Epoch 15/15\n",
      "3150/3150 [==============================] - 2s 780us/sample - loss: 0.5057 - accuracy: 0.7086 - precision_9: 0.5902 - recall_9: 0.7329\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "1350/1350 [==============================] - 0s 46us/sample - loss: 0.5102 - accuracy: 0.7052 - precision_9: 0.5883 - recall_9: 0.8201\n",
      "Test accuracy:0.7051851749420166, test recall: 0.8200757503509521, test precision: 0.5883151888847351\n",
      "\n",
      "\n",
      "Removing feature WSD\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 3150 samples\n",
      "Epoch 1/15\n",
      "3150/3150 [==============================] - 3s 885us/sample - loss: 0.6082 - accuracy: 0.6425 - precision_10: 0.5287 - recall_10: 0.4438\n",
      "Epoch 2/15\n",
      "3150/3150 [==============================] - 3s 795us/sample - loss: 0.5534 - accuracy: 0.6756 - precision_10: 0.5557 - recall_10: 0.6788\n",
      "Epoch 3/15\n",
      "3150/3150 [==============================] - 3s 839us/sample - loss: 0.5417 - accuracy: 0.6908 - precision_10: 0.5686 - recall_10: 0.7320\n",
      "Epoch 4/15\n",
      "3150/3150 [==============================] - 3s 798us/sample - loss: 0.5293 - accuracy: 0.6905 - precision_10: 0.5679 - recall_10: 0.7354\n",
      "Epoch 5/15\n",
      "3150/3150 [==============================] - 2s 783us/sample - loss: 0.5217 - accuracy: 0.6911 - precision_10: 0.5660 - recall_10: 0.7616\n",
      "Epoch 6/15\n",
      "3150/3150 [==============================] - 3s 803us/sample - loss: 0.5185 - accuracy: 0.6971 - precision_10: 0.5740 - recall_10: 0.7506\n",
      "Epoch 7/15\n",
      "3150/3150 [==============================] - 3s 796us/sample - loss: 0.5116 - accuracy: 0.7016 - precision_10: 0.5807 - recall_10: 0.7388\n",
      "Epoch 8/15\n",
      "3150/3150 [==============================] - 3s 797us/sample - loss: 0.5104 - accuracy: 0.7003 - precision_10: 0.5782 - recall_10: 0.7473\n",
      "Epoch 9/15\n",
      "3150/3150 [==============================] - 2s 785us/sample - loss: 0.5087 - accuracy: 0.7057 - precision_10: 0.5841 - recall_10: 0.7515\n",
      "Epoch 10/15\n",
      "3150/3150 [==============================] - 3s 798us/sample - loss: 0.5047 - accuracy: 0.7057 - precision_10: 0.5836 - recall_10: 0.7557\n",
      "Epoch 11/15\n",
      "3150/3150 [==============================] - 3s 809us/sample - loss: 0.5049 - accuracy: 0.7086 - precision_10: 0.5917 - recall_10: 0.7227\n",
      "Epoch 12/15\n",
      "3150/3150 [==============================] - 3s 824us/sample - loss: 0.5021 - accuracy: 0.7095 - precision_10: 0.5892 - recall_10: 0.7481\n",
      "Epoch 13/15\n",
      "3150/3150 [==============================] - 3s 797us/sample - loss: 0.5017 - accuracy: 0.7133 - precision_10: 0.5931 - recall_10: 0.7540\n",
      "Epoch 14/15\n",
      "3150/3150 [==============================] - 3s 797us/sample - loss: 0.4994 - accuracy: 0.7149 - precision_10: 0.5907 - recall_10: 0.7844\n",
      "Epoch 15/15\n",
      "3150/3150 [==============================] - 3s 797us/sample - loss: 0.4996 - accuracy: 0.7102 - precision_10: 0.5891 - recall_10: 0.7549\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "1350/1350 [==============================] - 0s 46us/sample - loss: 0.5099 - accuracy: 0.7089 - precision_10: 0.6174 - recall_10: 0.6723\n",
      "Test accuracy:0.7088888883590698, test recall: 0.6723484992980957, test precision: 0.6173912882804871\n",
      "\n",
      "\n",
      "Removing feature Synonym_Hypernym_Cosine\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 3150 samples\n",
      "Epoch 1/15\n",
      "3150/3150 [==============================] - 3s 855us/sample - loss: 0.5761 - accuracy: 0.6581 - precision_11: 0.5569 - recall_11: 0.4387\n",
      "Epoch 2/15\n",
      "3150/3150 [==============================] - 3s 798us/sample - loss: 0.5463 - accuracy: 0.6737 - precision_11: 0.5581 - recall_11: 0.6298\n",
      "Epoch 3/15\n",
      "3150/3150 [==============================] - 3s 796us/sample - loss: 0.5259 - accuracy: 0.6867 - precision_11: 0.5718 - recall_11: 0.6593\n",
      "Epoch 4/15\n",
      "3150/3150 [==============================] - 2s 788us/sample - loss: 0.5170 - accuracy: 0.6978 - precision_11: 0.5796 - recall_11: 0.7109\n",
      "Epoch 5/15\n",
      "3150/3150 [==============================] - 3s 802us/sample - loss: 0.5140 - accuracy: 0.6902 - precision_11: 0.5742 - recall_11: 0.6771\n",
      "Epoch 6/15\n",
      "3150/3150 [==============================] - 2s 786us/sample - loss: 0.5098 - accuracy: 0.7032 - precision_11: 0.5921 - recall_11: 0.6737\n",
      "Epoch 7/15\n",
      "3150/3150 [==============================] - 3s 810us/sample - loss: 0.5078 - accuracy: 0.6956 - precision_11: 0.5761 - recall_11: 0.7168\n",
      "Epoch 8/15\n",
      "3150/3150 [==============================] - 3s 820us/sample - loss: 0.5056 - accuracy: 0.7060 - precision_11: 0.5925 - recall_11: 0.6957\n",
      "Epoch 9/15\n",
      "3150/3150 [==============================] - 3s 816us/sample - loss: 0.5059 - accuracy: 0.7003 - precision_11: 0.5828 - recall_11: 0.7109\n",
      "Epoch 10/15\n",
      "3150/3150 [==============================] - 3s 795us/sample - loss: 0.5030 - accuracy: 0.7105 - precision_11: 0.5981 - recall_11: 0.6982\n",
      "Epoch 11/15\n",
      "3150/3150 [==============================] - 2s 771us/sample - loss: 0.5029 - accuracy: 0.7048 - precision_11: 0.5907 - recall_11: 0.6965\n",
      "Epoch 12/15\n",
      "3150/3150 [==============================] - 2s 787us/sample - loss: 0.5020 - accuracy: 0.7041 - precision_11: 0.5855 - recall_11: 0.7261\n",
      "Epoch 13/15\n",
      "3150/3150 [==============================] - 2s 775us/sample - loss: 0.5013 - accuracy: 0.7070 - precision_11: 0.5925 - recall_11: 0.7041\n",
      "Epoch 14/15\n",
      "3150/3150 [==============================] - 2s 769us/sample - loss: 0.5016 - accuracy: 0.7117 - precision_11: 0.5972 - recall_11: 0.7143\n",
      "Epoch 15/15\n",
      "3150/3150 [==============================] - 2s 777us/sample - loss: 0.5020 - accuracy: 0.7149 - precision_11: 0.5978 - recall_11: 0.7363\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "1350/1350 [==============================] - 0s 45us/sample - loss: 0.5130 - accuracy: 0.7067 - precision_11: 0.5962 - recall_11: 0.7746\n",
      "Test accuracy:0.7066666483879089, test recall: 0.7746211886405945, test precision: 0.5962098836898804\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quora data NN feature testing \n",
    "features = list(quora_data.columns.values)\n",
    "features.remove('is_Paraphrase')\n",
    "features.remove('Sentence_1')\n",
    "features.remove('Sentence_2')\n",
    "\n",
    "for feature in features:\n",
    "    print(\"Removing feature {}\".format(feature))\n",
    "    model_quora_testing = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(8,)),\n",
    "    keras.layers.Dense(7, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(7, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid),\n",
    "    ])\n",
    "    multilayer_perceptron_feature_testing(quora_data, model_quora_testing, feature)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing feature Cosine_Similarity\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 2764 samples\n",
      "Epoch 1/15\n",
      "2764/2764 [==============================] - 2s 836us/sample - loss: 0.5895 - accuracy: 0.6845 - precision_12: 0.7115 - recall_12: 0.9015\n",
      "Epoch 2/15\n",
      "2764/2764 [==============================] - 2s 805us/sample - loss: 0.5582 - accuracy: 0.6903 - precision_12: 0.7254 - recall_12: 0.8760\n",
      "Epoch 3/15\n",
      "2764/2764 [==============================] - 2s 820us/sample - loss: 0.5474 - accuracy: 0.6997 - precision_12: 0.7301 - recall_12: 0.8856\n",
      "Epoch 4/15\n",
      "2764/2764 [==============================] - 2s 819us/sample - loss: 0.5431 - accuracy: 0.6993 - precision_12: 0.7408 - recall_12: 0.8579\n",
      "Epoch 5/15\n",
      "2764/2764 [==============================] - 2s 840us/sample - loss: 0.5428 - accuracy: 0.7120 - precision_12: 0.7487 - recall_12: 0.8675s - loss: 0.5432 - accuracy: 0.7062 - precision_12: 0.7428 - recall\n",
      "Epoch 6/15\n",
      "2764/2764 [==============================] - 2s 811us/sample - loss: 0.5425 - accuracy: 0.7069 - precision_12: 0.7568 - recall_12: 0.8382\n",
      "Epoch 7/15\n",
      "2764/2764 [==============================] - 2s 814us/sample - loss: 0.5430 - accuracy: 0.7120 - precision_12: 0.7467 - recall_12: 0.8723\n",
      "Epoch 8/15\n",
      "2764/2764 [==============================] - 2s 851us/sample - loss: 0.5360 - accuracy: 0.7109 - precision_12: 0.7470 - recall_12: 0.8691\n",
      "Epoch 9/15\n",
      "2764/2764 [==============================] - 2s 799us/sample - loss: 0.5305 - accuracy: 0.7171 - precision_12: 0.7536 - recall_12: 0.8675\n",
      "Epoch 10/15\n",
      "2764/2764 [==============================] - 2s 860us/sample - loss: 0.5327 - accuracy: 0.7091 - precision_12: 0.7447 - recall_12: 0.8707\n",
      "Epoch 11/15\n",
      "2764/2764 [==============================] - 2s 887us/sample - loss: 0.5325 - accuracy: 0.7131 - precision_12: 0.7588 - recall_12: 0.8473\n",
      "Epoch 12/15\n",
      "2764/2764 [==============================] - 2s 838us/sample - loss: 0.5325 - accuracy: 0.7098 - precision_12: 0.7525 - recall_12: 0.8542\n",
      "Epoch 13/15\n",
      "2764/2764 [==============================] - 2s 817us/sample - loss: 0.5305 - accuracy: 0.7109 - precision_12: 0.7567 - recall_12: 0.8473\n",
      "Epoch 14/15\n",
      "2764/2764 [==============================] - 2s 825us/sample - loss: 0.5282 - accuracy: 0.7200 - precision_12: 0.7605 - recall_12: 0.8584\n",
      "Epoch 15/15\n",
      "2764/2764 [==============================] - 2s 813us/sample - loss: 0.5308 - accuracy: 0.7182 - precision_12: 0.7585 - recall_12: 0.8590\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "1185/1185 [==============================] - 0s 51us/sample - loss: 0.5418 - accuracy: 0.7131 - precision_12: 0.7270 - recall_12: 0.9113\n",
      "Test accuracy:0.7130801677703857, test recall: 0.91128009557724, test precision: 0.7269969582557678\n",
      "\n",
      "\n",
      "Removing feature Edit_distance\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 2764 samples\n",
      "Epoch 1/15\n",
      "2764/2764 [==============================] - 2s 860us/sample - loss: 0.5722 - accuracy: 0.6831 - precision_13: 0.6840 - recall_13: 0.9920\n",
      "Epoch 2/15\n",
      "2764/2764 [==============================] - 2s 828us/sample - loss: 0.5410 - accuracy: 0.7211 - precision_13: 0.7301 - recall_13: 0.9356\n",
      "Epoch 3/15\n",
      "2764/2764 [==============================] - 2s 831us/sample - loss: 0.5323 - accuracy: 0.7240 - precision_13: 0.7520 - recall_13: 0.8861\n",
      "Epoch 4/15\n",
      "2764/2764 [==============================] - 2s 832us/sample - loss: 0.5298 - accuracy: 0.7305 - precision_13: 0.7708 - recall_13: 0.8590\n",
      "Epoch 5/15\n",
      "2764/2764 [==============================] - 2s 844us/sample - loss: 0.5280 - accuracy: 0.7250 - precision_13: 0.7604 - recall_13: 0.8696\n",
      "Epoch 6/15\n",
      "2764/2764 [==============================] - 2s 882us/sample - loss: 0.5259 - accuracy: 0.7258 - precision_13: 0.7650 - recall_13: 0.8611\n",
      "Epoch 7/15\n",
      "2764/2764 [==============================] - 2s 855us/sample - loss: 0.5253 - accuracy: 0.7236 - precision_13: 0.7676 - recall_13: 0.8510\n",
      "Epoch 8/15\n",
      "2764/2764 [==============================] - 2s 860us/sample - loss: 0.5251 - accuracy: 0.7279 - precision_13: 0.7695 - recall_13: 0.8563\n",
      "Epoch 9/15\n",
      "2764/2764 [==============================] - 2s 820us/sample - loss: 0.5239 - accuracy: 0.7247 - precision_13: 0.7662 - recall_13: 0.8563\n",
      "Epoch 10/15\n",
      "2764/2764 [==============================] - 2s 855us/sample - loss: 0.5238 - accuracy: 0.7290 - precision_13: 0.7743 - recall_13: 0.8489\n",
      "Epoch 11/15\n",
      "2764/2764 [==============================] - 2s 837us/sample - loss: 0.5241 - accuracy: 0.7268 - precision_13: 0.7621 - recall_13: 0.8696\n",
      "Epoch 12/15\n",
      "2764/2764 [==============================] - 2s 827us/sample - loss: 0.5227 - accuracy: 0.7283 - precision_13: 0.7676 - recall_13: 0.8611\n",
      "Epoch 13/15\n",
      "2764/2764 [==============================] - 2s 821us/sample - loss: 0.5221 - accuracy: 0.7229 - precision_13: 0.7641 - recall_13: 0.8568\n",
      "Epoch 14/15\n",
      "2764/2764 [==============================] - 2s 828us/sample - loss: 0.5216 - accuracy: 0.7272 - precision_13: 0.7670 - recall_13: 0.8600\n",
      "Epoch 15/15\n",
      "2764/2764 [==============================] - 2s 827us/sample - loss: 0.5216 - accuracy: 0.7294 - precision_13: 0.7702 - recall_13: 0.8579\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "1185/1185 [==============================] - 0s 51us/sample - loss: 0.5363 - accuracy: 0.7181 - precision_13: 0.7402 - recall_13: 0.8885\n",
      "Test accuracy:0.7181434631347656, test recall: 0.8884664177894592, test precision: 0.7402322888374329\n",
      "\n",
      "\n",
      "Removing feature Jaccard_similarity\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 2764 samples\n",
      "Epoch 1/15\n",
      "2764/2764 [==============================] - 2s 868us/sample - loss: 0.5899 - accuracy: 0.6606 - precision_14: 0.6871 - recall_14: 0.9196\n",
      "Epoch 2/15\n",
      "2764/2764 [==============================] - 2s 816us/sample - loss: 0.5534 - accuracy: 0.6878 - precision_14: 0.7043 - recall_14: 0.9319\n",
      "Epoch 3/15\n",
      "2764/2764 [==============================] - 2s 818us/sample - loss: 0.5468 - accuracy: 0.6939 - precision_14: 0.7128 - recall_14: 0.9207\n",
      "Epoch 4/15\n",
      "2764/2764 [==============================] - 2s 789us/sample - loss: 0.5390 - accuracy: 0.7030 - precision_14: 0.7191 - recall_14: 0.9239\n",
      "Epoch 5/15\n",
      "2764/2764 [==============================] - 2s 788us/sample - loss: 0.5354 - accuracy: 0.6975 - precision_14: 0.7438 - recall_14: 0.8467\n",
      "Epoch 6/15\n",
      "2764/2764 [==============================] - 2s 799us/sample - loss: 0.5348 - accuracy: 0.7091 - precision_14: 0.7544 - recall_14: 0.8483\n",
      "Epoch 7/15\n",
      "2764/2764 [==============================] - 2s 801us/sample - loss: 0.5313 - accuracy: 0.7127 - precision_14: 0.7637 - recall_14: 0.8361\n",
      "Epoch 8/15\n",
      "2764/2764 [==============================] - 2s 786us/sample - loss: 0.5313 - accuracy: 0.7138 - precision_14: 0.7588 - recall_14: 0.8489\n",
      "Epoch 9/15\n",
      "2764/2764 [==============================] - 2s 791us/sample - loss: 0.5310 - accuracy: 0.7127 - precision_14: 0.7706 - recall_14: 0.8222\n",
      "Epoch 10/15\n",
      "2764/2764 [==============================] - 2s 786us/sample - loss: 0.5268 - accuracy: 0.7131 - precision_14: 0.7685 - recall_14: 0.8270\n",
      "Epoch 11/15\n",
      "2764/2764 [==============================] - 2s 786us/sample - loss: 0.5308 - accuracy: 0.7192 - precision_14: 0.7784 - recall_14: 0.8206\n",
      "Epoch 12/15\n",
      "2764/2764 [==============================] - 2s 812us/sample - loss: 0.5293 - accuracy: 0.7135 - precision_14: 0.7735 - recall_14: 0.8180\n",
      "Epoch 13/15\n",
      "2764/2764 [==============================] - 2s 799us/sample - loss: 0.5296 - accuracy: 0.7124 - precision_14: 0.7673 - recall_14: 0.8281\n",
      "Epoch 14/15\n",
      "2764/2764 [==============================] - 2s 796us/sample - loss: 0.5258 - accuracy: 0.7174 - precision_14: 0.7734 - recall_14: 0.8265\n",
      "Epoch 15/15\n",
      "2764/2764 [==============================] - 2s 791us/sample - loss: 0.5266 - accuracy: 0.7131 - precision_14: 0.7680 - recall_14: 0.8281\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "1185/1185 [==============================] - 0s 50us/sample - loss: 0.5339 - accuracy: 0.7190 - precision_14: 0.7801 - recall_14: 0.8048\n",
      "Test accuracy:0.7189873456954956, test recall: 0.8048162460327148, test precision: 0.7800982594490051\n",
      "\n",
      "\n",
      "Removing feature Sequence_matcher\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 2764 samples\n",
      "Epoch 1/15\n",
      "2764/2764 [==============================] - 2s 849us/sample - loss: 0.7228 - accuracy: 0.6791 - precision_15: 0.6821 - recall_15: 0.9888\n",
      "Epoch 2/15\n",
      "2764/2764 [==============================] - 2s 764us/sample - loss: 0.5630 - accuracy: 0.6975 - precision_15: 0.7129 - recall_15: 0.9292\n",
      "Epoch 3/15\n",
      "2764/2764 [==============================] - 2s 764us/sample - loss: 0.5492 - accuracy: 0.6946 - precision_15: 0.7255 - recall_15: 0.8861\n",
      "Epoch 4/15\n",
      "2764/2764 [==============================] - 2s 776us/sample - loss: 0.5427 - accuracy: 0.7008 - precision_15: 0.7376 - recall_15: 0.8691\n",
      "Epoch 5/15\n",
      "2764/2764 [==============================] - 2s 778us/sample - loss: 0.5398 - accuracy: 0.6983 - precision_15: 0.7457 - recall_15: 0.8441\n",
      "Epoch 6/15\n",
      "2764/2764 [==============================] - 2s 793us/sample - loss: 0.5384 - accuracy: 0.7059 - precision_15: 0.7597 - recall_15: 0.8297\n",
      "Epoch 7/15\n",
      "2764/2764 [==============================] - 2s 761us/sample - loss: 0.5387 - accuracy: 0.7015 - precision_15: 0.7596 - recall_15: 0.8206\n",
      "Epoch 8/15\n",
      "2764/2764 [==============================] - 2s 784us/sample - loss: 0.5374 - accuracy: 0.7106 - precision_15: 0.7704 - recall_15: 0.8180\n",
      "Epoch 9/15\n",
      "2764/2764 [==============================] - 2s 779us/sample - loss: 0.5352 - accuracy: 0.7051 - precision_15: 0.7644 - recall_15: 0.8185\n",
      "Epoch 10/15\n",
      "2764/2764 [==============================] - 2s 780us/sample - loss: 0.5351 - accuracy: 0.7066 - precision_15: 0.7683 - recall_15: 0.8137\n",
      "Epoch 11/15\n",
      "2764/2764 [==============================] - 2s 793us/sample - loss: 0.5341 - accuracy: 0.7080 - precision_15: 0.7675 - recall_15: 0.8185\n",
      "Epoch 12/15\n",
      "2764/2764 [==============================] - 2s 795us/sample - loss: 0.5322 - accuracy: 0.7120 - precision_15: 0.7679 - recall_15: 0.8260\n",
      "Epoch 13/15\n",
      "2764/2764 [==============================] - 2s 797us/sample - loss: 0.5330 - accuracy: 0.7106 - precision_15: 0.7675 - recall_15: 0.8238\n",
      "Epoch 14/15\n",
      "2764/2764 [==============================] - 2s 805us/sample - loss: 0.5323 - accuracy: 0.7116 - precision_15: 0.7755 - recall_15: 0.8105\n",
      "Epoch 15/15\n",
      "2764/2764 [==============================] - 2s 800us/sample - loss: 0.5292 - accuracy: 0.7221 - precision_15: 0.7833 - recall_15: 0.8175\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "1185/1185 [==============================] - 0s 51us/sample - loss: 0.5352 - accuracy: 0.7350 - precision_15: 0.7831 - recall_15: 0.8327\n",
      "Test accuracy:0.7350211143493652, test recall: 0.8326995968818665, test precision: 0.7830750942230225\n",
      "\n",
      "\n",
      "Removing feature N-gram_measure\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 2764 samples\n",
      "Epoch 1/15\n",
      "2764/2764 [==============================] - 2s 842us/sample - loss: 0.5946 - accuracy: 0.6805 - precision_16: 0.6804 - recall_16: 0.9995\n",
      "Epoch 2/15\n",
      "2764/2764 [==============================] - 2s 798us/sample - loss: 0.5465 - accuracy: 0.6798 - precision_16: 0.6798 - recall_16: 1.0000\n",
      "Epoch 3/15\n",
      "2764/2764 [==============================] - 2s 790us/sample - loss: 0.5403 - accuracy: 0.6842 - precision_16: 0.7204 - recall_16: 0.8749\n",
      "Epoch 4/15\n",
      "2764/2764 [==============================] - 2s 782us/sample - loss: 0.5366 - accuracy: 0.7116 - precision_16: 0.7735 - recall_16: 0.8143\n",
      "Epoch 5/15\n",
      "2764/2764 [==============================] - 2s 808us/sample - loss: 0.5352 - accuracy: 0.7062 - precision_16: 0.7699 - recall_16: 0.8100\n",
      "Epoch 6/15\n",
      "2764/2764 [==============================] - 2s 798us/sample - loss: 0.5333 - accuracy: 0.7167 - precision_16: 0.7700 - recall_16: 0.8318\n",
      "Epoch 7/15\n",
      "2764/2764 [==============================] - 2s 800us/sample - loss: 0.5307 - accuracy: 0.7084 - precision_16: 0.7700 - recall_16: 0.8143\n",
      "Epoch 8/15\n",
      "2764/2764 [==============================] - 2s 808us/sample - loss: 0.5312 - accuracy: 0.7098 - precision_16: 0.7780 - recall_16: 0.8020\n",
      "Epoch 9/15\n",
      "2764/2764 [==============================] - 2s 787us/sample - loss: 0.5313 - accuracy: 0.7145 - precision_16: 0.7795 - recall_16: 0.8089\n",
      "Epoch 10/15\n",
      "2764/2764 [==============================] - 2s 792us/sample - loss: 0.5302 - accuracy: 0.7149 - precision_16: 0.7779 - recall_16: 0.8127\n",
      "Epoch 11/15\n",
      "2764/2764 [==============================] - 2s 786us/sample - loss: 0.5286 - accuracy: 0.7192 - precision_16: 0.7895 - recall_16: 0.8004\n",
      "Epoch 12/15\n",
      "2764/2764 [==============================] - 2s 797us/sample - loss: 0.5286 - accuracy: 0.7160 - precision_16: 0.7802 - recall_16: 0.8105\n",
      "Epoch 13/15\n",
      "2764/2764 [==============================] - 2s 791us/sample - loss: 0.5293 - accuracy: 0.7145 - precision_16: 0.7801 - recall_16: 0.8079\n",
      "Epoch 14/15\n",
      "2764/2764 [==============================] - 2s 805us/sample - loss: 0.5302 - accuracy: 0.7182 - precision_16: 0.7758 - recall_16: 0.8233\n",
      "Epoch 15/15\n",
      "2764/2764 [==============================] - 2s 797us/sample - loss: 0.5282 - accuracy: 0.7196 - precision_16: 0.7808 - recall_16: 0.8169\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "1185/1185 [==============================] - 0s 51us/sample - loss: 0.5342 - accuracy: 0.7291 - precision_16: 0.7940 - recall_16: 0.8010\n",
      "Test accuracy:0.7291139364242554, test recall: 0.8010139465332031, test precision: 0.7939698696136475\n",
      "\n",
      "\n",
      "Removing feature WMD_distance\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 2764 samples\n",
      "Epoch 1/15\n",
      "2764/2764 [==============================] - 2s 846us/sample - loss: 0.7052 - accuracy: 0.6950 - precision_17: 0.7229 - recall_17: 0.8941\n",
      "Epoch 2/15\n",
      "2764/2764 [==============================] - 2s 812us/sample - loss: 0.5398 - accuracy: 0.7062 - precision_17: 0.7511 - recall_17: 0.8494\n",
      "Epoch 3/15\n",
      "2764/2764 [==============================] - 2s 781us/sample - loss: 0.5379 - accuracy: 0.7066 - precision_17: 0.7493 - recall_17: 0.8542\n",
      "Epoch 4/15\n",
      "2764/2764 [==============================] - 2s 792us/sample - loss: 0.5354 - accuracy: 0.7077 - precision_17: 0.7566 - recall_17: 0.8403\n",
      "Epoch 5/15\n",
      "2764/2764 [==============================] - 2s 801us/sample - loss: 0.5345 - accuracy: 0.7091 - precision_17: 0.7544 - recall_17: 0.8483\n",
      "Epoch 6/15\n",
      "2764/2764 [==============================] - 2s 794us/sample - loss: 0.5311 - accuracy: 0.7142 - precision_17: 0.7565 - recall_17: 0.8547\n",
      "Epoch 7/15\n",
      "2764/2764 [==============================] - 2s 774us/sample - loss: 0.5324 - accuracy: 0.7109 - precision_17: 0.7562 - recall_17: 0.8483\n",
      "Epoch 8/15\n",
      "2764/2764 [==============================] - 2s 780us/sample - loss: 0.5318 - accuracy: 0.7145 - precision_17: 0.7588 - recall_17: 0.8505s - loss: 0.5321 - accuracy: 0.7158 - precision_17: 0.7558 - recall_1\n",
      "Epoch 9/15\n",
      "2764/2764 [==============================] - 2s 819us/sample - loss: 0.5295 - accuracy: 0.7149 - precision_17: 0.7562 - recall_17: 0.8568\n",
      "Epoch 10/15\n",
      "2764/2764 [==============================] - 2s 780us/sample - loss: 0.5308 - accuracy: 0.7095 - precision_17: 0.7526 - recall_17: 0.8531\n",
      "Epoch 11/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2764/2764 [==============================] - 2s 789us/sample - loss: 0.5307 - accuracy: 0.7127 - precision_17: 0.7572 - recall_17: 0.8499\n",
      "Epoch 12/15\n",
      "2764/2764 [==============================] - 2s 790us/sample - loss: 0.5304 - accuracy: 0.7138 - precision_17: 0.7573 - recall_17: 0.8520\n",
      "Epoch 13/15\n",
      "2764/2764 [==============================] - 2s 803us/sample - loss: 0.5302 - accuracy: 0.7185 - precision_17: 0.7566 - recall_17: 0.8638\n",
      "Epoch 14/15\n",
      "2764/2764 [==============================] - 2s 810us/sample - loss: 0.5292 - accuracy: 0.7167 - precision_17: 0.7587 - recall_17: 0.8552\n",
      "Epoch 15/15\n",
      "2764/2764 [==============================] - 2s 809us/sample - loss: 0.5297 - accuracy: 0.7225 - precision_17: 0.7658 - recall_17: 0.8526\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "1185/1185 [==============================] - 0s 50us/sample - loss: 0.5353 - accuracy: 0.7342 - precision_17: 0.7505 - recall_17: 0.8999\n",
      "Test accuracy:0.7341772317886353, test recall: 0.8998732566833496, test precision: 0.7505285143852234\n",
      "\n",
      "\n",
      "Removing feature NER_similarity\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 2764 samples\n",
      "Epoch 1/15\n",
      "2764/2764 [==============================] - 2s 888us/sample - loss: 0.6889 - accuracy: 0.6795 - precision_18: 0.6798 - recall_18: 0.9989\n",
      "Epoch 2/15\n",
      "2764/2764 [==============================] - 2s 778us/sample - loss: 0.5876 - accuracy: 0.6780 - precision_18: 0.6849 - recall_18: 0.9750\n",
      "Epoch 3/15\n",
      "2764/2764 [==============================] - 2s 763us/sample - loss: 0.5654 - accuracy: 0.6957 - precision_18: 0.7081 - recall_18: 0.9399\n",
      "Epoch 4/15\n",
      "2764/2764 [==============================] - 2s 758us/sample - loss: 0.5507 - accuracy: 0.7022 - precision_18: 0.7173 - recall_18: 0.9276\n",
      "Epoch 5/15\n",
      "2764/2764 [==============================] - 2s 778us/sample - loss: 0.5493 - accuracy: 0.7048 - precision_18: 0.7361 - recall_18: 0.8819\n",
      "Epoch 6/15\n",
      "2764/2764 [==============================] - 2s 771us/sample - loss: 0.5438 - accuracy: 0.7135 - precision_18: 0.7434 - recall_18: 0.8834\n",
      "Epoch 7/15\n",
      "2764/2764 [==============================] - 2s 771us/sample - loss: 0.5401 - accuracy: 0.7026 - precision_18: 0.7305 - recall_18: 0.8914\n",
      "Epoch 8/15\n",
      "2764/2764 [==============================] - 2s 802us/sample - loss: 0.5378 - accuracy: 0.7080 - precision_18: 0.7495 - recall_18: 0.8568\n",
      "Epoch 9/15\n",
      "2764/2764 [==============================] - 2s 768us/sample - loss: 0.5339 - accuracy: 0.7160 - precision_18: 0.7566 - recall_18: 0.8584\n",
      "Epoch 10/15\n",
      "2764/2764 [==============================] - 2s 799us/sample - loss: 0.5320 - accuracy: 0.7160 - precision_18: 0.7588 - recall_18: 0.8536\n",
      "Epoch 11/15\n",
      "2764/2764 [==============================] - 2s 780us/sample - loss: 0.5339 - accuracy: 0.7142 - precision_18: 0.7599 - recall_18: 0.8473s - loss: 0.5379 - accuracy: 0.7102 - precision_18: 0.7555 - recall_18: \n",
      "Epoch 12/15\n",
      "2764/2764 [==============================] - 2s 787us/sample - loss: 0.5317 - accuracy: 0.7229 - precision_18: 0.7687 - recall_18: 0.8473\n",
      "Epoch 13/15\n",
      "2764/2764 [==============================] - 2s 771us/sample - loss: 0.5296 - accuracy: 0.7174 - precision_18: 0.7657 - recall_18: 0.8419\n",
      "Epoch 14/15\n",
      "2764/2764 [==============================] - 2s 804us/sample - loss: 0.5321 - accuracy: 0.7167 - precision_18: 0.7697 - recall_18: 0.8324\n",
      "Epoch 15/15\n",
      "2764/2764 [==============================] - 2s 789us/sample - loss: 0.5305 - accuracy: 0.7225 - precision_18: 0.7717 - recall_18: 0.8403\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "1185/1185 [==============================] - 0s 50us/sample - loss: 0.5335 - accuracy: 0.7291 - precision_18: 0.7854 - recall_18: 0.8162\n",
      "Test accuracy:0.7291139364242554, test recall: 0.8162230849266052, test precision: 0.785365879535675\n",
      "\n",
      "\n",
      "Removing feature WSD\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 2764 samples\n",
      "Epoch 1/15\n",
      "2764/2764 [==============================] - 2s 832us/sample - loss: 0.6283 - accuracy: 0.6733 - precision_19: 0.6969 - recall_19: 0.9191\n",
      "Epoch 2/15\n",
      "2764/2764 [==============================] - 2s 785us/sample - loss: 0.5801 - accuracy: 0.6939 - precision_19: 0.7167 - recall_19: 0.9090\n",
      "Epoch 3/15\n",
      "2764/2764 [==============================] - 2s 781us/sample - loss: 0.5729 - accuracy: 0.6986 - precision_19: 0.7252 - recall_19: 0.8962\n",
      "Epoch 4/15\n",
      "2764/2764 [==============================] - 2s 796us/sample - loss: 0.5665 - accuracy: 0.6993 - precision_19: 0.7259 - recall_19: 0.8962\n",
      "Epoch 5/15\n",
      "2764/2764 [==============================] - 2s 802us/sample - loss: 0.5647 - accuracy: 0.7037 - precision_19: 0.7267 - recall_19: 0.9042\n",
      "Epoch 6/15\n",
      "2764/2764 [==============================] - 2s 809us/sample - loss: 0.5619 - accuracy: 0.6986 - precision_19: 0.7290 - recall_19: 0.8861\n",
      "Epoch 7/15\n",
      "2764/2764 [==============================] - 2s 804us/sample - loss: 0.5569 - accuracy: 0.7095 - precision_19: 0.7372 - recall_19: 0.8898\n",
      "Epoch 8/15\n",
      "2764/2764 [==============================] - 2s 811us/sample - loss: 0.5570 - accuracy: 0.7091 - precision_19: 0.7358 - recall_19: 0.8925s - loss: 0.5585 - accuracy: 0.7211 - pr\n",
      "Epoch 9/15\n",
      "2764/2764 [==============================] - 2s 801us/sample - loss: 0.5555 - accuracy: 0.7106 - precision_19: 0.7414 - recall_19: 0.8819\n",
      "Epoch 10/15\n",
      "2764/2764 [==============================] - 2s 811us/sample - loss: 0.5535 - accuracy: 0.7142 - precision_19: 0.7432 - recall_19: 0.8856\n",
      "Epoch 11/15\n",
      "2764/2764 [==============================] - 2s 801us/sample - loss: 0.5477 - accuracy: 0.7207 - precision_19: 0.7506 - recall_19: 0.8824\n",
      "Epoch 12/15\n",
      "2764/2764 [==============================] - 2s 832us/sample - loss: 0.5513 - accuracy: 0.7138 - precision_19: 0.7403 - recall_19: 0.8920\n",
      "Epoch 13/15\n",
      "2764/2764 [==============================] - 2s 820us/sample - loss: 0.5498 - accuracy: 0.7214 - precision_19: 0.7470 - recall_19: 0.8925\n",
      "Epoch 14/15\n",
      "2764/2764 [==============================] - 2s 805us/sample - loss: 0.5474 - accuracy: 0.7156 - precision_19: 0.7465 - recall_19: 0.8808s - loss: 0.5503 - accuracy: 0.7201 - precision_19: 0.7447\n",
      "Epoch 15/15\n",
      "2764/2764 [==============================] - 2s 808us/sample - loss: 0.5434 - accuracy: 0.7164 - precision_19: 0.7499 - recall_19: 0.8744\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "1185/1185 [==============================] - 0s 51us/sample - loss: 0.5455 - accuracy: 0.7114 - precision_19: 0.7185 - recall_19: 0.9316\n",
      "Test accuracy:0.7113924026489258, test recall: 0.9315589070320129, test precision: 0.7184750437736511\n",
      "\n",
      "\n",
      "Removing feature Synonym_Hypernym_Cosine\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 2764 samples\n",
      "Epoch 1/15\n",
      "2764/2764 [==============================] - 2s 832us/sample - loss: 0.5863 - accuracy: 0.6838 - precision_20: 0.6961 - recall_20: 0.9494\n",
      "Epoch 2/15\n",
      "2764/2764 [==============================] - 2s 773us/sample - loss: 0.5539 - accuracy: 0.6979 - precision_20: 0.7221 - recall_20: 0.9031\n",
      "Epoch 3/15\n",
      "2764/2764 [==============================] - 2s 785us/sample - loss: 0.5429 - accuracy: 0.6990 - precision_20: 0.7267 - recall_20: 0.8930\n",
      "Epoch 4/15\n",
      "2764/2764 [==============================] - 2s 786us/sample - loss: 0.5369 - accuracy: 0.7044 - precision_20: 0.7392 - recall_20: 0.8733\n",
      "Epoch 5/15\n",
      "2764/2764 [==============================] - 2s 794us/sample - loss: 0.5354 - accuracy: 0.7116 - precision_20: 0.7493 - recall_20: 0.8654\n",
      "Epoch 6/15\n",
      "2764/2764 [==============================] - 2s 803us/sample - loss: 0.5348 - accuracy: 0.7095 - precision_20: 0.7569 - recall_20: 0.8435\n",
      "Epoch 7/15\n",
      "2764/2764 [==============================] - 2s 798us/sample - loss: 0.5340 - accuracy: 0.7066 - precision_20: 0.7572 - recall_20: 0.8366\n",
      "Epoch 8/15\n",
      "2764/2764 [==============================] - 2s 813us/sample - loss: 0.5316 - accuracy: 0.7149 - precision_20: 0.7660 - recall_20: 0.8361\n",
      "Epoch 9/15\n",
      "2764/2764 [==============================] - 2s 820us/sample - loss: 0.5287 - accuracy: 0.7178 - precision_20: 0.7656 - recall_20: 0.8430\n",
      "Epoch 10/15\n",
      "2764/2764 [==============================] - 2s 814us/sample - loss: 0.5305 - accuracy: 0.7131 - precision_20: 0.7712 - recall_20: 0.8217\n",
      "Epoch 11/15\n",
      "2764/2764 [==============================] - 2s 802us/sample - loss: 0.5317 - accuracy: 0.7135 - precision_20: 0.7658 - recall_20: 0.8334\n",
      "Epoch 12/15\n",
      "2764/2764 [==============================] - 2s 842us/sample - loss: 0.5329 - accuracy: 0.7211 - precision_20: 0.7705 - recall_20: 0.8398\n",
      "Epoch 13/15\n",
      "2764/2764 [==============================] - 2s 820us/sample - loss: 0.5300 - accuracy: 0.7106 - precision_20: 0.7659 - recall_20: 0.8270s - loss: 0.5317 - accuracy: 0.7084 - precision_20: 0.7633 - recall_20: 0.82\n",
      "Epoch 14/15\n",
      "2764/2764 [==============================] - 2s 824us/sample - loss: 0.5285 - accuracy: 0.7240 - precision_20: 0.7719 - recall_20: 0.8430\n",
      "Epoch 15/15\n",
      "2764/2764 [==============================] - 2s 838us/sample - loss: 0.5303 - accuracy: 0.7178 - precision_20: 0.7698 - recall_20: 0.8345\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "1185/1185 [==============================] - 0s 52us/sample - loss: 0.5342 - accuracy: 0.7232 - precision_20: 0.7878 - recall_20: 0.7997\n",
      "Test accuracy:0.7232067584991455, test recall: 0.7997465133666992, test precision: 0.7877652645111084\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Microsfot data feature testing\n",
    "features = list(mrp_data.columns.values)\n",
    "features.remove('is_Paraphrase')\n",
    "features.remove('Sentence_1')\n",
    "features.remove('Sentence_2')\n",
    "\n",
    "for feature in features:\n",
    "    print(\"Removing feature {}\".format(feature))\n",
    "    model_mrp_testing = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(8,)),\n",
    "    keras.layers.Dense(7, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(7, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid),\n",
    "    ])\n",
    "    multilayer_perceptron_feature_testing(mrp_data, model_mrp_testing, feature)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Evaluation and Conclusion </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, our baseline model performed better on the Quora data set, while our Neural Network performed better on the Microsoft data set. This could be because the Quora data set contains more noisy data and is closer to day-to-day human language in the sense that it contains more slang and the use of emoji, while Microsoft data set is more formal. Our models performed pretty well capturing similarities, however this is not the same as capturing paraphrases. For example, recall the sentences ‘What is the step by step guide to invest in share market in India?’ vs ‘What is the step by step guide to invest in share market?’ that we saw before. These would have a high similarity as calculated by our input features, however, these would not be considered paraphrases because of the one distinguishing word 'india' that alters the overall context. Additionally, sometimes sentences are paraphrases but they don’t have any words in common, meaning that our input measures would not be able to capture this. We thought that our WMD distance measure would aid in this. However, it is very likely that WMD distance measure on its own would not be able to influence the Neural Network. We hypothesized that syntactic and semantic similarity were both important while predicting if a pair of sentences were paraphrases. We built our features based on this hypothesis, creating both syntactic and semantic features. However, in the future, we may have to consider having more semantic features rather than a balance of both semantic and syntactic features. Furthermore, we only deal with 9 input features for our model when ideally we would use way more, encompassing topic features, linguistic features, and much more.  Lastly, we could increase the number of samples and the way we sample our data. For example, using k-fold validation and balancing our classes through over sampling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> References </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.gotrained.com/nltk-edit-distance-jaccard-distance/ <br>\n",
    "http://www.nltk.org/_modules/nltk/model/ngram.html <br>\n",
    "https://medium.com/@nikhiljaiswal_9475/sequencematcher-in-python-6b1e6f3915fc <br>\n",
    "https://towardsdatascience.com/word-distance-between-word-embeddings-cc3e9cf1d632 <br>\n",
    "https://radimrehurek.com/gensim/models/keyedvectors.html <br>\n",
    "https://stackoverflow.com/questions/31836058/nltk-named-entity-recognition-to-a-python-list/48738383 <br>\n",
    "https://www.kaggle.com/antriksh5235/semantic-similarity-using-wordnet <br>\n",
    "https://pdfs.semanticscholar.org/651e/e5def5cabff3cdf03b6c1a44c00aad9ef527.pdf <br>\n",
    "https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw <br>\n",
    "https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/ <br>\n",
    "http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
